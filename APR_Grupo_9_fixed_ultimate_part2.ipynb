{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación de Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de Prioritized Experience Replay (PER)\n",
    "class PrioritizedMemory(Memory):\n",
    "    def __init__(self, limit, alpha=0.6, beta=0.4, beta_increment=0.001, window_length=1):\n",
    "        super(PrioritizedMemory, self).__init__()\n",
    "        self.limit = limit\n",
    "        self.alpha = alpha  # Determina cuánto se usa la prioridad (0 = sin prioridad, 1 = solo prioridad)\n",
    "        self.beta = beta    # Importancia del muestreo (0 = sin corrección, 1 = corrección completa)\n",
    "        self.beta_increment = beta_increment  # Incremento de beta durante el entrenamiento\n",
    "        self.window_length = window_length\n",
    "        \n",
    "        # Inicializar buffers\n",
    "        self.actions = np.zeros(limit, dtype=np.uint8)\n",
    "        self.rewards = np.zeros(limit, dtype=np.float32)\n",
    "        self.terminals = np.zeros(limit, dtype=np.bool)\n",
    "        self.observations = [None] * limit\n",
    "        \n",
    "        # Variables para PER\n",
    "        self.priorities = np.zeros(limit, dtype=np.float32)\n",
    "        self.tree = SumTree(limit)\n",
    "        self.max_priority = 1.0\n",
    "        \n",
    "        self.position = 0\n",
    "        self.nb_entries = 0\n",
    "    \n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super(PrioritizedMemory, self).append(observation, action, reward, terminal, training=training)\n",
    "        \n",
    "        # Almacenar en buffers\n",
    "        self.observations[self.position] = observation\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.terminals[self.position] = terminal\n",
    "        \n",
    "        # Asignar máxima prioridad a nuevas experiencias\n",
    "        self.tree.add(self.max_priority, self.position)\n",
    "        \n",
    "        # Actualizar posición e incrementar entradas\n",
    "        self.position = (self.position + 1) % self.limit\n",
    "        if self.nb_entries < self.limit:\n",
    "            self.nb_entries += 1\n",
    "    \n",
    "    def _sample_batch_indices(self, batch_size):\n",
    "        # Incrementar beta para corrección de importancia\n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        \n",
    "        # Muestreo basado en prioridad\n",
    "        indices = []\n",
    "        priorities = []\n",
    "        segment = self.tree.total() / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            s = random.uniform(a, b)\n",
    "            idx, p, _ = self.tree.get(s)\n",
    "            indices.append(idx)\n",
    "            priorities.append(p)\n",
    "        \n",
    "        # Calcular pesos para corrección de importancia\n",
    "        sampling_probabilities = np.array(priorities) / self.tree.total()\n",
    "        is_weights = np.power(self.nb_entries * sampling_probabilities, -self.beta)\n",
    "        is_weights /= is_weights.max()  # Normalizar\n",
    "        \n",
    "        return indices, is_weights\n",
    "    \n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        if batch_idxs is None:\n",
    "            batch_idxs, is_weights = self._sample_batch_indices(batch_size)\n",
    "        else:\n",
    "            is_weights = np.ones((len(batch_idxs),), dtype=np.float32)\n",
    "        \n",
    "        # Crear batch\n",
    "        batch = {}\n",
    "        batch['is_weights'] = is_weights\n",
    "        batch['batch_idxs'] = batch_idxs\n",
    "        \n",
    "        # Extraer experiencias\n",
    "        batch['observations0'] = []\n",
    "        for idx in batch_idxs:\n",
    "            batch['observations0'].append(self.observations[idx])\n",
    "        \n",
    "        batch['actions'] = self.actions[batch_idxs]\n",
    "        batch['rewards'] = self.rewards[batch_idxs]\n",
    "        batch['terminals'] = self.terminals[batch_idxs]\n",
    "        \n",
    "        # Obtener observaciones siguientes\n",
    "        batch['observations1'] = []\n",
    "        for idx in batch_idxs:\n",
    "            terminal = self.terminals[idx]\n",
    "            if terminal:\n",
    "                next_idx = idx\n",
    "            else:\n",
    "                next_idx = (idx + 1) % self.limit\n",
    "            batch['observations1'].append(self.observations[next_idx])\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def update_priorities(self, batch_idxs, td_errors):\n",
    "        # Actualizar prioridades basadas en errores TD\n",
    "        for idx, error in zip(batch_idxs, td_errors):\n",
    "            priority = (np.abs(error) + 1e-6) ** self.alpha  # Evitar prioridad cero\n",
    "            self.tree.update(idx, priority)\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'limit': self.limit,\n",
    "            'alpha': self.alpha,\n",
    "            'beta': self.beta,\n",
    "            'beta_increment': self.beta_increment,\n",
    "            'window_length': self.window_length\n",
    "        }\n",
    "        return config\n",
    "\n",
    "# Estructura de datos SumTree para PER\n",
    "class SumTree:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1, dtype=np.float32)\n",
    "        self.data = np.zeros(capacity, dtype=np.int32)\n",
    "        self.n_entries = 0\n",
    "        self.write = 0\n",
    "    \n",
    "    def _propagate(self, idx, change):\n",
    "        # Propagar cambio hacia arriba\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        \n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "    \n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        \n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        \n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "    \n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "    \n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "        \n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "        \n",
    "        self.write = (self.write + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "    \n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        \n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "    \n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "        \n",
    "        return (self.data[dataIdx], self.tree[idx], dataIdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación de callbacks personalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback para guardar pesos después de cada episodio\n",
    "class EpisodeCheckpoint(Callback):\n",
    "    def __init__(self, filepath, interval=1, verbose=1):\n",
    "        super(EpisodeCheckpoint, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.interval = interval\n",
    "        self.verbose = verbose\n",
    "        self.episode = 0\n",
    "        self.best_reward = -np.inf\n",
    "        \n",
    "    def on_episode_end(self, episode, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.episode += 1\n",
    "        \n",
    "        # Guardar pesos cada 'interval' episodios\n",
    "        if self.episode % self.interval == 0:\n",
    "            filepath = self.filepath.format(episode=self.episode, **logs)\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nEpisodio {self.episode}: guardando pesos en {filepath}')\n",
    "            self.model.save_weights(filepath, overwrite=True)\n",
    "        \n",
    "        # Guardar los mejores pesos basados en la recompensa\n",
    "        if logs.get('episode_reward', -np.inf) > self.best_reward:\n",
    "            self.best_reward = logs.get('episode_reward')\n",
    "            best_filepath = self.filepath.format(episode='best')\n",
    "            if self.verbose > 0:\n",
    "                print(f'\\nNueva mejor recompensa: {self.best_reward:.2f}, guardando en {best_filepath}')\n",
    "            self.model.save_weights(best_filepath, overwrite=True)\n",
    "\n",
    "# Callback para visualizar el progreso del entrenamiento\n",
    "class TrainingVisualization(Callback):\n",
    "    def __init__(self, log_file, plot_interval=5):\n",
    "        super(TrainingVisualization, self).__init__()\n",
    "        self.log_file = log_file\n",
    "        self.plot_interval = plot_interval\n",
    "        self.episode_rewards = []\n",
    "        self.episode_losses = []\n",
    "        self.episode_maes = []\n",
    "        self.episode = 0\n",
    "        self.moving_avg_rewards = []\n",
    "        self.window_size = 10  # Tamaño de la ventana para promedio móvil\n",
    "        \n",
    "    def on_episode_end(self, episode, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.episode += 1\n",
    "        \n",
    "        # Guardar métricas\n",
    "        reward = logs.get('episode_reward', 0)\n",
    "        self.episode_rewards.append(reward)\n",
    "        self.episode_losses.append(logs.get('loss', 0))\n",
    "        self.episode_maes.append(logs.get('mae', 0))\n",
    "        \n",
    "        # Calcular promedio móvil de recompensas\n",
    "        if len(self.episode_rewards) >= self.window_size:\n",
    "            avg_reward = np.mean(self.episode_rewards[-self.window_size:])\n",
    "        else:\n",
    "            avg_reward = np.mean(self.episode_rewards)\n",
    "        self.moving_avg_rewards.append(avg_reward)\n",
    "        \n",
    "        # Guardar datos en archivo JSON\n",
    "        data = {\n",
    "            'episode_rewards': self.episode_rewards,\n",
    "            'episode_losses': self.episode_losses,\n",
    "            'episode_maes': self.episode_maes,\n",
    "            'moving_avg_rewards': self.moving_avg_rewards\n",
    "        }\n",
    "        with open(self.log_file, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "        \n",
    "        # Visualizar progreso cada plot_interval episodios\n",
    "        if self.episode % self.plot_interval == 0:\n",
    "            self.visualize_training()\n",
    "    \n",
    "    def visualize_training(self):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Gráfico de recompensas\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(self.episode_rewards)\n",
    "        plt.title('Recompensas por episodio')\n",
    "        plt.xlabel('Episodio')\n",
    "        plt.ylabel('Recompensa')\n",
    "        \n",
    "        # Gráfico de promedio móvil de recompensas\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(self.moving_avg_rewards)\n",
    "        plt.title(f'Promedio móvil de recompensas (ventana={self.window_size})')\n",
    "        plt.xlabel('Episodio')\n",
    "        plt.ylabel('Recompensa promedio')\n",
    "        \n",
    "        # Gráfico de pérdidas\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(self.episode_losses)\n",
    "        plt.title('Pérdida por episodio')\n",
    "        plt.xlabel('Episodio')\n",
    "        plt.ylabel('Pérdida')\n",
    "        \n",
    "        # Gráfico de MAE\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(self.episode_maes)\n",
    "        plt.title('MAE por episodio')\n",
    "        plt.xlabel('Episodio')\n",
    "        plt.ylabel('MAE')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Callback para ajustar la tasa de aprendizaje durante el entrenamiento\n",
    "class LearningRateScheduler(Callback):\n",
    "    def __init__(self, initial_lr=0.00025, min_lr=0.00001, decay_factor=0.5, decay_episodes=50):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.decay_factor = decay_factor\n",
    "        self.decay_episodes = decay_episodes\n",
    "        self.episode = 0\n",
    "        \n",
    "    def on_episode_end(self, episode, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.episode += 1\n",
    "        \n",
    "        # Ajustar tasa de aprendizaje cada decay_episodes episodios\n",
    "        if self.episode % self.decay_episodes == 0:\n",
    "            old_lr = K.get_value(self.model.optimizer.lr)\n",
    "            new_lr = max(old_lr * self.decay_factor, self.min_lr)  # No bajar más del mínimo\n",
    "            K.set_value(self.model.optimizer.lr, new_lr)\n",
    "            print(f'\\nEpisodio {self.episode}: tasa de aprendizaje ajustada de {old_lr:.6f} a {new_lr:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación del procesador de observaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # Convertir a escala de grises\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')  # Guardar como uint8 para ahorrar memoria\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.  # Normalizar a [0, 1]\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)  # Recortar recompensas para estabilizar el aprendizaje"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
