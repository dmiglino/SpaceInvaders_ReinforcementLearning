{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo mejorado con arquitectura más profunda y técnicas avanzadas\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    # (width, height, channels)\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    # (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "\n",
    "# Primera capa convolucional\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4)))\n",
    "model.add(BatchNormalization())  # Normalización por lotes para estabilizar el entrenamiento\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Segunda capa convolucional\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Tercera capa convolucional\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Cuarta capa convolucional adicional para mayor capacidad\n",
    "model.add(Convolution2D(128, (3, 3), strides=(1, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Aplanar para capas densas\n",
    "model.add(Flatten())\n",
    "\n",
    "# Primera capa densa\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))  # Dropout para evitar sobreajuste\n",
    "\n",
    "# Segunda capa densa\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Capa de salida\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implementación de la solución DQN con Prioritized Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la memoria con Prioritized Experience Replay\n",
    "memory = PrioritizedMemory(limit=100000,  # Memoria grande para almacenar más experiencias\n",
    "                          alpha=0.6,      # Prioridad basada en errores TD\n",
    "                          beta=0.4,       # Corrección de importancia del muestreo\n",
    "                          beta_increment=0.0005,  # Incremento gradual de beta\n",
    "                          window_length=WINDOW_LENGTH)\n",
    "\n",
    "# Procesador para las observaciones\n",
    "processor = AtariProcessor()\n",
    "\n",
    "# Política de exploración con decaimiento lineal\n",
    "# Comenzamos con exploración completa (1.0) y reducimos gradualmente a 0.05\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                              value_max=1.0, value_min=0.05, value_test=0.01,\n",
    "                              nb_steps=150000)  # Decaimiento más lento para mejor exploración\n",
    "\n",
    "# Definición del agente DQN\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
    "               memory=memory, processor=processor,\n",
    "               nb_steps_warmup=10000,  # Más pasos de calentamiento para llenar la memoria\n",
    "               gamma=0.99,  # Factor de descuento alto para valorar recompensas futuras\n",
    "               target_model_update=10000,  # Actualización menos frecuente de la red objetivo\n",
    "               train_interval=4,  # Entrenar cada 4 pasos para estabilidad\n",
    "               delta_clip=1.0)  # Recortar el error delta para evitar explosiones de gradiente\n",
    "\n",
    "# Compilación del agente con optimizador RMSprop (mejor para DQN en Atari)\n",
    "dqn.compile(RMSprop(learning_rate=0.00025, rho=0.95, epsilon=0.01), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuración de callbacks y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de callbacks\n",
    "weights_filename = 'dqn_ultimate_{}_weights.h5f'.format(env_name)\n",
    "checkpoint_weights_filename = 'dqn_ultimate_' + env_name + '_weights_episode_{episode}.h5f'\n",
    "log_filename = 'dqn_ultimate_{}_log.json'.format(env_name)\n",
    "visualization_log = 'dqn_ultimate_{}_visualization.json'.format(env_name)\n",
    "\n",
    "# Crear directorio para checkpoints si no existe\n",
    "checkpoint_dir = 'checkpoints'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# Callbacks personalizados\n",
    "callbacks = [\n",
    "    # Guardar pesos cada 5 episodios\n",
    "    EpisodeCheckpoint(os.path.join(checkpoint_dir, checkpoint_weights_filename), interval=5),\n",
    "    \n",
    "    # Visualizar progreso cada 5 episodios\n",
    "    TrainingVisualization(visualization_log, plot_interval=5),\n",
    "    \n",
    "    # Ajustar tasa de aprendizaje cada 50 episodios\n",
    "    LearningRateScheduler(initial_lr=0.00025, min_lr=0.00001, decay_factor=0.5, decay_episodes=50),\n",
    "    \n",
    "    # Logger estándar\n",
    "    FileLogger(log_filename, interval=100)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del agente\n",
    "dqn.fit(env, callbacks=callbacks, nb_steps=250000, log_interval=1000, visualize=False)\n",
    "\n",
    "# Guardar pesos finales\n",
    "dqn.save_weights(weights_filename, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualización de resultados del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y visualizar datos de entrenamiento\n",
    "if os.path.exists(visualization_log):\n",
    "    with open(visualization_log, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Gráfico de recompensas\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(data['episode_rewards'])\n",
    "    plt.title('Recompensas por episodio')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Recompensa')\n",
    "    \n",
    "    # Gráfico de promedio móvil de recompensas\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(data['moving_avg_rewards'])\n",
    "    plt.title('Promedio móvil de recompensas')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Recompensa promedio')\n",
    "    \n",
    "    # Gráfico de pérdidas\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(data['episode_losses'])\n",
    "    plt.title('Pérdida por episodio')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('Pérdida')\n",
    "    \n",
    "    # Gráfico de MAE\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(data['episode_maes'])\n",
    "    plt.title('MAE por episodio')\n",
    "    plt.xlabel('Episodio')\n",
    "    plt.ylabel('MAE')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test del agente entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar con los mejores pesos\n",
    "best_weights_filename = os.path.join(checkpoint_dir, 'dqn_ultimate_' + env_name + '_weights_episode_best.h5f')\n",
    "if os.path.exists(best_weights_filename):\n",
    "    print(f\"Cargando los mejores pesos desde: {best_weights_filename}\")\n",
    "    dqn.load_weights(best_weights_filename)\n",
    "else:\n",
    "    print(f\"No se encontraron los mejores pesos, usando los pesos finales: {weights_filename}\")\n",
    "    dqn.load_weights(weights_filename)\n",
    "\n",
    "# Test de n episodios para calcular la recompensa final\n",
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justificación de los parámetros seleccionados\n",
    "\n",
    "En esta implementación optimizada para máximo rendimiento, se han incorporado numerosas mejoras basadas en investigaciones recientes en aprendizaje por refuerzo profundo:\n",
    "\n",
    "1. **Prioritized Experience Replay (PER)**:\n",
    "   - Implementación completa de PER que prioriza experiencias con mayor error TD.\n",
    "   - Parámetros α=0.6 y β=0.4 inicialmente, con incremento gradual de β hasta 1.0.\n",
    "   - Esta técnica permite un aprendizaje más eficiente al muestrear con mayor frecuencia experiencias más informativas.\n",
    "\n",
    "2. **Arquitectura de red neuronal mejorada**:\n",
    "   - Red convolucional profunda con 4 capas convolucionales (vs 3 en implementaciones estándar).\n",
    "   - Capa adicional de 128 filtros para capturar patrones más complejos.\n",
    "   - BatchNormalization después de cada capa para estabilizar y acelerar el entrenamiento.\n",
    "   - Dos capas densas (512 y 256 neuronas) para mayor capacidad de representación.\n",
    "   - Dropout (20%) para prevenir el sobreajuste.\n",
    "\n",
    "3. **Memoria de experiencia mucho más grande**:\n",
    "   - Aumentada a 100,000 experiencias para mantener un historial más amplio.\n",
    "   - Permite al agente aprender de una variedad mucho mayor de situaciones.\n",
    "\n",
    "4. **Exploración más efectiva**:\n",
    "   - Decaimiento de epsilon más lento (150,000 pasos).\n",
    "   - Valor mínimo de epsilon reducido a 0.05 para mantener algo de exploración incluso al final.\n",
    "   - Valor de test reducido a 0.01 para evaluación más determinista.\n",
    "\n",
    "5. **Entrenamiento extenso**:\n",
    "   - 250,000 pasos de entrenamiento para permitir un aprendizaje profundo.\n",
    "   - 10,000 pasos de calentamiento para llenar adecuadamente la memoria antes de comenzar el aprendizaje.\n",
    "\n",
    "6. **Callbacks personalizados**:\n",
    "   - EpisodeCheckpoint: Guarda pesos cada 5 episodios y mantiene los mejores pesos basados en recompensa.\n",
    "   - TrainingVisualization: Visualiza el progreso del entrenamiento con gráficos detallados.\n",
    "   - LearningRateScheduler: Ajusta automáticamente la tasa de aprendizaje para evitar estancamiento.\n",
    "\n",
    "7. **Optimizador RMSprop optimizado**:\n",
    "   - Parámetros específicos (learning_rate=0.00025, rho=0.95) que han demostrado mejor rendimiento en DQN para juegos Atari.\n",
    "\n",
    "### Resultados esperados\n",
    "\n",
    "Con estas mejoras, esperamos que el agente logre una puntuación significativamente superior al requisito mínimo de 20 puntos. Basándonos en implementaciones similares en la literatura científica, este modelo optimizado debería ser capaz de alcanzar puntuaciones en el rango de 500-1500 puntos en Space Invaders, lo que representa un rendimiento muy competente.\n",
    "\n",
    "Las mejoras implementadas siguen las mejores prácticas establecidas en los artículos seminales sobre DQN y sus variantes, particularmente:\n",
    "\n",
    "1. **Prioritized Experience Replay** (Schaul et al., 2015)\n",
    "2. **Deep Q-Network con mejoras arquitectónicas** (Mnih et al., 2015)\n",
    "3. **Técnicas de estabilización del entrenamiento** como BatchNormalization y Dropout\n",
    "4. **Ajuste dinámico de hiperparámetros** durante el entrenamiento\n",
    "\n",
    "La combinación de estas técnicas avanzadas debería permitir al agente desarrollar estrategias sofisticadas para maximizar su puntuación en Space Invaders, superando ampliamente los requisitos mínimos del proyecto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
