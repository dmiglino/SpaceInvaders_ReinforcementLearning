{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "\n",
    "*   Alumno 1: Granizo, Mateo\n",
    "*   Alumno 2: Maiolo, Pablo\n",
    "*   Alumno 3: Miglino, Diego\n",
    "  \n",
    "https://github.com/dmiglino/SpaceInvaders_ReinforcementLearning/tree/entrega\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_YDFwZ-JscI",
    "outputId": "01a99aa0-3d4e-4cd1-b1bc-309aef65070a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/TP_Grupal\"\n",
    "mount='./'\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "print(IN_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6n7MIefJ21i",
    "outputId": "9a6fc610-9fb0-4f63-8562-46754d7d75fc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio: \n",
      "['.git', '.ipynb_checkpoints', 'anaconda_projects', 'apr_g9_dqn_SpaceInvaders-v0_4M_log.json', 'apr_g9_dqn_SpaceInvaders-v0_5M_GPU_Local_log.json', 'apr_g9_dqn_SpaceInvaders-v0_6M_log.json', 'apr_g9_dqn_SpaceInvaders-v0_log.json', 'apr_g9_dqn_SpaceInvaders-v0_weights_1000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_10000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_10000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1000000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1000000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1000000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1000000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_100000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_100000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_100_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_100_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1020000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1020000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1020000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1020000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1040000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1040000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1050000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1050000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1050000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1050000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1050_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1050_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_105_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_105_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1080000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1080000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1080000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1080000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_10_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_10_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1100000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1100000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1100_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1100_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_110_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_110_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1110000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1110000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1120000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1120000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1140000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1140000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1150_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1150_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_115_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_115_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1160000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1160000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1170000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1170000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_120000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_120000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1200000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1200000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1200000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1200000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_120000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_120000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_120000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_120000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1200_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1200_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1200_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1200_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_120_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_120_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1230000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1230000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1240000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1240000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1250_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1250_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_125_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_125_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1260000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1260000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1280000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1280000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1290000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1290000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1300_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1300_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_130_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_130_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1320000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1320000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1320000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1320000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1350000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1350000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1350_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1350_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_135_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_135_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1360000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1360000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1380000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1380000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1400000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1400000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1400_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1400_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_140_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_140_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1410000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1410000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1440000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1440000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1440000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1440000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1450_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1450_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_145_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_145_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1470000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1470000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1480000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1480000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_150000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_150000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1500000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1500000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_150000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_150000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1500_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1500_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_150_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_150_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1520000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1520000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1530000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1530000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1550_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1550_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_155_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_155_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1560000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1560000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1560000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1560000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1590000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1590000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_15_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_15_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1600000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1600000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_160000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_160000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1600_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1600_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1600_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1600_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_160_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_160_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1620000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1620000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1640000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1640000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1650000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1650000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1650_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1650_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_165_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_165_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1680000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1680000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1680000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1680000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1700_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1700_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_170_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_170_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1710000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1710000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1720000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1720000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1740000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1740000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1750_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1750_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_175_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_175_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1760000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1760000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1770000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1770000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_180000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_180000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1800000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1800000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1800000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1800000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_180000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_180000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1800_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1800_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_180_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_180_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1830000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1830000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1840000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1840000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1850_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1850_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_185_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_185_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1860000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1860000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1880000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1880000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1890000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1890000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1900_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1900_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_190_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_190_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1920000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1920000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1920000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1920000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1950000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1950000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1950_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1950_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_195_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_195_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1960000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1960000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_1980000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_1980000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2000000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2000000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_200000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_200000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_200000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_200000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_200_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_200_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2010000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2010000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2040000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2040000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2040000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2040000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2050_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2050_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_205_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_205_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2070000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2070000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2080000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2080000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_20_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_20_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_210000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_210000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2100000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2100000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2100_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2100_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_210_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_210_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2120000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2120000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2130000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2130000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2150_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2150_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_215_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_215_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2160000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2160000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2160000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2160000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2190000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2190000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2200000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2200000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2200_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2200_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_220_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_220_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2220000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2220000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2240000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2240000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2250000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2250000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2250_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2250_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_225_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_225_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2280000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2280000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2280000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2280000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2300_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2300_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_230_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_230_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2310000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2310000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2320000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2320000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2340000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2340000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2350_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2350_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_235_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_235_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2360000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2360000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2370000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2370000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_240000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_240000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2400000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2400000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2400000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2400000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_240000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_240000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_240000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_240000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2400_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2400_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2400_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2400_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_240_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_240_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2430000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2430000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2440000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2440000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2450_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2450_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_245_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_245_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2460000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2460000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2480000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2480000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2490000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2490000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_250000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_250000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2500_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2500_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_250_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_250_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2520000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2520000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2520000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2520000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2550000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2550000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2550_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2550_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_255_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_255_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2560000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2560000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2580000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2580000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_25_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_25_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2600000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2600000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2600_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2600_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_260_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_260_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2610000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2610000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2640000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2640000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2640000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2640000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2650_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2650_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_265_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_265_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2670000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2670000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2680000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2680000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_270000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_270000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2700000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2700000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2700_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2700_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_270_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_270_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2720000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2720000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2730000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2730000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2750_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2750_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_275_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_275_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2760000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2760000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2760000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2760000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2790000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2790000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2800000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2800000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_280000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_280000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2800_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2800_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2800_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2800_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_280_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_280_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2820000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2820000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2840000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2840000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2850000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2850000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2850_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2850_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_285_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_285_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2880000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2880000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2880000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2880000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2900_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2900_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_290_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_290_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2910000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2910000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2920000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2920000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2940000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2940000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2950_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2950_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_295_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_295_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2960000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2960000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_2970000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_2970000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_30000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_30000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_300000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_300000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3000000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3000000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3000000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3000000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_300000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_300000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_300000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_300000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_300_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_300_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3040000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3040000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3050_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3050_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_305_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_305_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3080000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3080000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_30_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_30_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3100_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3100_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_310_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_310_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3120000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3120000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3150_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3150_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_315_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_315_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3160000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3160000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3200000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3200000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_320000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_320000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3200_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3200_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_320_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_320_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3240000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3240000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3250_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3250_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_325_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_325_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3280000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3280000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_330000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_330000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3300_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3300_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_330_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_330_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3320000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3320000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3350_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3350_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_335_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_335_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3360000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3360000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3400000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3400000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3400_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3400_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_340_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_340_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3440000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3440000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3450_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3450_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_345_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_345_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3480000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3480000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_350000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_350000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3500_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3500_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_350_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_350_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3520000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3520000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3550_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3550_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_355_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_355_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3560000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3560000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_35_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_35_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_360000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_360000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3600000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3600000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_360000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_360000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_360000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_360000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3600_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3600_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_360_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_360_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3640000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3640000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3650_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3650_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_365_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_365_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3680000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3680000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3700_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3700_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_370_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_370_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3720000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3720000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3750_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3750_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_375_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_375_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3760000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3760000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3800000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3800000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3800_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3800_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_380_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_380_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3840000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3840000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3850_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3850_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_385_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_385_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3880000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3880000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_390000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_390000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3900_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3900_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_390_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_390_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3920000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3920000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3950_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3950_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_395_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_395_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_3960000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_3960000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4000000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4000000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_400000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_400000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_400000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_400000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_40000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_40000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_40000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_40000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_400_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_400_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_400_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_400_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4050_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4050_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_405_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_405_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_40_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_40_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4100_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4100_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_410_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_410_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4150_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4150_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_415_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_415_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_420000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_420000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_420000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_420000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4200_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4200_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_420_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_420_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4250_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4250_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_425_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_425_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4300_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4300_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_430_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_430_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4350_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4350_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_435_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_435_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_440000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_440000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4400_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4400_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_440_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_440_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4450_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4450_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_445_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_445_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_450000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_450000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_450000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_450000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4500_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4500_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_450_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_450_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4550_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4550_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_455_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_455_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_45_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_45_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4600_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4600_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_460_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_460_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4650_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4650_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_465_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_465_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4700_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4700_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_470_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_470_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4750_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4750_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_475_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_475_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_480000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_480000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_480000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_480000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_480000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_480000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4800_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4800_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_480_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_480_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4850_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4850_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_485_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_485_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4900_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4900_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_490_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_490_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_4950_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_4950_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_495_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_495_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_5000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_5000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_500000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_500000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_50000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_50000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_5000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_5000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_500_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_500_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_50_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_50_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_510000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_510000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_520000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_520000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_540000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_540000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_540000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_540000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_550000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_550000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_550_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_550_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_55_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_55_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_560000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_560000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_570000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_570000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_5_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_5_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_6000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_6000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_60000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_60000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_600000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_600000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_600000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_600000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_600000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_600000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_600000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_600000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_60000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_60000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_600_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_600_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_60_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_60_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_630000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_630000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_640000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_640000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_650000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_650000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_650_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_650_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_65_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_65_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_660000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_660000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_660000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_660000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_680000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_680000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_690000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_690000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_7000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_7000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_700000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_700000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_700_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_700_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_70_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_70_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_720000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_720000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_720000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_720000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_720000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_720000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_750000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_750000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_750000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_750000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_750_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_750_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_75_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_75_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_760000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_760000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_780000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_780000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_780000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_780000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_8000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_8000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_800000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_800000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_800000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_800000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_80000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_80000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_80000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_80000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_800_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_800_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_800_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_800_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_80_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_80_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_810000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_810000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_840000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_840000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_840000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_840000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_840000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_840000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_850000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_850000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_850_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_850_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_85_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_85_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_870000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_870000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_880000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_880000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_9000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_9000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_90000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_90000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_900000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_900000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_900000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_900000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_900000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_900000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_900_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_900_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_90_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_90_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_920000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_920000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_930000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_930000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_950000_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_950000_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_950_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_950_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_95_5M_GPU_Local.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_95_5M_GPU_Local.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_960000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_960000.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_960000_4M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_960000_4M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_960000_6M.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_960000_6M.h5f.index', 'apr_g9_dqn_SpaceInvaders-v0_weights_990000.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights_990000.h5f.index', 'APR_Grupo_9-1M-GPU-Local.ipynb', 'APR_Grupo_9-4M.ipynb', 'APR_Grupo_9-5M-GPU-Local.ipynb', 'APR_Grupo_9-6M-GPU.ipynb', 'APR_Grupo_9-6M.ipynb', 'APR_Grupo_9.ipynb', 'best_model_weights.h5f.data-00000-of-00001', 'best_model_weights.h5f.index', 'best_model_weights_4M.h5f.data-00000-of-00001', 'best_model_weights_4M.h5f.index', 'best_model_weights_5M_GPU_Local.h5f.data-00000-of-00001', 'best_model_weights_5M_GPU_Local.h5f.index', 'best_model_weights_6M.h5f.data-00000-of-00001', 'best_model_weights_6M.h5f.index', 'checkpoint', 'grafico_recompensas.png', 'grafico_recompensas_4M.png', 'grafico_recompensas_6M.png', 'mejor_ejecucion.txt', 'mejor_ejecucion_4M.txt', 'mejor_ejecucion_5M_GPU_Local.txt', 'mejor_ejecucion_6M.txt', 'Notebooks_APR', 'README.md', 'test_rewards.npy', 'test_rewards_4M.npy', 'test_rewards_5M_GPU_Local.npy', 'test_rewards_6M.npy']\n"
     ]
    }
   ],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UbVRjvHCJ8UF",
    "outputId": "fe539761-ae1b-4e9f-95de-9a3cfa9ae8bc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.17.3 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (0.17.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from gym==0.17.3) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from gym==0.17.3) (1.24.4)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from gym==0.17.3) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from gym==0.17.3) (1.6.0)\n",
      "Requirement already satisfied: future in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/Kojoley/atari-py.git\n",
      "  Cloning https://github.com/Kojoley/atari-py.git to c:\\users\\dmigl\\appdata\\local\\temp\\pip-req-build-r02cc9bg\n",
      "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from atari-py==1.2.2) (1.24.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git 'C:\\Users\\dmigl\\AppData\\Local\\Temp\\pip-req-build-r02cc9bg'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet==1.5.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: future in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from pyglet==1.5.0) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: h5py==3.1.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.5 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from h5py==3.1.0) (1.24.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Pillow==9.5.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (9.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: keras-rl2==1.0.5 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from keras-rl2==1.0.5) (2.5.3)\n",
      "Collecting numpy~=1.19.2 (from tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached numpy-1.19.5-cp38-cp38-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.20.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.44.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->keras-rl2==1.0.5) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (75.3.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (8.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->keras-rl2==1.0.5) (3.2.2)\n",
      "Using cached numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed numpy-1.19.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\dmigl\\anaconda3\\envs\\env_apr_g9\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Keras==2.2.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from Keras==2.2.4) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from Keras==2.2.4) (1.10.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from Keras==2.2.4) (1.15.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from Keras==2.2.4) (6.0.2)\n",
      "Requirement already satisfied: h5py in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from Keras==2.2.4) (3.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from Keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from Keras==2.2.4) (1.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tensorflow==2.5.3 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (2.5.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (1.19.5)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (0.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (3.20.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (0.44.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow==2.5.3) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (75.3.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.3) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.3) (8.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow==2.5.3) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.3) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.3) (3.2.2)\n",
      "Requirement already satisfied: torch==2.0.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (2.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from torch==2.0.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from torch==2.0.1) (3.7.4.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from torch==2.0.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from torch==2.0.1) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from torch==2.0.1) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Requirement already satisfied: agents==1.4.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from agents==1.4.0) (2.5.3)\n",
      "Requirement already satisfied: gym in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from agents==1.4.0) (0.17.3)\n",
      "Requirement already satisfied: ruamel.yaml in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from agents==1.4.0) (0.18.14)\n",
      "Requirement already satisfied: scipy in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from gym->agents==1.4.0) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from gym->agents==1.4.0) (1.19.5)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from gym->agents==1.4.0) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from gym->agents==1.4.0) (1.6.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from ruamel.yaml->agents==1.4.0) (0.2.8)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.20.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.44.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.34.1)\n",
      "Requirement already satisfied: future in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym->agents==1.4.0) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (75.3.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->agents==1.4.0) (8.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\dmigl\\anaconda3\\envs\\env_apr_g9\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ejecutar solo la primera vez..\n",
    "\n",
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.12\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0\n",
    "  %pip install matplotlib==3.3.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de TensorFlow: 2.5.3\n",
      "GPU detectada: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Versión de TensorFlow:\", tf.__version__)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(\"GPU detectada:\", gpus)\n",
    "else:\n",
    "    print(\"No se detectó GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j3eRhgI-Gb2a"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwOE6I_KGb2a",
    "outputId": "941f9c3a-e542-42e6-bd55-a097f9b71828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de acciones disponibles: 6 -> ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
      "Formato de las observaciones: Box(0, 255, (210, 160, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 4\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "print(\"Numero de acciones disponibles: \" + str(nb_actions) + \" -> \" + str(env.unwrapped.get_action_meanings()))\n",
    "print(\"Formato de las observaciones: \" + str(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        # removemos la parte superior (score) y inferior (vida del jugador)\n",
    "        cropped = observation[20:200, 8:152]\n",
    "        img = Image.fromarray(cropped)\n",
    "        img = img.resize(INPUT_SHAPE, Image.LANCZOS).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n",
      "(4, 84, 84)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 84, 84, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 1,816,998\n",
      "Trainable params: 1,816,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# modelo mas complejo - usar este\n",
    "input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "print(K.image_data_format())\n",
    "print(input_shape)\n",
    "\n",
    "if K.image_data_format() == 'channels_last':\n",
    "    # (width, height, channels)\n",
    "    model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "elif K.image_data_format() == 'channels_first':\n",
    "    # (channels, width, height)\n",
    "    model.add(Permute((1, 2, 3), input_shape=input_shape))\n",
    "else:\n",
    "    raise RuntimeError('Unknown image_dim_ordering.')\n",
    "\n",
    "# Capas convolucionales mejoradas con inicialización HeNormal\n",
    "model.add(Convolution2D(32, (8, 8), strides=(4, 4), \n",
    "                       activation='relu', kernel_initializer=HeNormal()))\n",
    " # model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(64, (4, 4), strides=(2, 2), \n",
    "                       activation='relu', kernel_initializer=HeNormal()))\n",
    " # model.add(BatchNormalization())\n",
    "\n",
    "model.add(Convolution2D(64, (3, 3), strides=(1, 1), \n",
    "                       activation='relu', kernel_initializer=HeNormal()))\n",
    " # model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Capas densas con dropout y batch normalization\n",
    "model.add(Dense(512, activation='relu', kernel_initializer=HeNormal()))\n",
    " # model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256, activation='relu', kernel_initializer=HeNormal()))\n",
    " # model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Capa de salida\n",
    "model.add(Dense(nb_actions, activation='linear', kernel_initializer=HeNormal()))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "2. Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.callbacks import Callback\n",
    "\n",
    "class HyperparameterMonitorCallback(Callback):\n",
    "    \"\"\"Callback para monitorear el estado de la memoria de los actuales valores de los hiperparametros\"\"\"\n",
    "    \n",
    "    def __init__(self, interval=5000):\n",
    "        self.interval = interval\n",
    "        self.steps_since_log = 0\n",
    "        \n",
    "    def on_step_end(self, step, logs=None):\n",
    "        self.steps_since_log += 1\n",
    "        if self.steps_since_log >= self.interval:\n",
    "            self.steps_since_log = 0\n",
    "            \n",
    "            print(f\"\\n>>> Memoria e Hiperparametros:\")\n",
    "            \n",
    "            # Estado de la memoria/buffer\n",
    "            memory = self.model.memory\n",
    "            print(f\"    [MEMORY] Memory Size: {len(memory)}\")\n",
    "            print(f\"    [MEMORY] Elite Fraction: {memory.elite_fraction}\")\n",
    "            \n",
    "            # Valores de los hiperparametros\n",
    "            print(f\"    [TRAIN] Batch Size: {dqn.batch_size}\")\n",
    "            print(f\"    [TRAIN] Step actual: {self.model.step}\")\n",
    "            \n",
    "            policy = getattr(self.model, 'policy', None)\n",
    "            if isinstance(policy, LinearAnnealedPolicy):\n",
    "                eps = policy.get_current_value()\n",
    "                print(f\"    [POLICY] epsilon actual: {eps:.4f}\")\n",
    "            optimizer = self.model.model.optimizer\n",
    "            lr_schedule = optimizer.learning_rate\n",
    "\n",
    "            if isinstance(lr_schedule, ExponentialDecay):\n",
    "                # Usamos el número de paso actual para evaluar el schedule\n",
    "                current_lr = float(lr_schedule(self.model.step).numpy())\n",
    "            else:\n",
    "                # Puede ser una variable o valor fijo\n",
    "                current_lr = float(K.get_value(lr_schedule))\n",
    "            print(f\"    [LR] Learning rate actual: {current_lr:.6f}\")\n",
    "            \n",
    "\n",
    "    def on_episode_end(self, episode, logs=None):\n",
    "        \"\"\"Mostrar stats al final del episodio\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.callbacks import Callback\n",
    "\n",
    "class BatchSizeSchedulerCallback(Callback):\n",
    "    def __init__(self, total_steps, initial_bs=32, schedule=None):\n",
    "        self.total_steps = total_steps\n",
    "        self.initial_bs = initial_bs\n",
    "        self.schedule = schedule if schedule else [(1/3, 2), (2/3, 2)]\n",
    "        self.applied = [False] * len(self.schedule)\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model.batch_size = self.initial_bs\n",
    "        print(f\"[BATCH] Entrenamiento iniciado con batch_size={self.initial_bs}\")\n",
    "\n",
    "    def on_step_end(self, step, logs=None):\n",
    "        current_step = self.model.step  # paso global del agente\n",
    "\n",
    "        for i, (threshold_frac, multiplier) in enumerate(self.schedule):\n",
    "            if not self.applied[i] and current_step >= int(self.total_steps * threshold_frac):\n",
    "                new_bs = self.model.batch_size * multiplier\n",
    "                self.model.batch_size = new_bs\n",
    "                self.applied[i] = True\n",
    "                print(f\"[BATCH] Step {current_step}: batch_size actualizado a {new_bs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.callbacks import Callback\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class AdaptiveEliteTuningAndSaveBestModelCallback(Callback):\n",
    "    def __init__(self, \n",
    "                 env,\n",
    "                 filename='best_model.h5f',\n",
    "                 test_episodes=5,\n",
    "                 warmup_steps=1000,\n",
    "                 monitor_interval=1000,\n",
    "                 improve_threshold=1.0,\n",
    "                 decay_threshold=0.1,\n",
    "                 decay_factor=0.7,\n",
    "                 grow_factor=0.7,\n",
    "                 min_elite=0.1,\n",
    "                 max_elite=0.9):\n",
    "        \n",
    "        self.env = env\n",
    "        self.filename = filename\n",
    "        self.test_episodes = test_episodes\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.monitor_interval = monitor_interval\n",
    "        self.improve_threshold = improve_threshold\n",
    "        self.decay_threshold = decay_threshold\n",
    "        self.decay_factor = decay_factor\n",
    "        self.grow_factor = grow_factor\n",
    "        self.min_elite = min_elite\n",
    "        self.max_elite = max_elite\n",
    "        self.best_avg_reward = -np.inf\n",
    "        self.steps_since_last_eval = 0\n",
    "\n",
    "    def on_step_end(self, step, logs=None):\n",
    "        self.steps_since_last_eval += 1\n",
    "        \n",
    "        if self.steps_since_last_eval >= self.monitor_interval:\n",
    "            self.steps_since_last_eval = 0\n",
    "            saved_step = self.model.step\n",
    "\n",
    "            # Evaluación\n",
    "            history = self.model.test(\n",
    "                self.env,\n",
    "                nb_episodes=self.test_episodes,\n",
    "                visualize=False,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            avg_reward = np.mean(history.history['episode_reward'])\n",
    "            max_reward = np.max(history.history['episode_reward'])\n",
    "            delta = avg_reward - self.best_avg_reward\n",
    "\n",
    "            # Restaurar contador del modelo (importante para evitar errores internos)\n",
    "            self.model.step = saved_step\n",
    "            self.model.training = True\n",
    "\n",
    "            print(f\"\\n>>> Step {saved_step} -> Avg={avg_reward:.2f}, Max={max_reward:.2f} -> {history.history['episode_reward']} -> Mejor Actual: {self.best_avg_reward:.2f}\")\n",
    "\n",
    "            # Guardar pesos si mejora\n",
    "            if avg_reward > self.best_avg_reward:\n",
    "                self.best_avg_reward = avg_reward\n",
    "                self.model.save_weights(self.filename, overwrite=True)\n",
    "                print(f\">>> Nuevo Máximo: {avg_reward:.2f} -> Guardado en {self.filename}\")\n",
    "\n",
    "            # Ajuste de elite_fraction si la memoria lo permite\n",
    "            if self.model.step >= self.warmup_steps and not math.isinf(delta):\n",
    "                mem = getattr(self.model, 'memory', None)\n",
    "                current_elite = getattr(mem, 'elite_fraction', None)\n",
    "    \n",
    "                if current_elite is not None:\n",
    "                    print(f\"[AdaptiveElite] Δ={delta:.2f}, elite_fraction={current_elite:.2f}\")\n",
    "                    if delta > self.improve_threshold:\n",
    "                        new_elite = min(current_elite * self.grow_factor, self.max_elite)\n",
    "                        print(f\"[AdaptiveElite] Mejora → Aumento elite_fraction a {new_elite:.2f}\")\n",
    "                        mem.elite_fraction = new_elite\n",
    "                    elif delta < self.decay_threshold:\n",
    "                        new_elite = max(current_elite * self.decay_factor, self.min_elite)\n",
    "                        print(f\"[AdaptiveElite] Estancamiento → Reduzco elite_fraction a {new_elite:.2f}\")\n",
    "                        mem.elite_fraction = new_elite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "from collections import deque\n",
    "from rl.memory import sample_batch_indexes, zeroed_observation\n",
    "\n",
    "class ElitistSequentialMemory(SequentialMemory):\n",
    "    def __init__(self, limit, window_length=1, elite_fraction=0.5, batch_size=128, reward_threshold=0.0, **kwargs):\n",
    "        super().__init__(limit=limit, window_length=window_length, **kwargs)\n",
    "        self.elite_fraction = elite_fraction\n",
    "        self.batch_size = batch_size\n",
    "        self.reward_threshold = reward_threshold\n",
    "        self.limit = limit\n",
    "        self.window_length = window_length\n",
    "\n",
    "        self.elite_fraction = elite_fraction  # porcentaje del batch que será elite\n",
    "        self.reward_threshold = reward_threshold  # criterio de recompensa mínima para ser elite\n",
    "        \n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.terminals = []\n",
    "        self.observations = []\n",
    "\n",
    "    def append(self, observation, action, reward, terminal, training=True):\n",
    "        super().append(observation, action, reward, terminal, training=training)\n",
    "        if not training:\n",
    "            return\n",
    "\n",
    "        if reward < 0:\n",
    "            print(f\"\\n[MEMORY] NEGATIVA {reward:.2f}\")\n",
    "\n",
    "        if len(self.observations) < self.limit:\n",
    "            self.observations.append(observation)\n",
    "            self.actions.append(action)\n",
    "            self.rewards.append(reward)\n",
    "            self.terminals.append(terminal)\n",
    "            # print(f\"\\n[MEMORY] Agregada experiencia nueva con reward {reward:.2f}\")\n",
    "        else:\n",
    "            # Buscar el índice de la peor experiencia actual\n",
    "            min_reward = min(self.rewards)\n",
    "            if reward > min_reward:\n",
    "                idx = self.rewards.index(min_reward)\n",
    "                #if reward > 0:\n",
    "                #    print(f\"\\n[MEMORY] Reemplazada exp (old: {min_reward:.2f}) -> nueva: {reward:.2f}\")\n",
    "                self.observations[idx] = observation\n",
    "                self.actions[idx] = action\n",
    "                self.rewards[idx] = reward\n",
    "                self.terminals[idx] = terminal\n",
    "\n",
    "    def sample(self, batch_size, batch_idxs=None):\n",
    "        all_experiences = super().sample(batch_size * 4)\n",
    "    \n",
    "        # Definimos una recompensa mínima para considerar élite (podés ajustar)\n",
    "        rewards = [e.reward for e in all_experiences]\n",
    "        threshold = np.percentile(rewards, 100 * (1 - self.elite_fraction))  # por ejemplo, top 20%\n",
    "    \n",
    "        elite = [e for e in all_experiences if e.reward >= threshold]\n",
    "        elite_ids = set(id(e) for e in elite)\n",
    "    \n",
    "        elite_count = min(len(elite), int(self.elite_fraction * batch_size))\n",
    "        selected_elite = elite[:elite_count]\n",
    "    \n",
    "        remaining = batch_size - len(selected_elite)\n",
    "        others = [e for e in all_experiences if id(e) not in elite_ids]\n",
    "    \n",
    "        if len(others) >= remaining:\n",
    "            final_others = others[:remaining]\n",
    "        else:\n",
    "            # Si no hay suficientes \"otros\", completamos duplicando de los mejores\n",
    "            needed = remaining - len(others)\n",
    "            if len(others) > 0:\n",
    "                final_others = others + random.choices(others, k=needed)\n",
    "            else:\n",
    "                final_others = random.choices(selected_elite, k=remaining)\n",
    "        \n",
    "        final_batch = selected_elite + final_others\n",
    "        #print(f\"\\n[MEMORY] Se muestrea un batch de {batch_size} con {len(selected_elite)} exp de elite y {len(final_others)} restantes.\")\n",
    "        assert len(final_batch) == batch_size\n",
    "        return final_batch\n",
    "\n",
    "    @property\n",
    "    def nb_entries(self):\n",
    "        return len(self.observations)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['limit'] = self.limit\n",
    "        return config\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "foSlxWH1Gb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elitist Sequential Memory configurada: elite_fraction=0.7, batch_size=128, reward_threshold=0.5\n",
      "Estrategia configurada: value_max=1.0, value_min=0.08, nb_steps=900000\n",
      "Agente configurado: nb_steps_warmup=50000, gamma=0.994, target_model_update=10000\n",
      "Learning Rate configurado: initial_lr=0.0002, decay_rate=0.96, decay_steps=900000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "# memoria para almacenar la experiencia del agente\n",
    "# memory = SequentialMemory(limit=10000, window_length=WINDOW_LENGTH)\n",
    "# memory = EpisodeParameterMemory(limit=100000, window_length=WINDOW_LENGTH)\n",
    "# memory = PrioritizedMemory(limit=100000, alpha=.6, start_beta=.4, end_beta=1., steps_annealed=100000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "# Memoria con priorizacion de mejores episodios \n",
    "# memory = PrioritizedExperienceReplay(\n",
    "#     limit=3000,           # Tamaño de memoria\n",
    "#     alpha=0.75,              # Priorización: 0.6 es un buen balance\n",
    "#     beta_start=0.4,         # Importance sampling inicial\n",
    "#     beta_frames=2500,     # Frames para annealing (75% del entrenamiento)\n",
    "#     epsilon=1e-6,           # Evitar prioridades cero\n",
    "#     window_length=WINDOW_LENGTH,\n",
    "#    ignore_episode_boundaries=False\n",
    "# )\n",
    "# print(f\"Prioritized Experience Replay configurado: alpha={memory.alpha}, beta_start={memory.beta_start}\")\n",
    "memory = ElitistSequentialMemory(\n",
    "    limit=2000000,\n",
    "    window_length=4,\n",
    "    elite_fraction=0.7,     # Usar 70% de experiencias con buen reward\n",
    "    reward_threshold=0.5    # Solo se considera “elite” si el reward > 0.5\n",
    ")\n",
    "print(f\"Elitist Sequential Memory configurada: elite_fraction={memory.elite_fraction}, batch_size={memory.batch_size}, reward_threshold={memory.reward_threshold}\")\n",
    "\n",
    "# processor\n",
    "processor = AtariProcessor()\n",
    "\n",
    "# policy que el agente va a seguir\n",
    "#policy = BoltzmannQPolicy()\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps',\n",
    "                              value_max=1.00, value_min=0.08, \n",
    "                              value_test=.05, nb_steps=900000)\n",
    "print(f\"Estrategia configurada: value_max={policy.value_max}, value_min={policy.value_min}, nb_steps={policy.nb_steps}\")\n",
    "\n",
    "# definicion del agente\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, policy=policy,\n",
    "               memory=memory, processor=processor,\n",
    "               nb_steps_warmup=50000, gamma=.994, batch_size=32,\n",
    "               target_model_update=10000, train_interval=4, delta_clip=1.0,\n",
    "               enable_double_dqn=True, enable_dueling_network=False)\n",
    "print(f\"Agente configurado: nb_steps_warmup={dqn.nb_steps_warmup}, gamma={dqn.gamma}, target_model_update={dqn.target_model_update}\")\n",
    "\n",
    "# compilacion del agente\n",
    "initial_learning_rate = 0.0002  # Learning rate más conservador\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "dqn.compile(Adam(learning_rate=lr_schedule), metrics=['mae'])\n",
    "print(f\"Learning Rate configurado: initial_lr={initial_learning_rate}, decay_rate={lr_schedule.decay_rate}, decay_steps={lr_schedule.decay_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BATCH] Entrenamiento iniciado con batch_size=32\n",
      "Training for 1000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "2500/2500 [==============================] - 8s 3ms/step - reward: 0.0172\n",
      "2 episodes - episode_reward: 11.500 [8.000, 15.000] - ale.lives: 2.333\n",
      "\n",
      "Interval 2 (2500 steps performed)\n",
      "2487/2500 [============================>.] - ETA: 0s - reward: 0.0125\n",
      ">>> -inf\n",
      "\n",
      ">>> Step 4999 -> Avg=11.80, Max=12.00 -> [12.0, 12.0, 12.0, 11.0, 12.0] -> Mejor Actual: -inf\n",
      ">>> Nuevo Máximo: 11.80 -> Guardado en best_model_weights_1M_GPU_Local.h5f\n",
      "2500/2500 [==============================] - 21s 9ms/step - reward: 0.0124\n",
      "3 episodes - episode_reward: 12.667 [5.000, 22.000] - ale.lives: 2.385\n",
      "\n",
      "Interval 3 (5000 steps performed)\n",
      "2500/2500 [==============================] - 9s 3ms/step - reward: 0.0128\n",
      "5 episodes - episode_reward: 8.200 [3.000, 13.000] - ale.lives: 1.968\n",
      "\n",
      "Interval 4 (7500 steps performed)\n",
      "2484/2500 [============================>.] - ETA: 0s - reward: 0.0153\n",
      ">>> 11.8\n",
      "\n",
      ">>> Step 9999 -> Avg=8.80, Max=11.00 -> [11.0, 6.0, 11.0, 11.0, 5.0] -> Mejor Actual: 11.80\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 20028\n",
      "    [MEMORY] Elite Fraction: 0.7\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 9999\n",
      "    [POLICY] epsilon actual: 0.9898\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 18s 7ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 10.250 [9.000, 11.000] - ale.lives: 2.060\n",
      "\n",
      "Interval 5 (10000 steps performed)\n",
      "2500/2500 [==============================] - 9s 3ms/step - reward: 0.0152\n",
      "5 episodes - episode_reward: 6.200 [1.000, 10.000] - ale.lives: 2.313\n",
      "\n",
      "Interval 6 (12500 steps performed)\n",
      "2490/2500 [============================>.] - ETA: 0s - reward: 0.0112\n",
      ">>> 11.8\n",
      "\n",
      ">>> Step 14999 -> Avg=12.00, Max=13.00 -> [12.0, 13.0, 12.0, 11.0, 12.0] -> Mejor Actual: 11.80\n",
      ">>> Nuevo Máximo: 12.00 -> Guardado en best_model_weights_1M_GPU_Local.h5f\n",
      "2500/2500 [==============================] - 20s 8ms/step - reward: 0.0112\n",
      "3 episodes - episode_reward: 11.333 [6.000, 22.000] - ale.lives: 1.950\n",
      "\n",
      "Interval 7 (15000 steps performed)\n",
      "2500/2500 [==============================] - 8s 3ms/step - reward: 0.0148\n",
      "4 episodes - episode_reward: 8.750 [2.000, 16.000] - ale.lives: 2.212\n",
      "\n",
      "Interval 8 (17500 steps performed)\n",
      "2488/2500 [============================>.] - ETA: 0s - reward: 0.0153\n",
      ">>> 12.0\n",
      "\n",
      ">>> Step 19999 -> Avg=11.00, Max=13.00 -> [12.0, 6.0, 13.0, 12.0, 12.0] -> Mejor Actual: 12.00\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 40060\n",
      "    [MEMORY] Elite Fraction: 0.7\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 19999\n",
      "    [POLICY] epsilon actual: 0.9796\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 19s 8ms/step - reward: 0.0156\n",
      "4 episodes - episode_reward: 10.000 [5.000, 19.000] - ale.lives: 2.093\n",
      "\n",
      "Interval 9 (20000 steps performed)\n",
      "2500/2500 [==============================] - 8s 3ms/step - reward: 0.0136\n",
      "5 episodes - episode_reward: 7.200 [3.000, 12.000] - ale.lives: 2.157\n",
      "\n",
      "Interval 10 (22500 steps performed)\n",
      "2483/2500 [============================>.] - ETA: 0s - reward: 0.0101\n",
      ">>> 12.0\n",
      "\n",
      ">>> Step 24999 -> Avg=10.20, Max=12.00 -> [5.0, 11.0, 12.0, 12.0, 11.0] -> Mejor Actual: 12.00\n",
      "2500/2500 [==============================] - 19s 8ms/step - reward: 0.0100\n",
      "3 episodes - episode_reward: 6.667 [4.000, 9.000] - ale.lives: 1.901\n",
      "\n",
      "Interval 11 (25000 steps performed)\n",
      "2500/2500 [==============================] - 8s 3ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 8.250 [2.000, 17.000] - ale.lives: 2.087\n",
      "\n",
      "Interval 12 (27500 steps performed)\n",
      "2491/2500 [============================>.] - ETA: 0s - reward: 0.0153\n",
      ">>> 12.0\n",
      "\n",
      ">>> Step 29999 -> Avg=7.00, Max=13.00 -> [13.0, 5.0, 6.0, 6.0, 5.0] -> Mejor Actual: 12.00\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 60090\n",
      "    [MEMORY] Elite Fraction: 0.7\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 29999\n",
      "    [POLICY] epsilon actual: 0.9693\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 16s 7ms/step - reward: 0.0152\n",
      "3 episodes - episode_reward: 13.667 [5.000, 22.000] - ale.lives: 2.269\n",
      "\n",
      "Interval 13 (30000 steps performed)\n",
      "2500/2500 [==============================] - 8s 3ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 7.750 [5.000, 14.000] - ale.lives: 2.145\n",
      "\n",
      "Interval 14 (32500 steps performed)\n",
      "2485/2500 [============================>.] - ETA: 0s - reward: 0.0105\n",
      ">>> 12.0\n",
      "\n",
      ">>> Step 34999 -> Avg=11.80, Max=12.00 -> [12.0, 12.0, 11.0, 12.0, 12.0] -> Mejor Actual: 12.00\n",
      "2500/2500 [==============================] - 20s 8ms/step - reward: 0.0104\n",
      "4 episodes - episode_reward: 8.000 [4.000, 11.000] - ale.lives: 2.207\n",
      "\n",
      "Interval 15 (35000 steps performed)\n",
      "2500/2500 [==============================] - 8s 3ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 7.250 [4.000, 9.000] - ale.lives: 2.198\n",
      "\n",
      "Interval 16 (37500 steps performed)\n",
      "2499/2500 [============================>.] - ETA: 0s - reward: 0.0116\n",
      ">>> 12.0\n",
      "\n",
      ">>> Step 39999 -> Avg=11.00, Max=13.00 -> [13.0, 12.0, 12.0, 6.0, 12.0] -> Mejor Actual: 12.00\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 80126\n",
      "    [MEMORY] Elite Fraction: 0.7\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 39999\n",
      "    [POLICY] epsilon actual: 0.9591\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 20s 8ms/step - reward: 0.0116\n",
      "6 episodes - episode_reward: 6.333 [4.000, 9.000] - ale.lives: 2.189\n",
      "\n",
      "Interval 17 (40000 steps performed)\n",
      "2500/2500 [==============================] - 8s 3ms/step - reward: 0.0144\n",
      "5 episodes - episode_reward: 7.200 [0.000, 10.000] - ale.lives: 2.132\n",
      "\n",
      "Interval 18 (42500 steps performed)\n",
      "2488/2500 [============================>.] - ETA: 0s - reward: 0.0117\n",
      ">>> 12.0\n",
      "\n",
      ">>> Step 44999 -> Avg=9.40, Max=12.00 -> [6.0, 12.0, 11.0, 12.0, 6.0] -> Mejor Actual: 12.00\n",
      "2500/2500 [==============================] - 19s 7ms/step - reward: 0.0116\n",
      "4 episodes - episode_reward: 7.250 [2.000, 18.000] - ale.lives: 2.300\n",
      "\n",
      "Interval 19 (45000 steps performed)\n",
      "2500/2500 [==============================] - 8s 3ms/step - reward: 0.0164\n",
      "3 episodes - episode_reward: 10.000 [0.000, 15.000] - ale.lives: 2.114\n",
      "\n",
      "Interval 20 (47500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0136\n",
      ">>> 12.0\n",
      "\n",
      ">>> Step 49999 -> Avg=10.20, Max=12.00 -> [12.0, 6.0, 11.0, 11.0, 11.0] -> Mejor Actual: 12.00\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 100158\n",
      "    [MEMORY] Elite Fraction: 0.7\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 49999\n",
      "    [POLICY] epsilon actual: 0.9489\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 19s 8ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 9.250 [5.000, 11.000] - ale.lives: 2.116\n",
      "\n",
      "Interval 21 (50000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 10.750 [3.000, 18.000] - loss: 0.007 - mae: 0.029 - mean_q: 0.003 - mean_eps: 0.948 - ale.lives: 2.180\n",
      "\n",
      "Interval 22 (52500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0152\n",
      ">>> 12.0\n",
      "\n",
      ">>> Step 54999 -> Avg=16.00, Max=21.00 -> [17.0, 20.0, 21.0, 12.0, 10.0] -> Mejor Actual: 12.00\n",
      ">>> Nuevo Máximo: 16.00 -> Guardado en best_model_weights_1M_GPU_Local.h5f\n",
      "[AdaptiveElite] Δ=4.00, elite_fraction=0.70\n",
      "[AdaptiveElite] Mejora → Aumento elite_fraction a 0.77\n",
      "2500/2500 [==============================] - 68s 27ms/step - reward: 0.0152\n",
      "3 episodes - episode_reward: 10.333 [8.000, 12.000] - loss: 0.007 - mae: 0.035 - mean_q: -0.018 - mean_eps: 0.945 - ale.lives: 2.083\n",
      "\n",
      "Interval 23 (55000 steps performed)\n",
      "2500/2500 [==============================] - 53s 21ms/step - reward: 0.0128\n",
      "5 episodes - episode_reward: 8.000 [5.000, 12.000] - loss: 0.009 - mae: 0.029 - mean_q: -0.015 - mean_eps: 0.943 - ale.lives: 2.298\n",
      "\n",
      "Interval 24 (57500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 16.0\n",
      "\n",
      ">>> Step 59999 -> Avg=14.80, Max=17.00 -> [14.0, 17.0, 13.0, 16.0, 14.0] -> Mejor Actual: 16.00\n",
      "[AdaptiveElite] Δ=-1.20, elite_fraction=0.77\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.69\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 120190\n",
      "    [MEMORY] Elite Fraction: 0.6930000000000001\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 59999\n",
      "    [POLICY] epsilon actual: 0.9387\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 65s 26ms/step - reward: 0.0156\n",
      "4 episodes - episode_reward: 8.500 [5.000, 15.000] - loss: 0.008 - mae: 0.023 - mean_q: 0.002 - mean_eps: 0.940 - ale.lives: 2.201\n",
      "\n",
      "Interval 25 (60000 steps performed)\n",
      "2500/2500 [==============================] - 53s 21ms/step - reward: 0.0124\n",
      "4 episodes - episode_reward: 8.000 [4.000, 13.000] - loss: 0.006 - mae: 0.021 - mean_q: 0.004 - mean_eps: 0.937 - ale.lives: 2.046\n",
      "\n",
      "Interval 26 (62500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 16.0\n",
      "\n",
      ">>> Step 64999 -> Avg=6.80, Max=19.00 -> [4.0, 4.0, 19.0, 3.0, 4.0] -> Mejor Actual: 16.00\n",
      "[AdaptiveElite] Δ=-9.20, elite_fraction=0.69\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.62\n",
      "2500/2500 [==============================] - 61s 24ms/step - reward: 0.0132\n",
      "4 episodes - episode_reward: 6.750 [5.000, 8.000] - loss: 0.007 - mae: 0.023 - mean_q: 0.004 - mean_eps: 0.935 - ale.lives: 2.136\n",
      "\n",
      "Interval 27 (65000 steps performed)\n",
      "2500/2500 [==============================] - 54s 21ms/step - reward: 0.0144\n",
      "5 episodes - episode_reward: 8.600 [4.000, 10.000] - loss: 0.007 - mae: 0.025 - mean_q: 0.004 - mean_eps: 0.932 - ale.lives: 2.208\n",
      "\n",
      "Interval 28 (67500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0128\n",
      ">>> 16.0\n",
      "\n",
      ">>> Step 69999 -> Avg=15.00, Max=21.00 -> [21.0, 15.0, 12.0, 18.0, 9.0] -> Mejor Actual: 16.00\n",
      "[AdaptiveElite] Δ=-1.00, elite_fraction=0.62\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.56\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 140222\n",
      "    [MEMORY] Elite Fraction: 0.56133\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 69999\n",
      "    [POLICY] epsilon actual: 0.9284\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 66s 27ms/step - reward: 0.0128\n",
      "3 episodes - episode_reward: 10.000 [7.000, 12.000] - loss: 0.007 - mae: 0.026 - mean_q: 0.009 - mean_eps: 0.930 - ale.lives: 2.004\n",
      "\n",
      "Interval 29 (70000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0148\n",
      "4 episodes - episode_reward: 8.500 [5.000, 12.000] - loss: 0.007 - mae: 0.034 - mean_q: 0.027 - mean_eps: 0.927 - ale.lives: 2.189\n",
      "\n",
      "Interval 30 (72500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 16.0\n",
      "\n",
      ">>> Step 74999 -> Avg=13.80, Max=21.00 -> [6.0, 19.0, 21.0, 11.0, 12.0] -> Mejor Actual: 16.00\n",
      "[AdaptiveElite] Δ=-2.20, elite_fraction=0.56\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.51\n",
      "2500/2500 [==============================] - 68s 27ms/step - reward: 0.0132\n",
      "4 episodes - episode_reward: 10.250 [6.000, 14.000] - loss: 0.006 - mae: 0.033 - mean_q: 0.023 - mean_eps: 0.925 - ale.lives: 1.958\n",
      "\n",
      "Interval 31 (75000 steps performed)\n",
      "2500/2500 [==============================] - 53s 21ms/step - reward: 0.0168\n",
      "4 episodes - episode_reward: 9.250 [0.000, 19.000] - loss: 0.007 - mae: 0.032 - mean_q: 0.023 - mean_eps: 0.922 - ale.lives: 2.198\n",
      "\n",
      "Interval 32 (77500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0124\n",
      ">>> 16.0\n",
      "\n",
      ">>> Step 79999 -> Avg=13.00, Max=16.00 -> [10.0, 15.0, 16.0, 8.0, 16.0] -> Mejor Actual: 16.00\n",
      "[AdaptiveElite] Δ=-3.00, elite_fraction=0.51\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.45\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 160252\n",
      "    [MEMORY] Elite Fraction: 0.4546773\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 79999\n",
      "    [POLICY] epsilon actual: 0.9182\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 67s 27ms/step - reward: 0.0124\n",
      "3 episodes - episode_reward: 7.667 [3.000, 12.000] - loss: 0.007 - mae: 0.035 - mean_q: 0.027 - mean_eps: 0.920 - ale.lives: 2.130\n",
      "\n",
      "Interval 33 (80000 steps performed)\n",
      "2500/2500 [==============================] - 54s 21ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 7.750 [4.000, 13.000] - loss: 0.008 - mae: 0.046 - mean_q: 0.044 - mean_eps: 0.917 - ale.lives: 2.131\n",
      "\n",
      "Interval 34 (82500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0108\n",
      ">>> 16.0\n",
      "\n",
      ">>> Step 84999 -> Avg=19.80, Max=28.00 -> [11.0, 28.0, 20.0, 16.0, 24.0] -> Mejor Actual: 16.00\n",
      ">>> Nuevo Máximo: 19.80 -> Guardado en best_model_weights_1M_GPU_Local.h5f\n",
      "[AdaptiveElite] Δ=3.80, elite_fraction=0.45\n",
      "[AdaptiveElite] Mejora → Aumento elite_fraction a 0.50\n",
      "2500/2500 [==============================] - 68s 27ms/step - reward: 0.0108\n",
      "5 episodes - episode_reward: 8.600 [6.000, 17.000] - loss: 0.006 - mae: 0.042 - mean_q: 0.039 - mean_eps: 0.914 - ale.lives: 2.134\n",
      "\n",
      "Interval 35 (85000 steps performed)\n",
      "2500/2500 [==============================] - 59s 23ms/step - reward: 0.0120\n",
      "3 episodes - episode_reward: 8.667 [0.000, 16.000] - loss: 0.007 - mae: 0.046 - mean_q: 0.043 - mean_eps: 0.912 - ale.lives: 2.372\n",
      "\n",
      "Interval 36 (87500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0176\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 89999 -> Avg=1.80, Max=2.00 -> [2.0, 2.0, 1.0, 2.0, 2.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-18.00, elite_fraction=0.50\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.45\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 180282\n",
      "    [MEMORY] Elite Fraction: 0.4501305270000001\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 89999\n",
      "    [POLICY] epsilon actual: 0.9080\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 79s 32ms/step - reward: 0.0176\n",
      "3 episodes - episode_reward: 15.333 [11.000, 20.000] - loss: 0.008 - mae: 0.047 - mean_q: 0.043 - mean_eps: 0.909 - ale.lives: 2.200\n",
      "\n",
      "Interval 37 (90000 steps performed)\n",
      "2500/2500 [==============================] - 57s 23ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 9.500 [2.000, 18.000] - loss: 0.007 - mae: 0.050 - mean_q: 0.049 - mean_eps: 0.907 - ale.lives: 2.123\n",
      "\n",
      "Interval 38 (92500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 94999 -> Avg=0.00, Max=0.00 -> [0.0, 0.0, 0.0, 0.0, 0.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-19.80, elite_fraction=0.45\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.41\n",
      "2500/2500 [==============================] - 63s 25ms/step - reward: 0.0132\n",
      "3 episodes - episode_reward: 10.667 [10.000, 12.000] - loss: 0.007 - mae: 0.051 - mean_q: 0.052 - mean_eps: 0.904 - ale.lives: 2.233\n",
      "\n",
      "Interval 39 (95000 steps performed)\n",
      "2500/2500 [==============================] - 53s 21ms/step - reward: 0.0112\n",
      "4 episodes - episode_reward: 6.750 [1.000, 12.000] - loss: 0.007 - mae: 0.055 - mean_q: 0.057 - mean_eps: 0.902 - ale.lives: 2.058\n",
      "\n",
      "Interval 40 (97500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0116\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 99999 -> Avg=15.00, Max=24.00 -> [24.0, 15.0, 7.0, 12.0, 17.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-4.80, elite_fraction=0.41\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.36\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 200312\n",
      "    [MEMORY] Elite Fraction: 0.3646057268700001\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 99999\n",
      "    [POLICY] epsilon actual: 0.8978\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 65s 26ms/step - reward: 0.0116\n",
      "4 episodes - episode_reward: 7.250 [2.000, 12.000] - loss: 0.007 - mae: 0.051 - mean_q: 0.048 - mean_eps: 0.899 - ale.lives: 2.122\n",
      "\n",
      "Interval 41 (100000 steps performed)\n",
      "2500/2500 [==============================] - 64s 25ms/step - reward: 0.0156\n",
      "4 episodes - episode_reward: 8.250 [2.000, 14.000] - loss: 0.006 - mae: 0.059 - mean_q: 0.063 - mean_eps: 0.897 - ale.lives: 2.212\n",
      "\n",
      "Interval 42 (102500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 104999 -> Avg=11.60, Max=14.00 -> [14.0, 13.0, 12.0, 13.0, 6.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-8.20, elite_fraction=0.36\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.33\n",
      "2500/2500 [==============================] - 73s 29ms/step - reward: 0.0156\n",
      "3 episodes - episode_reward: 13.333 [5.000, 23.000] - loss: 0.006 - mae: 0.058 - mean_q: 0.061 - mean_eps: 0.894 - ale.lives: 2.004\n",
      "\n",
      "Interval 43 (105000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 9.750 [6.000, 17.000] - loss: 0.007 - mae: 0.060 - mean_q: 0.063 - mean_eps: 0.891 - ale.lives: 1.964\n",
      "\n",
      "Interval 44 (107500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0136\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 109999 -> Avg=17.00, Max=19.00 -> [15.0, 19.0, 19.0, 15.0, 17.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-2.80, elite_fraction=0.33\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.30\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 220340\n",
      "    [MEMORY] Elite Fraction: 0.2953306387647001\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 109999\n",
      "    [POLICY] epsilon actual: 0.8876\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 72s 29ms/step - reward: 0.0136\n",
      "3 episodes - episode_reward: 12.667 [5.000, 21.000] - loss: 0.007 - mae: 0.060 - mean_q: 0.062 - mean_eps: 0.889 - ale.lives: 1.816\n",
      "\n",
      "Interval 45 (110000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0168\n",
      "4 episodes - episode_reward: 8.000 [0.000, 18.000] - loss: 0.007 - mae: 0.073 - mean_q: 0.084 - mean_eps: 0.886 - ale.lives: 2.347\n",
      "\n",
      "Interval 46 (112500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0136\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 114999 -> Avg=8.00, Max=10.00 -> [6.0, 10.0, 10.0, 7.0, 7.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-11.80, elite_fraction=0.30\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.27\n",
      "2500/2500 [==============================] - 69s 28ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 10.250 [8.000, 16.000] - loss: 0.007 - mae: 0.073 - mean_q: 0.086 - mean_eps: 0.884 - ale.lives: 2.320\n",
      "\n",
      "Interval 47 (115000 steps performed)\n",
      "2500/2500 [==============================] - 65s 26ms/step - reward: 0.0140\n",
      "3 episodes - episode_reward: 7.667 [3.000, 11.000] - loss: 0.005 - mae: 0.067 - mean_q: 0.077 - mean_eps: 0.881 - ale.lives: 2.197\n",
      "\n",
      "Interval 48 (117500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0144\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 119999 -> Avg=2.00, Max=3.00 -> [2.0, 3.0, 2.0, 2.0, 1.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-17.80, elite_fraction=0.27\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.24\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 240370\n",
      "    [MEMORY] Elite Fraction: 0.23921781739940712\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 119999\n",
      "    [POLICY] epsilon actual: 0.8773\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 72s 29ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 8.500 [2.000, 16.000] - loss: 0.006 - mae: 0.072 - mean_q: 0.083 - mean_eps: 0.879 - ale.lives: 2.146\n",
      "\n",
      "Interval 49 (120000 steps performed)\n",
      "2500/2500 [==============================] - 53s 21ms/step - reward: 0.0168\n",
      "3 episodes - episode_reward: 13.333 [7.000, 17.000] - loss: 0.006 - mae: 0.070 - mean_q: 0.079 - mean_eps: 0.876 - ale.lives: 2.133\n",
      "\n",
      "Interval 50 (122500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0168\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 124999 -> Avg=14.00, Max=17.00 -> [12.0, 17.0, 16.0, 11.0, 14.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-5.80, elite_fraction=0.24\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.22\n",
      "2500/2500 [==============================] - 68s 27ms/step - reward: 0.0168\n",
      "3 episodes - episode_reward: 16.333 [9.000, 24.000] - loss: 0.007 - mae: 0.073 - mean_q: 0.081 - mean_eps: 0.874 - ale.lives: 1.724\n",
      "\n",
      "Interval 51 (125000 steps performed)\n",
      "2500/2500 [==============================] - 66s 26ms/step - reward: 0.0088\n",
      "5 episodes - episode_reward: 6.200 [4.000, 12.000] - loss: 0.008 - mae: 0.074 - mean_q: 0.082 - mean_eps: 0.871 - ale.lives: 2.184\n",
      "\n",
      "Interval 52 (127500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0144\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 129999 -> Avg=14.60, Max=17.00 -> [11.0, 11.0, 17.0, 17.0, 17.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-5.20, elite_fraction=0.22\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.19\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 260400\n",
      "    [MEMORY] Elite Fraction: 0.1937664320935198\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 129999\n",
      "    [POLICY] epsilon actual: 0.8671\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 73s 29ms/step - reward: 0.0148\n",
      "4 episodes - episode_reward: 8.000 [5.000, 11.000] - loss: 0.008 - mae: 0.074 - mean_q: 0.081 - mean_eps: 0.868 - ale.lives: 2.024\n",
      "\n",
      "Interval 53 (130000 steps performed)\n",
      "2500/2500 [==============================] - 53s 21ms/step - reward: 0.0100\n",
      "5 episodes - episode_reward: 6.600 [4.000, 8.000] - loss: 0.006 - mae: 0.073 - mean_q: 0.088 - mean_eps: 0.866 - ale.lives: 1.982\n",
      "\n",
      "Interval 54 (132500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0124\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 134999 -> Avg=15.60, Max=21.00 -> [15.0, 15.0, 21.0, 12.0, 15.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-4.20, elite_fraction=0.19\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.17\n",
      "2500/2500 [==============================] - 72s 29ms/step - reward: 0.0124\n",
      "4 episodes - episode_reward: 7.750 [4.000, 15.000] - loss: 0.007 - mae: 0.076 - mean_q: 0.091 - mean_eps: 0.863 - ale.lives: 2.082\n",
      "\n",
      "Interval 55 (135000 steps performed)\n",
      "2500/2500 [==============================] - 65s 26ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 8.250 [0.000, 19.000] - loss: 0.006 - mae: 0.072 - mean_q: 0.087 - mean_eps: 0.861 - ale.lives: 2.170\n",
      "\n",
      "Interval 56 (137500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0136\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 139999 -> Avg=6.00, Max=6.00 -> [6.0, 6.0, 6.0, 6.0, 6.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-13.80, elite_fraction=0.17\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.16\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 280434\n",
      "    [MEMORY] Elite Fraction: 0.15695080999575103\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 139999\n",
      "    [POLICY] epsilon actual: 0.8569\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 74s 29ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 8.750 [4.000, 15.000] - loss: 0.006 - mae: 0.076 - mean_q: 0.091 - mean_eps: 0.858 - ale.lives: 1.824\n",
      "\n",
      "Interval 57 (140000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0120\n",
      "5 episodes - episode_reward: 5.800 [1.000, 9.000] - loss: 0.007 - mae: 0.087 - mean_q: 0.105 - mean_eps: 0.856 - ale.lives: 2.077\n",
      "\n",
      "Interval 58 (142500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0128\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 144999 -> Avg=12.00, Max=12.00 -> [12.0, 12.0, 12.0, 12.0, 12.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-7.80, elite_fraction=0.16\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 66s 26ms/step - reward: 0.0128\n",
      "4 episodes - episode_reward: 7.750 [5.000, 11.000] - loss: 0.005 - mae: 0.084 - mean_q: 0.103 - mean_eps: 0.853 - ale.lives: 1.897\n",
      "\n",
      "Interval 59 (145000 steps performed)\n",
      "2500/2500 [==============================] - 53s 21ms/step - reward: 0.0176\n",
      "4 episodes - episode_reward: 11.500 [3.000, 16.000] - loss: 0.008 - mae: 0.088 - mean_q: 0.106 - mean_eps: 0.851 - ale.lives: 2.251\n",
      "\n",
      "Interval 60 (147500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0096\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 149999 -> Avg=5.80, Max=10.00 -> [5.0, 5.0, 5.0, 10.0, 4.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-14.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 300468\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 149999\n",
      "    [POLICY] epsilon actual: 0.8467\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 71s 28ms/step - reward: 0.0096\n",
      "4 episodes - episode_reward: 6.250 [3.000, 14.000] - loss: 0.008 - mae: 0.093 - mean_q: 0.112 - mean_eps: 0.848 - ale.lives: 2.107\n",
      "\n",
      "Interval 61 (150000 steps performed)\n",
      "2500/2500 [==============================] - 66s 27ms/step - reward: 0.0120\n",
      "4 episodes - episode_reward: 5.750 [0.000, 12.000] - loss: 0.005 - mae: 0.100 - mean_q: 0.122 - mean_eps: 0.845 - ale.lives: 2.427\n",
      "\n",
      "Interval 62 (152500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0144\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 154999 -> Avg=13.20, Max=15.00 -> [15.0, 12.0, 12.0, 12.0, 15.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-6.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 74s 30ms/step - reward: 0.0144\n",
      "3 episodes - episode_reward: 11.333 [8.000, 13.000] - loss: 0.008 - mae: 0.103 - mean_q: 0.126 - mean_eps: 0.843 - ale.lives: 2.000\n",
      "\n",
      "Interval 63 (155000 steps performed)\n",
      "2500/2500 [==============================] - 55s 22ms/step - reward: 0.0140\n",
      "3 episodes - episode_reward: 11.000 [9.000, 13.000] - loss: 0.008 - mae: 0.104 - mean_q: 0.128 - mean_eps: 0.840 - ale.lives: 1.902\n",
      "\n",
      "Interval 64 (157500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0180\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 159999 -> Avg=15.20, Max=28.00 -> [28.0, 8.0, 12.0, 17.0, 11.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-4.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 320494\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 159999\n",
      "    [POLICY] epsilon actual: 0.8364\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 0.0180\n",
      "3 episodes - episode_reward: 15.333 [11.000, 18.000] - loss: 0.006 - mae: 0.103 - mean_q: 0.127 - mean_eps: 0.838 - ale.lives: 2.221\n",
      "\n",
      "Interval 65 (160000 steps performed)\n",
      "2500/2500 [==============================] - 66s 26ms/step - reward: 0.0172\n",
      "4 episodes - episode_reward: 12.250 [10.000, 16.000] - loss: 0.007 - mae: 0.107 - mean_q: 0.131 - mean_eps: 0.835 - ale.lives: 2.300\n",
      "\n",
      "Interval 66 (162500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0164\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 164999 -> Avg=8.20, Max=11.00 -> [6.0, 6.0, 8.0, 11.0, 10.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-11.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 81s 32ms/step - reward: 0.0164\n",
      "2 episodes - episode_reward: 15.000 [8.000, 22.000] - loss: 0.007 - mae: 0.107 - mean_q: 0.129 - mean_eps: 0.833 - ale.lives: 1.924\n",
      "\n",
      "Interval 67 (165000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0180\n",
      "3 episodes - episode_reward: 16.333 [10.000, 24.000] - loss: 0.008 - mae: 0.104 - mean_q: 0.126 - mean_eps: 0.830 - ale.lives: 2.086\n",
      "\n",
      "Interval 68 (167500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0092\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 169999 -> Avg=7.20, Max=11.00 -> [6.0, 6.0, 7.0, 6.0, 11.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-12.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 340516\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 169999\n",
      "    [POLICY] epsilon actual: 0.8262\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 64s 26ms/step - reward: 0.0092\n",
      "2 episodes - episode_reward: 13.000 [12.000, 14.000] - loss: 0.011 - mae: 0.112 - mean_q: 0.132 - mean_eps: 0.828 - ale.lives: 1.761\n",
      "\n",
      "Interval 69 (170000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0112\n",
      "4 episodes - episode_reward: 7.500 [5.000, 10.000] - loss: 0.007 - mae: 0.123 - mean_q: 0.150 - mean_eps: 0.825 - ale.lives: 2.247\n",
      "\n",
      "Interval 70 (172500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0172\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 174999 -> Avg=10.00, Max=16.00 -> [16.0, 5.0, 8.0, 5.0, 16.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-9.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 73s 29ms/step - reward: 0.0172\n",
      "3 episodes - episode_reward: 16.333 [8.000, 22.000] - loss: 0.006 - mae: 0.119 - mean_q: 0.145 - mean_eps: 0.822 - ale.lives: 2.258\n",
      "\n",
      "Interval 71 (175000 steps performed)\n",
      "2500/2500 [==============================] - 66s 26ms/step - reward: 0.0116\n",
      "4 episodes - episode_reward: 4.750 [0.000, 10.000] - loss: 0.007 - mae: 0.125 - mean_q: 0.151 - mean_eps: 0.820 - ale.lives: 2.303\n",
      "\n",
      "Interval 72 (177500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0184\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 179999 -> Avg=6.20, Max=8.00 -> [8.0, 6.0, 7.0, 4.0, 6.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-13.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 360544\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 179999\n",
      "    [POLICY] epsilon actual: 0.8160\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 79s 32ms/step - reward: 0.0184\n",
      "3 episodes - episode_reward: 16.333 [12.000, 24.000] - loss: 0.006 - mae: 0.119 - mean_q: 0.143 - mean_eps: 0.817 - ale.lives: 2.118\n",
      "\n",
      "Interval 73 (180000 steps performed)\n",
      "2500/2500 [==============================] - 59s 23ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 11.250 [7.000, 20.000] - loss: 0.008 - mae: 0.144 - mean_q: 0.174 - mean_eps: 0.815 - ale.lives: 2.054\n",
      "\n",
      "Interval 74 (182500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 184999 -> Avg=10.80, Max=21.00 -> [21.0, 7.0, 12.0, 7.0, 7.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-9.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 67s 27ms/step - reward: 0.0156\n",
      "3 episodes - episode_reward: 13.000 [5.000, 17.000] - loss: 0.009 - mae: 0.144 - mean_q: 0.173 - mean_eps: 0.812 - ale.lives: 2.482\n",
      "\n",
      "Interval 75 (185000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0148\n",
      "4 episodes - episode_reward: 7.750 [0.000, 12.000] - loss: 0.005 - mae: 0.138 - mean_q: 0.168 - mean_eps: 0.810 - ale.lives: 2.187\n",
      "\n",
      "Interval 76 (187500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0168\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 189999 -> Avg=10.20, Max=19.00 -> [1.0, 4.0, 12.0, 19.0, 15.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-9.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 380574\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 189999\n",
      "    [POLICY] epsilon actual: 0.8058\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 77s 31ms/step - reward: 0.0168\n",
      "4 episodes - episode_reward: 8.500 [2.000, 21.000] - loss: 0.008 - mae: 0.142 - mean_q: 0.170 - mean_eps: 0.807 - ale.lives: 2.402\n",
      "\n",
      "Interval 77 (190000 steps performed)\n",
      "2500/2500 [==============================] - 66s 27ms/step - reward: 0.0120\n",
      "3 episodes - episode_reward: 11.667 [5.000, 16.000] - loss: 0.008 - mae: 0.172 - mean_q: 0.210 - mean_eps: 0.805 - ale.lives: 1.921\n",
      "\n",
      "Interval 78 (192500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0200\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 194999 -> Avg=11.00, Max=13.00 -> [13.0, 12.0, 13.0, 9.0, 8.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-8.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 75s 30ms/step - reward: 0.0204\n",
      "3 episodes - episode_reward: 13.000 [9.000, 16.000] - loss: 0.006 - mae: 0.168 - mean_q: 0.205 - mean_eps: 0.802 - ale.lives: 2.055\n",
      "\n",
      "Interval 79 (195000 steps performed)\n",
      "2500/2500 [==============================] - 55s 22ms/step - reward: 0.0176\n",
      "4 episodes - episode_reward: 16.250 [11.000, 21.000] - loss: 0.008 - mae: 0.165 - mean_q: 0.201 - mean_eps: 0.799 - ale.lives: 1.767\n",
      "\n",
      "Interval 80 (197500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 199999 -> Avg=8.60, Max=11.00 -> [11.0, 6.0, 9.0, 6.0, 11.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-11.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 400600\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 199999\n",
      "    [POLICY] epsilon actual: 0.7956\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 68s 27ms/step - reward: 0.0156\n",
      "3 episodes - episode_reward: 10.667 [4.000, 16.000] - loss: 0.007 - mae: 0.168 - mean_q: 0.204 - mean_eps: 0.797 - ale.lives: 2.078\n",
      "\n",
      "Interval 81 (200000 steps performed)\n",
      "2500/2500 [==============================] - 65s 26ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 8.250 [6.000, 13.000] - loss: 0.006 - mae: 0.175 - mean_q: 0.213 - mean_eps: 0.794 - ale.lives: 1.943\n",
      "\n",
      "Interval 82 (202500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0160\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 204999 -> Avg=7.60, Max=8.00 -> [6.0, 8.0, 8.0, 8.0, 8.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-12.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 79s 32ms/step - reward: 0.0160\n",
      "2 episodes - episode_reward: 18.000 [10.000, 26.000] - loss: 0.008 - mae: 0.175 - mean_q: 0.212 - mean_eps: 0.792 - ale.lives: 2.625\n",
      "\n",
      "Interval 83 (205000 steps performed)\n",
      "2500/2500 [==============================] - 57s 23ms/step - reward: 0.0128\n",
      "4 episodes - episode_reward: 9.500 [6.000, 14.000] - loss: 0.008 - mae: 0.177 - mean_q: 0.217 - mean_eps: 0.789 - ale.lives: 2.218\n",
      "\n",
      "Interval 84 (207500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0164\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 209999 -> Avg=1.80, Max=3.00 -> [1.0, 3.0, 2.0, 1.0, 2.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-18.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 420628\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 209999\n",
      "    [POLICY] epsilon actual: 0.7853\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 65s 26ms/step - reward: 0.0164\n",
      "4 episodes - episode_reward: 10.500 [5.000, 16.000] - loss: 0.008 - mae: 0.178 - mean_q: 0.218 - mean_eps: 0.787 - ale.lives: 2.084\n",
      "\n",
      "Interval 85 (210000 steps performed)\n",
      "2500/2500 [==============================] - 63s 25ms/step - reward: 0.0100\n",
      "5 episodes - episode_reward: 6.000 [4.000, 9.000] - loss: 0.009 - mae: 0.196 - mean_q: 0.239 - mean_eps: 0.784 - ale.lives: 2.156\n",
      "\n",
      "Interval 86 (212500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0200\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 214999 -> Avg=15.20, Max=16.00 -> [16.0, 16.0, 12.0, 16.0, 16.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-4.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 82s 33ms/step - reward: 0.0200\n",
      "2 episodes - episode_reward: 19.500 [10.000, 29.000] - loss: 0.007 - mae: 0.192 - mean_q: 0.234 - mean_eps: 0.782 - ale.lives: 1.927\n",
      "\n",
      "Interval 87 (215000 steps performed)\n",
      "2500/2500 [==============================] - 64s 26ms/step - reward: 0.0108\n",
      "4 episodes - episode_reward: 9.500 [6.000, 13.000] - loss: 0.007 - mae: 0.187 - mean_q: 0.226 - mean_eps: 0.779 - ale.lives: 2.148\n",
      "\n",
      "Interval 88 (217500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0180\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 219999 -> Avg=12.60, Max=16.00 -> [10.0, 14.0, 11.0, 16.0, 12.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-7.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 440658\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 219999\n",
      "    [POLICY] epsilon actual: 0.7751\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 66s 27ms/step - reward: 0.0180\n",
      "4 episodes - episode_reward: 8.750 [4.000, 17.000] - loss: 0.007 - mae: 0.196 - mean_q: 0.240 - mean_eps: 0.776 - ale.lives: 2.175\n",
      "\n",
      "Interval 89 (220000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0220\n",
      "3 episodes - episode_reward: 18.333 [12.000, 24.000] - loss: 0.006 - mae: 0.203 - mean_q: 0.248 - mean_eps: 0.774 - ale.lives: 2.269\n",
      "\n",
      "Interval 90 (222500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0160\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 224999 -> Avg=16.40, Max=24.00 -> [15.0, 6.0, 19.0, 24.0, 18.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-3.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 71s 28ms/step - reward: 0.0160\n",
      "5 episodes - episode_reward: 9.200 [4.000, 14.000] - loss: 0.007 - mae: 0.209 - mean_q: 0.256 - mean_eps: 0.771 - ale.lives: 2.127\n",
      "\n",
      "Interval 91 (225000 steps performed)\n",
      "2500/2500 [==============================] - 66s 26ms/step - reward: 0.0116\n",
      "4 episodes - episode_reward: 7.250 [6.000, 11.000] - loss: 0.008 - mae: 0.205 - mean_q: 0.250 - mean_eps: 0.769 - ale.lives: 2.245\n",
      "\n",
      "Interval 92 (227500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0124\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 229999 -> Avg=17.60, Max=28.00 -> [12.0, 16.0, 16.0, 28.0, 16.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-2.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 460690\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 229999\n",
      "    [POLICY] epsilon actual: 0.7649\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 84s 34ms/step - reward: 0.0124\n",
      "4 episodes - episode_reward: 7.500 [6.000, 10.000] - loss: 0.008 - mae: 0.209 - mean_q: 0.258 - mean_eps: 0.766 - ale.lives: 2.030\n",
      "\n",
      "Interval 93 (230000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0108\n",
      "4 episodes - episode_reward: 6.500 [3.000, 10.000] - loss: 0.007 - mae: 0.246 - mean_q: 0.303 - mean_eps: 0.764 - ale.lives: 2.136\n",
      "\n",
      "Interval 94 (232500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0160\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 234999 -> Avg=14.60, Max=26.00 -> [19.0, 7.0, 7.0, 26.0, 14.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-5.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 65s 26ms/step - reward: 0.0160\n",
      "4 episodes - episode_reward: 9.500 [4.000, 14.000] - loss: 0.009 - mae: 0.243 - mean_q: 0.297 - mean_eps: 0.761 - ale.lives: 2.099\n",
      "\n",
      "Interval 95 (235000 steps performed)\n",
      "2500/2500 [==============================] - 65s 26ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 10.000 [4.000, 14.000] - loss: 0.006 - mae: 0.242 - mean_q: 0.294 - mean_eps: 0.759 - ale.lives: 2.313\n",
      "\n",
      "Interval 96 (237500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0128\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 239999 -> Avg=2.00, Max=2.00 -> [2.0, 2.0, 2.0, 2.0, 2.0] -> Mejor Actual: 19.80\n",
      "[AdaptiveElite] Δ=-17.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 480720\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 239999\n",
      "    [POLICY] epsilon actual: 0.7547\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 78s 31ms/step - reward: 0.0128\n",
      "3 episodes - episode_reward: 11.000 [11.000, 11.000] - loss: 0.008 - mae: 0.237 - mean_q: 0.290 - mean_eps: 0.756 - ale.lives: 2.146\n",
      "\n",
      "Interval 97 (240000 steps performed)\n",
      "2500/2500 [==============================] - 62s 25ms/step - reward: 0.0152\n",
      "3 episodes - episode_reward: 12.000 [4.000, 20.000] - loss: 0.005 - mae: 0.250 - mean_q: 0.306 - mean_eps: 0.753 - ale.lives: 2.410\n",
      "\n",
      "Interval 98 (242500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0188\n",
      ">>> 19.8\n",
      "\n",
      ">>> Step 244999 -> Avg=20.80, Max=25.00 -> [22.0, 25.0, 25.0, 17.0, 15.0] -> Mejor Actual: 19.80\n",
      ">>> Nuevo Máximo: 20.80 -> Guardado en best_model_weights_1M_GPU_Local.h5f\n",
      "[AdaptiveElite] Δ=1.00, elite_fraction=0.15\n",
      "2500/2500 [==============================] - 73s 29ms/step - reward: 0.0188\n",
      "2 episodes - episode_reward: 17.000 [10.000, 24.000] - loss: 0.008 - mae: 0.252 - mean_q: 0.306 - mean_eps: 0.751 - ale.lives: 2.174\n",
      "\n",
      "Interval 99 (245000 steps performed)\n",
      "2500/2500 [==============================] - 59s 24ms/step - reward: 0.0136\n",
      "5 episodes - episode_reward: 10.400 [7.000, 19.000] - loss: 0.009 - mae: 0.259 - mean_q: 0.313 - mean_eps: 0.748 - ale.lives: 2.190\n",
      "\n",
      "Interval 100 (247500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0148\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 249999 -> Avg=16.80, Max=18.00 -> [18.0, 16.0, 18.0, 16.0, 16.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-4.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 500748\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 249999\n",
      "    [POLICY] epsilon actual: 0.7444\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 85s 34ms/step - reward: 0.0148\n",
      "4 episodes - episode_reward: 8.250 [6.000, 12.000] - loss: 0.006 - mae: 0.247 - mean_q: 0.301 - mean_eps: 0.746 - ale.lives: 2.237\n",
      "\n",
      "Interval 101 (250000 steps performed)\n",
      "2500/2500 [==============================] - 65s 26ms/step - reward: 0.0180\n",
      "4 episodes - episode_reward: 12.500 [2.000, 24.000] - loss: 0.009 - mae: 0.263 - mean_q: 0.321 - mean_eps: 0.743 - ale.lives: 2.165\n",
      "\n",
      "Interval 102 (252500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0148\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 254999 -> Avg=0.20, Max=1.00 -> [0.0, 0.0, 0.0, 1.0, 0.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-20.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 61s 24ms/step - reward: 0.0148\n",
      "4 episodes - episode_reward: 9.000 [5.000, 12.000] - loss: 0.009 - mae: 0.255 - mean_q: 0.309 - mean_eps: 0.741 - ale.lives: 2.152\n",
      "\n",
      "Interval 103 (255000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0148\n",
      "5 episodes - episode_reward: 7.600 [1.000, 12.000] - loss: 0.009 - mae: 0.258 - mean_q: 0.311 - mean_eps: 0.738 - ale.lives: 2.304\n",
      "\n",
      "Interval 104 (257500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0192\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 259999 -> Avg=10.40, Max=15.00 -> [4.0, 13.0, 12.0, 8.0, 15.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-10.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 520780\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 259999\n",
      "    [POLICY] epsilon actual: 0.7342\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 65s 26ms/step - reward: 0.0192\n",
      "3 episodes - episode_reward: 12.667 [5.000, 17.000] - loss: 0.008 - mae: 0.261 - mean_q: 0.316 - mean_eps: 0.736 - ale.lives: 1.750\n",
      "\n",
      "Interval 105 (260000 steps performed)\n",
      "2500/2500 [==============================] - 63s 25ms/step - reward: 0.0120\n",
      "5 episodes - episode_reward: 7.600 [2.000, 17.000] - loss: 0.009 - mae: 0.284 - mean_q: 0.343 - mean_eps: 0.733 - ale.lives: 2.184\n",
      "\n",
      "Interval 106 (262500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0140\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 264999 -> Avg=16.20, Max=27.00 -> [8.0, 19.0, 18.0, 9.0, 27.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-4.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 86s 34ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 9.250 [7.000, 12.000] - loss: 0.009 - mae: 0.279 - mean_q: 0.337 - mean_eps: 0.730 - ale.lives: 2.082\n",
      "\n",
      "Interval 107 (265000 steps performed)\n",
      "2500/2500 [==============================] - 67s 27ms/step - reward: 0.0172\n",
      "4 episodes - episode_reward: 10.500 [0.000, 18.000] - loss: 0.011 - mae: 0.289 - mean_q: 0.349 - mean_eps: 0.728 - ale.lives: 2.214\n",
      "\n",
      "Interval 108 (267500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0140\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 269999 -> Avg=20.60, Max=35.00 -> [31.0, 7.0, 15.0, 35.0, 15.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-0.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 540814\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 269999\n",
      "    [POLICY] epsilon actual: 0.7240\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 74s 29ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 7.750 [4.000, 13.000] - loss: 0.007 - mae: 0.286 - mean_q: 0.345 - mean_eps: 0.725 - ale.lives: 2.038\n",
      "\n",
      "Interval 109 (270000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0132\n",
      "4 episodes - episode_reward: 8.000 [5.000, 12.000] - loss: 0.010 - mae: 0.293 - mean_q: 0.355 - mean_eps: 0.723 - ale.lives: 2.059\n",
      "\n",
      "Interval 110 (272500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0196\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 274999 -> Avg=9.80, Max=16.00 -> [10.0, 5.0, 12.0, 16.0, 6.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-11.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 78s 31ms/step - reward: 0.0196\n",
      "4 episodes - episode_reward: 12.250 [6.000, 19.000] - loss: 0.008 - mae: 0.301 - mean_q: 0.366 - mean_eps: 0.720 - ale.lives: 1.956\n",
      "\n",
      "Interval 111 (275000 steps performed)\n",
      "2500/2500 [==============================] - 64s 26ms/step - reward: 0.0120\n",
      "5 episodes - episode_reward: 7.200 [4.000, 15.000] - loss: 0.009 - mae: 0.296 - mean_q: 0.358 - mean_eps: 0.718 - ale.lives: 2.003\n",
      "\n",
      "Interval 112 (277500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0184\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 279999 -> Avg=18.80, Max=21.00 -> [16.0, 17.0, 21.0, 19.0, 21.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-2.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 560848\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 279999\n",
      "    [POLICY] epsilon actual: 0.7138\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 69s 27ms/step - reward: 0.0184\n",
      "4 episodes - episode_reward: 11.500 [4.000, 22.000] - loss: 0.008 - mae: 0.290 - mean_q: 0.353 - mean_eps: 0.715 - ale.lives: 1.742\n",
      "\n",
      "Interval 113 (280000 steps performed)\n",
      "2500/2500 [==============================] - 57s 23ms/step - reward: 0.0180\n",
      "4 episodes - episode_reward: 9.750 [0.000, 22.000] - loss: 0.010 - mae: 0.307 - mean_q: 0.373 - mean_eps: 0.713 - ale.lives: 2.099\n",
      "\n",
      "Interval 114 (282500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0184\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 284999 -> Avg=16.80, Max=19.00 -> [19.0, 16.0, 18.0, 12.0, 19.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-4.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 83s 33ms/step - reward: 0.0184\n",
      "2 episodes - episode_reward: 18.000 [18.000, 18.000] - loss: 0.008 - mae: 0.301 - mean_q: 0.367 - mean_eps: 0.710 - ale.lives: 2.212\n",
      "\n",
      "Interval 115 (285000 steps performed)\n",
      "2500/2500 [==============================] - 71s 28ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 13.000 [5.000, 16.000] - loss: 0.010 - mae: 0.310 - mean_q: 0.374 - mean_eps: 0.707 - ale.lives: 2.428\n",
      "\n",
      "Interval 116 (287500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0144\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 289999 -> Avg=15.00, Max=23.00 -> [14.0, 14.0, 14.0, 10.0, 23.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-5.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 580876\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 289999\n",
      "    [POLICY] epsilon actual: 0.7036\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 76s 30ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 9.500 [2.000, 14.000] - loss: 0.010 - mae: 0.301 - mean_q: 0.365 - mean_eps: 0.705 - ale.lives: 1.932\n",
      "\n",
      "Interval 117 (290000 steps performed)\n",
      "2500/2500 [==============================] - 56s 22ms/step - reward: 0.0152\n",
      "3 episodes - episode_reward: 11.000 [0.000, 20.000] - loss: 0.010 - mae: 0.319 - mean_q: 0.386 - mean_eps: 0.702 - ale.lives: 2.141\n",
      "\n",
      "Interval 118 (292500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0168\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 294999 -> Avg=18.00, Max=18.00 -> [18.0, 18.0, 18.0, 18.0, 18.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-2.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 82s 33ms/step - reward: 0.0168\n",
      "3 episodes - episode_reward: 10.333 [4.000, 18.000] - loss: 0.009 - mae: 0.317 - mean_q: 0.383 - mean_eps: 0.700 - ale.lives: 1.966\n",
      "\n",
      "Interval 119 (295000 steps performed)\n",
      "2500/2500 [==============================] - 67s 27ms/step - reward: 0.0188\n",
      "3 episodes - episode_reward: 16.000 [16.000, 16.000] - loss: 0.010 - mae: 0.324 - mean_q: 0.393 - mean_eps: 0.697 - ale.lives: 2.101\n",
      "\n",
      "Interval 120 (297500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0124\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 299999 -> Avg=2.60, Max=5.00 -> [5.0, 0.0, 0.0, 5.0, 3.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-18.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 600900\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 299999\n",
      "    [POLICY] epsilon actual: 0.6933\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 82s 33ms/step - reward: 0.0124\n",
      "3 episodes - episode_reward: 13.667 [5.000, 23.000] - loss: 0.010 - mae: 0.327 - mean_q: 0.395 - mean_eps: 0.695 - ale.lives: 1.862\n",
      "\n",
      "Interval 121 (300000 steps performed)\n",
      "2500/2500 [==============================] - 68s 27ms/step - reward: 0.0128\n",
      "4 episodes - episode_reward: 7.500 [5.000, 13.000] - loss: 0.011 - mae: 0.329 - mean_q: 0.395 - mean_eps: 0.692 - ale.lives: 2.022\n",
      "\n",
      "Interval 122 (302500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 304999 -> Avg=1.80, Max=3.00 -> [0.0, 1.0, 3.0, 2.0, 3.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-19.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 64s 26ms/step - reward: 0.0132\n",
      "5 episodes - episode_reward: 8.000 [5.000, 12.000] - loss: 0.010 - mae: 0.317 - mean_q: 0.383 - mean_eps: 0.690 - ale.lives: 2.005\n",
      "\n",
      "Interval 123 (305000 steps performed)\n",
      "2500/2500 [==============================] - 54s 22ms/step - reward: 0.0164\n",
      "3 episodes - episode_reward: 10.000 [0.000, 27.000] - loss: 0.007 - mae: 0.313 - mean_q: 0.378 - mean_eps: 0.687 - ale.lives: 1.622\n",
      "\n",
      "Interval 124 (307500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0164\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 309999 -> Avg=16.00, Max=16.00 -> [16.0, 16.0, 16.0, 16.0, 16.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-4.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 620934\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 309999\n",
      "    [POLICY] epsilon actual: 0.6831\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 83s 33ms/step - reward: 0.0164\n",
      "5 episodes - episode_reward: 10.400 [5.000, 18.000] - loss: 0.010 - mae: 0.318 - mean_q: 0.384 - mean_eps: 0.684 - ale.lives: 2.036\n",
      "\n",
      "Interval 125 (310000 steps performed)\n",
      "2500/2500 [==============================] - 67s 27ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 9.250 [0.000, 16.000] - loss: 0.010 - mae: 0.336 - mean_q: 0.406 - mean_eps: 0.682 - ale.lives: 2.220\n",
      "\n",
      "Interval 126 (312500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0120\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 314999 -> Avg=2.20, Max=3.00 -> [2.0, 2.0, 2.0, 3.0, 2.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-18.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 75s 30ms/step - reward: 0.0120\n",
      "4 episodes - episode_reward: 6.250 [4.000, 9.000] - loss: 0.010 - mae: 0.328 - mean_q: 0.394 - mean_eps: 0.679 - ale.lives: 1.982\n",
      "\n",
      "Interval 127 (315000 steps performed)\n",
      "2500/2500 [==============================] - 58s 23ms/step - reward: 0.0172\n",
      "4 episodes - episode_reward: 10.250 [6.000, 14.000] - loss: 0.008 - mae: 0.325 - mean_q: 0.392 - mean_eps: 0.677 - ale.lives: 2.094\n",
      "\n",
      "Interval 128 (317500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0140\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 319999 -> Avg=1.80, Max=2.00 -> [2.0, 1.0, 2.0, 2.0, 2.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-19.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 640968\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 319999\n",
      "    [POLICY] epsilon actual: 0.6729\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 64s 26ms/step - reward: 0.0140\n",
      "5 episodes - episode_reward: 8.600 [5.000, 12.000] - loss: 0.008 - mae: 0.324 - mean_q: 0.391 - mean_eps: 0.674 - ale.lives: 2.182\n",
      "\n",
      "Interval 129 (320000 steps performed)\n",
      "2500/2500 [==============================] - 66s 26ms/step - reward: 0.0124\n",
      "2 episodes - episode_reward: 7.000 [0.000, 14.000] - loss: 0.009 - mae: 0.341 - mean_q: 0.412 - mean_eps: 0.672 - ale.lives: 2.070\n",
      "\n",
      "Interval 130 (322500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 324999 -> Avg=13.00, Max=22.00 -> [7.0, 13.0, 6.0, 17.0, 22.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-7.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 81s 33ms/step - reward: 0.0156\n",
      "3 episodes - episode_reward: 17.333 [16.000, 19.000] - loss: 0.008 - mae: 0.339 - mean_q: 0.407 - mean_eps: 0.669 - ale.lives: 1.988\n",
      "\n",
      "Interval 131 (325000 steps performed)\n",
      "2500/2500 [==============================] - 64s 25ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 7.000 [4.000, 10.000] - loss: 0.008 - mae: 0.329 - mean_q: 0.398 - mean_eps: 0.667 - ale.lives: 2.282\n",
      "\n",
      "Interval 132 (327500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0144\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 329999 -> Avg=0.00, Max=0.00 -> [0.0, 0.0, 0.0, 0.0, 0.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-20.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 660994\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 32\n",
      "    [TRAIN] Step actual: 329999\n",
      "    [POLICY] epsilon actual: 0.6627\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 64s 25ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 10.000 [6.000, 18.000] - loss: 0.009 - mae: 0.338 - mean_q: 0.408 - mean_eps: 0.664 - ale.lives: 1.932\n",
      "\n",
      "Interval 133 (330000 steps performed)\n",
      "2500/2500 [==============================] - 60s 24ms/step - reward: 0.0156\n",
      "4 episodes - episode_reward: 11.250 [5.000, 22.000] - loss: 0.012 - mae: 0.390 - mean_q: 0.472 - mean_eps: 0.661 - ale.lives: 2.208\n",
      "\n",
      "Interval 134 (332500 steps performed)\n",
      " 833/2500 [========>.....................] - ETA: 48s - reward: 0.0084[BATCH] Step 333333: batch_size actualizado a 64\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0128\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 334999 -> Avg=12.00, Max=12.00 -> [12.0, 12.0, 12.0, 12.0, 12.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-8.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 112s 45ms/step - reward: 0.0128\n",
      "3 episodes - episode_reward: 9.000 [7.000, 11.000] - loss: 0.009 - mae: 0.380 - mean_q: 0.461 - mean_eps: 0.659 - ale.lives: 2.162\n",
      "\n",
      "Interval 135 (335000 steps performed)\n",
      "2500/2500 [==============================] - 109s 44ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 10.250 [6.000, 16.000] - loss: 0.009 - mae: 0.382 - mean_q: 0.460 - mean_eps: 0.656 - ale.lives: 2.076\n",
      "\n",
      "Interval 136 (337500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 339999 -> Avg=4.00, Max=8.00 -> [1.0, 6.0, 1.0, 8.0, 4.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-16.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 681022\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 339999\n",
      "    [POLICY] epsilon actual: 0.6524\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 114s 45ms/step - reward: 0.0132\n",
      "3 episodes - episode_reward: 9.333 [1.000, 17.000] - loss: 0.010 - mae: 0.375 - mean_q: 0.450 - mean_eps: 0.654 - ale.lives: 1.879\n",
      "\n",
      "Interval 137 (340000 steps performed)\n",
      "2500/2500 [==============================] - 87s 35ms/step - reward: 0.0160\n",
      "3 episodes - episode_reward: 14.333 [5.000, 20.000] - loss: 0.010 - mae: 0.395 - mean_q: 0.475 - mean_eps: 0.651 - ale.lives: 1.900\n",
      "\n",
      "Interval 138 (342500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0140\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 344999 -> Avg=8.00, Max=14.00 -> [6.0, 11.0, 3.0, 6.0, 14.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-12.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 116s 46ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 8.000 [6.000, 10.000] - loss: 0.009 - mae: 0.392 - mean_q: 0.472 - mean_eps: 0.649 - ale.lives: 2.043\n",
      "\n",
      "Interval 139 (345000 steps performed)\n",
      "2500/2500 [==============================] - 92s 37ms/step - reward: 0.0168\n",
      "5 episodes - episode_reward: 8.400 [4.000, 18.000] - loss: 0.009 - mae: 0.393 - mean_q: 0.471 - mean_eps: 0.646 - ale.lives: 2.062\n",
      "\n",
      "Interval 140 (347500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0144\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 349999 -> Avg=6.00, Max=11.00 -> [4.0, 11.0, 7.0, 6.0, 2.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-14.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 701052\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 349999\n",
      "    [POLICY] epsilon actual: 0.6422\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 98s 39ms/step - reward: 0.0144\n",
      "3 episodes - episode_reward: 11.667 [6.000, 15.000] - loss: 0.010 - mae: 0.392 - mean_q: 0.471 - mean_eps: 0.644 - ale.lives: 2.128\n",
      "\n",
      "Interval 141 (350000 steps performed)\n",
      "2500/2500 [==============================] - 87s 35ms/step - reward: 0.0188\n",
      "3 episodes - episode_reward: 17.667 [6.000, 26.000] - loss: 0.009 - mae: 0.403 - mean_q: 0.487 - mean_eps: 0.641 - ale.lives: 2.246\n",
      "\n",
      "Interval 142 (352500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0136\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 354999 -> Avg=14.20, Max=18.00 -> [18.0, 18.0, 7.0, 16.0, 12.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-6.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 105s 42ms/step - reward: 0.0136\n",
      "3 episodes - episode_reward: 9.667 [7.000, 13.000] - loss: 0.010 - mae: 0.403 - mean_q: 0.484 - mean_eps: 0.638 - ale.lives: 2.077\n",
      "\n",
      "Interval 143 (355000 steps performed)\n",
      "2500/2500 [==============================] - 85s 34ms/step - reward: 0.0180\n",
      "4 episodes - episode_reward: 8.750 [3.000, 18.000] - loss: 0.009 - mae: 0.401 - mean_q: 0.482 - mean_eps: 0.636 - ale.lives: 2.488\n",
      "\n",
      "Interval 144 (357500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 359999 -> Avg=18.00, Max=29.00 -> [9.0, 16.0, 29.0, 14.0, 22.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-2.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 721080\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 359999\n",
      "    [POLICY] epsilon actual: 0.6320\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 99s 40ms/step - reward: 0.0156\n",
      "4 episodes - episode_reward: 12.250 [5.000, 24.000] - loss: 0.009 - mae: 0.407 - mean_q: 0.489 - mean_eps: 0.633 - ale.lives: 2.041\n",
      "\n",
      "Interval 145 (360000 steps performed)\n",
      "2500/2500 [==============================] - 86s 34ms/step - reward: 0.0204\n",
      "4 episodes - episode_reward: 12.250 [5.000, 21.000] - loss: 0.008 - mae: 0.406 - mean_q: 0.489 - mean_eps: 0.631 - ale.lives: 2.032\n",
      "\n",
      "Interval 146 (362500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0124\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 364999 -> Avg=4.80, Max=7.00 -> [4.0, 3.0, 7.0, 3.0, 7.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-16.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 101s 40ms/step - reward: 0.0124\n",
      "4 episodes - episode_reward: 8.000 [5.000, 11.000] - loss: 0.010 - mae: 0.413 - mean_q: 0.496 - mean_eps: 0.628 - ale.lives: 1.881\n",
      "\n",
      "Interval 147 (365000 steps performed)\n",
      "2500/2500 [==============================] - 86s 34ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 6.750 [3.000, 11.000] - loss: 0.012 - mae: 0.421 - mean_q: 0.507 - mean_eps: 0.626 - ale.lives: 2.080\n",
      "\n",
      "Interval 148 (367500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0172\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 369999 -> Avg=10.20, Max=14.00 -> [3.0, 13.0, 14.0, 12.0, 9.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-10.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 741112\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 369999\n",
      "    [POLICY] epsilon actual: 0.6218\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 101s 40ms/step - reward: 0.0172\n",
      "4 episodes - episode_reward: 14.500 [9.000, 23.000] - loss: 0.009 - mae: 0.421 - mean_q: 0.507 - mean_eps: 0.623 - ale.lives: 1.934\n",
      "\n",
      "Interval 149 (370000 steps performed)\n",
      "2500/2500 [==============================] - 85s 34ms/step - reward: 0.0136\n",
      "3 episodes - episode_reward: 9.333 [0.000, 18.000] - loss: 0.009 - mae: 0.417 - mean_q: 0.501 - mean_eps: 0.621 - ale.lives: 2.431\n",
      "\n",
      "Interval 150 (372500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 374999 -> Avg=16.80, Max=28.00 -> [14.0, 11.0, 18.0, 28.0, 13.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-4.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 123s 49ms/step - reward: 0.0132\n",
      "4 episodes - episode_reward: 9.750 [6.000, 12.000] - loss: 0.011 - mae: 0.427 - mean_q: 0.515 - mean_eps: 0.618 - ale.lives: 2.201\n",
      "\n",
      "Interval 151 (375000 steps performed)\n",
      "2500/2500 [==============================] - 97s 39ms/step - reward: 0.0112\n",
      "3 episodes - episode_reward: 7.000 [0.000, 11.000] - loss: 0.009 - mae: 0.421 - mean_q: 0.506 - mean_eps: 0.615 - ale.lives: 2.012\n",
      "\n",
      "Interval 152 (377500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0084\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 379999 -> Avg=16.40, Max=19.00 -> [16.0, 19.0, 13.0, 16.0, 18.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-4.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 761138\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 379999\n",
      "    [POLICY] epsilon actual: 0.6116\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 98s 39ms/step - reward: 0.0088\n",
      "3 episodes - episode_reward: 6.667 [3.000, 10.000] - loss: 0.011 - mae: 0.420 - mean_q: 0.504 - mean_eps: 0.613 - ale.lives: 2.206\n",
      "\n",
      "Interval 153 (380000 steps performed)\n",
      "2500/2500 [==============================] - 108s 43ms/step - reward: 0.0116\n",
      "3 episodes - episode_reward: 11.000 [9.000, 14.000] - loss: 0.009 - mae: 0.425 - mean_q: 0.512 - mean_eps: 0.610 - ale.lives: 2.074\n",
      "\n",
      "Interval 154 (382500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0112\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 384999 -> Avg=9.20, Max=12.00 -> [12.0, 10.0, 8.0, 9.0, 7.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-11.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 124s 49ms/step - reward: 0.0112\n",
      "4 episodes - episode_reward: 8.250 [4.000, 11.000] - loss: 0.007 - mae: 0.421 - mean_q: 0.507 - mean_eps: 0.608 - ale.lives: 1.921\n",
      "\n",
      "Interval 155 (385000 steps performed)\n",
      "2500/2500 [==============================] - 87s 35ms/step - reward: 0.0124\n",
      "5 episodes - episode_reward: 5.400 [0.000, 11.000] - loss: 0.011 - mae: 0.429 - mean_q: 0.516 - mean_eps: 0.605 - ale.lives: 2.391\n",
      "\n",
      "Interval 156 (387500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0184\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 389999 -> Avg=1.80, Max=3.00 -> [2.0, 2.0, 1.0, 1.0, 3.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-19.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 781168\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 389999\n",
      "    [POLICY] epsilon actual: 0.6013\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 114s 45ms/step - reward: 0.0184\n",
      "3 episodes - episode_reward: 14.333 [11.000, 18.000] - loss: 0.011 - mae: 0.438 - mean_q: 0.524 - mean_eps: 0.603 - ale.lives: 1.809\n",
      "\n",
      "Interval 157 (390000 steps performed)\n",
      "2500/2500 [==============================] - 107s 43ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 8.500 [6.000, 13.000] - loss: 0.009 - mae: 0.446 - mean_q: 0.537 - mean_eps: 0.600 - ale.lives: 2.206\n",
      "\n",
      "Interval 158 (392500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 394999 -> Avg=16.00, Max=24.00 -> [24.0, 15.0, 16.0, 10.0, 15.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-4.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 97s 39ms/step - reward: 0.0156\n",
      "4 episodes - episode_reward: 9.750 [7.000, 12.000] - loss: 0.009 - mae: 0.440 - mean_q: 0.529 - mean_eps: 0.598 - ale.lives: 2.069\n",
      "\n",
      "Interval 159 (395000 steps performed)\n",
      "2500/2500 [==============================] - 110s 44ms/step - reward: 0.0164\n",
      "4 episodes - episode_reward: 10.000 [7.000, 17.000] - loss: 0.009 - mae: 0.446 - mean_q: 0.537 - mean_eps: 0.595 - ale.lives: 2.096\n",
      "\n",
      "Interval 160 (397500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0168\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 399999 -> Avg=17.00, Max=17.00 -> [17.0, 17.0, 17.0, 17.0, 17.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-3.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 801202\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 399999\n",
      "    [POLICY] epsilon actual: 0.5911\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 127s 51ms/step - reward: 0.0168\n",
      "5 episodes - episode_reward: 9.000 [5.000, 12.000] - loss: 0.010 - mae: 0.448 - mean_q: 0.538 - mean_eps: 0.592 - ale.lives: 2.045\n",
      "\n",
      "Interval 161 (400000 steps performed)\n",
      "2500/2500 [==============================] - 88s 35ms/step - reward: 0.0164\n",
      "5 episodes - episode_reward: 7.800 [6.000, 11.000] - loss: 0.009 - mae: 0.460 - mean_q: 0.553 - mean_eps: 0.590 - ale.lives: 2.013\n",
      "\n",
      "Interval 162 (402500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0140\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 404999 -> Avg=12.00, Max=15.00 -> [11.0, 15.0, 11.0, 8.0, 15.0] -> Mejor Actual: 20.80\n",
      "[AdaptiveElite] Δ=-8.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 110s 44ms/step - reward: 0.0140\n",
      "5 episodes - episode_reward: 8.600 [3.000, 13.000] - loss: 0.008 - mae: 0.454 - mean_q: 0.548 - mean_eps: 0.587 - ale.lives: 1.936\n",
      "\n",
      "Interval 163 (405000 steps performed)\n",
      "2500/2500 [==============================] - 95s 38ms/step - reward: 0.0184\n",
      "3 episodes - episode_reward: 9.333 [0.000, 21.000] - loss: 0.009 - mae: 0.457 - mean_q: 0.547 - mean_eps: 0.585 - ale.lives: 2.007\n",
      "\n",
      "Interval 164 (407500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0196\n",
      ">>> 20.8\n",
      "\n",
      ">>> Step 409999 -> Avg=22.00, Max=31.00 -> [24.0, 31.0, 31.0, 15.0, 9.0] -> Mejor Actual: 20.80\n",
      ">>> Nuevo Máximo: 22.00 -> Guardado en best_model_weights_1M_GPU_Local.h5f\n",
      "[AdaptiveElite] Δ=1.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Mejora → Aumento elite_fraction a 0.17\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 821238\n",
      "    [MEMORY] Elite Fraction: 0.165\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 409999\n",
      "    [POLICY] epsilon actual: 0.5809\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 110s 44ms/step - reward: 0.0196\n",
      "5 episodes - episode_reward: 13.200 [4.000, 24.000] - loss: 0.008 - mae: 0.454 - mean_q: 0.547 - mean_eps: 0.582 - ale.lives: 1.879\n",
      "\n",
      "Interval 165 (410000 steps performed)\n",
      "2500/2500 [==============================] - 92s 37ms/step - reward: 0.0128\n",
      "4 episodes - episode_reward: 8.250 [1.000, 15.000] - loss: 0.012 - mae: 0.476 - mean_q: 0.572 - mean_eps: 0.580 - ale.lives: 2.374\n",
      "\n",
      "Interval 166 (412500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0128\n",
      ">>> 22.0\n",
      "\n",
      ">>> Step 414999 -> Avg=0.40, Max=1.00 -> [0.0, 1.0, 0.0, 0.0, 1.0] -> Mejor Actual: 22.00\n",
      "[AdaptiveElite] Δ=-21.60, elite_fraction=0.17\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 102s 41ms/step - reward: 0.0128\n",
      "2 episodes - episode_reward: 12.000 [10.000, 14.000] - loss: 0.009 - mae: 0.472 - mean_q: 0.567 - mean_eps: 0.577 - ale.lives: 2.093\n",
      "\n",
      "Interval 167 (415000 steps performed)\n",
      "2500/2500 [==============================] - 90s 36ms/step - reward: 0.0120\n",
      "5 episodes - episode_reward: 6.200 [4.000, 8.000] - loss: 0.010 - mae: 0.468 - mean_q: 0.561 - mean_eps: 0.575 - ale.lives: 1.975\n",
      "\n",
      "Interval 168 (417500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0164\n",
      ">>> 22.0\n",
      "\n",
      ">>> Step 419999 -> Avg=13.00, Max=18.00 -> [6.0, 18.0, 18.0, 6.0, 17.0] -> Mejor Actual: 22.00\n",
      "[AdaptiveElite] Δ=-9.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 841268\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 419999\n",
      "    [POLICY] epsilon actual: 0.5707\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 103s 41ms/step - reward: 0.0164\n",
      "4 episodes - episode_reward: 10.250 [6.000, 17.000] - loss: 0.008 - mae: 0.472 - mean_q: 0.566 - mean_eps: 0.572 - ale.lives: 1.828\n",
      "\n",
      "Interval 169 (420000 steps performed)\n",
      "2500/2500 [==============================] - 89s 36ms/step - reward: 0.0184\n",
      "4 episodes - episode_reward: 13.250 [7.000, 22.000] - loss: 0.010 - mae: 0.478 - mean_q: 0.575 - mean_eps: 0.569 - ale.lives: 1.892\n",
      "\n",
      "Interval 170 (422500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0164\n",
      ">>> 22.0\n",
      "\n",
      ">>> Step 424999 -> Avg=18.00, Max=23.00 -> [18.0, 23.0, 19.0, 9.0, 21.0] -> Mejor Actual: 22.00\n",
      "[AdaptiveElite] Δ=-4.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 104s 42ms/step - reward: 0.0164\n",
      "4 episodes - episode_reward: 9.500 [5.000, 13.000] - loss: 0.009 - mae: 0.474 - mean_q: 0.573 - mean_eps: 0.567 - ale.lives: 2.078\n",
      "\n",
      "Interval 171 (425000 steps performed)\n",
      "2500/2500 [==============================] - 89s 35ms/step - reward: 0.0176\n",
      "3 episodes - episode_reward: 9.667 [3.000, 17.000] - loss: 0.010 - mae: 0.477 - mean_q: 0.574 - mean_eps: 0.564 - ale.lives: 2.134\n",
      "\n",
      "Interval 172 (427500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0124\n",
      ">>> 22.0\n",
      "\n",
      ">>> Step 429999 -> Avg=20.00, Max=31.00 -> [14.0, 16.0, 15.0, 24.0, 31.0] -> Mejor Actual: 22.00\n",
      "[AdaptiveElite] Δ=-2.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 861298\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 429999\n",
      "    [POLICY] epsilon actual: 0.5604\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 108s 43ms/step - reward: 0.0124\n",
      "4 episodes - episode_reward: 12.250 [4.000, 21.000] - loss: 0.009 - mae: 0.468 - mean_q: 0.565 - mean_eps: 0.562 - ale.lives: 1.998\n",
      "\n",
      "Interval 173 (430000 steps performed)\n",
      "2500/2500 [==============================] - 91s 36ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 6.000 [0.000, 13.000] - loss: 0.009 - mae: 0.477 - mean_q: 0.572 - mean_eps: 0.559 - ale.lives: 2.332\n",
      "\n",
      "Interval 174 (432500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0140\n",
      ">>> 22.0\n",
      "\n",
      ">>> Step 434999 -> Avg=4.60, Max=13.00 -> [13.0, 1.0, 4.0, 1.0, 4.0] -> Mejor Actual: 22.00\n",
      "[AdaptiveElite] Δ=-17.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 105s 42ms/step - reward: 0.0140\n",
      "2 episodes - episode_reward: 19.000 [17.000, 21.000] - loss: 0.009 - mae: 0.477 - mean_q: 0.572 - mean_eps: 0.557 - ale.lives: 1.781\n",
      "\n",
      "Interval 175 (435000 steps performed)\n",
      "2500/2500 [==============================] - 92s 37ms/step - reward: 0.0176\n",
      "3 episodes - episode_reward: 8.000 [7.000, 9.000] - loss: 0.010 - mae: 0.478 - mean_q: 0.574 - mean_eps: 0.554 - ale.lives: 1.895\n",
      "\n",
      "Interval 176 (437500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0184\n",
      ">>> 22.0\n",
      "\n",
      ">>> Step 439999 -> Avg=21.40, Max=32.00 -> [6.0, 7.0, 32.0, 31.0, 31.0] -> Mejor Actual: 22.00\n",
      "[AdaptiveElite] Δ=-0.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 881324\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 439999\n",
      "    [POLICY] epsilon actual: 0.5502\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 102s 41ms/step - reward: 0.0184\n",
      "4 episodes - episode_reward: 13.250 [4.000, 32.000] - loss: 0.009 - mae: 0.473 - mean_q: 0.568 - mean_eps: 0.552 - ale.lives: 2.196\n",
      "\n",
      "Interval 177 (440000 steps performed)\n",
      "2500/2500 [==============================] - 89s 35ms/step - reward: 0.0188\n",
      "4 episodes - episode_reward: 16.750 [13.000, 22.000] - loss: 0.010 - mae: 0.496 - mean_q: 0.596 - mean_eps: 0.549 - ale.lives: 1.939\n",
      "\n",
      "Interval 178 (442500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 22.0\n",
      "\n",
      ">>> Step 444999 -> Avg=4.80, Max=16.00 -> [1.0, 5.0, 1.0, 16.0, 1.0] -> Mejor Actual: 22.00\n",
      "[AdaptiveElite] Δ=-17.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 99s 39ms/step - reward: 0.0156\n",
      "4 episodes - episode_reward: 9.500 [3.000, 21.000] - loss: 0.009 - mae: 0.486 - mean_q: 0.586 - mean_eps: 0.546 - ale.lives: 2.222\n",
      "\n",
      "Interval 179 (445000 steps performed)\n",
      "2500/2500 [==============================] - 90s 36ms/step - reward: 0.0152\n",
      "5 episodes - episode_reward: 6.800 [3.000, 11.000] - loss: 0.009 - mae: 0.489 - mean_q: 0.588 - mean_eps: 0.544 - ale.lives: 2.127\n",
      "\n",
      "Interval 180 (447500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0176\n",
      ">>> 22.0\n",
      "\n",
      ">>> Step 449999 -> Avg=10.40, Max=14.00 -> [14.0, 9.0, 13.0, 11.0, 5.0] -> Mejor Actual: 22.00\n",
      "[AdaptiveElite] Δ=-11.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 901358\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 449999\n",
      "    [POLICY] epsilon actual: 0.5400\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 100s 40ms/step - reward: 0.0176\n",
      "4 episodes - episode_reward: 8.750 [6.000, 15.000] - loss: 0.010 - mae: 0.487 - mean_q: 0.585 - mean_eps: 0.541 - ale.lives: 1.936\n",
      "\n",
      "Interval 181 (450000 steps performed)\n",
      "2500/2500 [==============================] - 89s 36ms/step - reward: 0.0160\n",
      "3 episodes - episode_reward: 14.333 [13.000, 16.000] - loss: 0.010 - mae: 0.496 - mean_q: 0.596 - mean_eps: 0.539 - ale.lives: 1.969\n",
      "\n",
      "Interval 182 (452500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0208\n",
      ">>> 22.0\n",
      "\n",
      ">>> Step 454999 -> Avg=24.40, Max=30.00 -> [20.0, 29.0, 27.0, 16.0, 30.0] -> Mejor Actual: 22.00\n",
      ">>> Nuevo Máximo: 24.40 -> Guardado en best_model_weights_1M_GPU_Local.h5f\n",
      "[AdaptiveElite] Δ=2.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Mejora → Aumento elite_fraction a 0.17\n",
      "2500/2500 [==============================] - 106s 42ms/step - reward: 0.0208\n",
      "4 episodes - episode_reward: 15.250 [5.000, 24.000] - loss: 0.010 - mae: 0.496 - mean_q: 0.595 - mean_eps: 0.536 - ale.lives: 2.318\n",
      "\n",
      "Interval 183 (455000 steps performed)\n",
      "2500/2500 [==============================] - 89s 36ms/step - reward: 0.0184\n",
      "4 episodes - episode_reward: 8.750 [4.000, 22.000] - loss: 0.009 - mae: 0.496 - mean_q: 0.597 - mean_eps: 0.534 - ale.lives: 1.994\n",
      "\n",
      "Interval 184 (457500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0212\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 459999 -> Avg=6.60, Max=13.00 -> [2.0, 3.0, 11.0, 13.0, 4.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-17.80, elite_fraction=0.17\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 921388\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 459999\n",
      "    [POLICY] epsilon actual: 0.5298\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 100s 40ms/step - reward: 0.0216\n",
      "4 episodes - episode_reward: 15.750 [10.000, 22.000] - loss: 0.010 - mae: 0.503 - mean_q: 0.605 - mean_eps: 0.531 - ale.lives: 2.274\n",
      "\n",
      "Interval 185 (460000 steps performed)\n",
      "2500/2500 [==============================] - 89s 35ms/step - reward: 0.0192\n",
      "4 episodes - episode_reward: 11.000 [6.000, 14.000] - loss: 0.011 - mae: 0.522 - mean_q: 0.628 - mean_eps: 0.529 - ale.lives: 2.112\n",
      "\n",
      "Interval 186 (462500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0224\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 464999 -> Avg=12.40, Max=18.00 -> [6.0, 6.0, 17.0, 15.0, 18.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-12.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 106s 42ms/step - reward: 0.0224\n",
      "3 episodes - episode_reward: 20.667 [16.000, 23.000] - loss: 0.008 - mae: 0.515 - mean_q: 0.621 - mean_eps: 0.526 - ale.lives: 2.411\n",
      "\n",
      "Interval 187 (465000 steps performed)\n",
      "2500/2500 [==============================] - 88s 35ms/step - reward: 0.0160\n",
      "5 episodes - episode_reward: 8.600 [4.000, 17.000] - loss: 0.010 - mae: 0.515 - mean_q: 0.618 - mean_eps: 0.523 - ale.lives: 1.996\n",
      "\n",
      "Interval 188 (467500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0172\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 469999 -> Avg=0.00, Max=0.00 -> [0.0, 0.0, 0.0, 0.0, 0.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-24.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 941418\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 469999\n",
      "    [POLICY] epsilon actual: 0.5196\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 101s 40ms/step - reward: 0.0172\n",
      "3 episodes - episode_reward: 12.000 [2.000, 18.000] - loss: 0.009 - mae: 0.514 - mean_q: 0.618 - mean_eps: 0.521 - ale.lives: 1.998\n",
      "\n",
      "Interval 189 (470000 steps performed)\n",
      "2500/2500 [==============================] - 88s 35ms/step - reward: 0.0216\n",
      "3 episodes - episode_reward: 17.667 [8.000, 23.000] - loss: 0.009 - mae: 0.524 - mean_q: 0.630 - mean_eps: 0.518 - ale.lives: 2.244\n",
      "\n",
      "Interval 190 (472500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0168\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 474999 -> Avg=13.20, Max=17.00 -> [15.0, 17.0, 15.0, 12.0, 7.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-11.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 100s 40ms/step - reward: 0.0168\n",
      "4 episodes - episode_reward: 11.500 [3.000, 17.000] - loss: 0.010 - mae: 0.535 - mean_q: 0.644 - mean_eps: 0.516 - ale.lives: 2.040\n",
      "\n",
      "Interval 191 (475000 steps performed)\n",
      "2500/2500 [==============================] - 101s 40ms/step - reward: 0.0192\n",
      "4 episodes - episode_reward: 12.750 [5.000, 18.000] - loss: 0.010 - mae: 0.534 - mean_q: 0.641 - mean_eps: 0.513 - ale.lives: 2.105\n",
      "\n",
      "Interval 192 (477500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0124\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 479999 -> Avg=0.60, Max=1.00 -> [1.0, 0.0, 1.0, 0.0, 1.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-23.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 961448\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 479999\n",
      "    [POLICY] epsilon actual: 0.5093\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 121s 48ms/step - reward: 0.0124\n",
      "4 episodes - episode_reward: 8.000 [4.000, 12.000] - loss: 0.010 - mae: 0.522 - mean_q: 0.627 - mean_eps: 0.511 - ale.lives: 1.884\n",
      "\n",
      "Interval 193 (480000 steps performed)\n",
      "2500/2500 [==============================] - 88s 35ms/step - reward: 0.0176\n",
      "3 episodes - episode_reward: 9.667 [1.000, 17.000] - loss: 0.010 - mae: 0.542 - mean_q: 0.651 - mean_eps: 0.508 - ale.lives: 2.129\n",
      "\n",
      "Interval 194 (482500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0220\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 484999 -> Avg=18.60, Max=25.00 -> [25.0, 18.0, 22.0, 7.0, 21.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-5.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 103s 41ms/step - reward: 0.0220\n",
      "4 episodes - episode_reward: 17.750 [5.000, 29.000] - loss: 0.010 - mae: 0.541 - mean_q: 0.648 - mean_eps: 0.506 - ale.lives: 1.870\n",
      "\n",
      "Interval 195 (485000 steps performed)\n",
      "2500/2500 [==============================] - 111s 44ms/step - reward: 0.0168\n",
      "3 episodes - episode_reward: 11.000 [0.000, 25.000] - loss: 0.010 - mae: 0.541 - mean_q: 0.652 - mean_eps: 0.503 - ale.lives: 2.759\n",
      "\n",
      "Interval 196 (487500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0152\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 489999 -> Avg=6.60, Max=11.00 -> [8.0, 9.0, 3.0, 11.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-17.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 981476\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 489999\n",
      "    [POLICY] epsilon actual: 0.4991\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 104s 42ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 11.250 [5.000, 16.000] - loss: 0.011 - mae: 0.533 - mean_q: 0.640 - mean_eps: 0.500 - ale.lives: 2.067\n",
      "\n",
      "Interval 197 (490000 steps performed)\n",
      "2500/2500 [==============================] - 89s 36ms/step - reward: 0.0184\n",
      "4 episodes - episode_reward: 10.500 [2.000, 18.000] - loss: 0.011 - mae: 0.552 - mean_q: 0.663 - mean_eps: 0.498 - ale.lives: 1.918\n",
      "\n",
      "Interval 198 (492500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0144\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 494999 -> Avg=20.20, Max=34.00 -> [14.0, 21.0, 15.0, 34.0, 17.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-4.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 122s 49ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 10.500 [6.000, 23.000] - loss: 0.009 - mae: 0.539 - mean_q: 0.648 - mean_eps: 0.495 - ale.lives: 2.299\n",
      "\n",
      "Interval 199 (495000 steps performed)\n",
      "2500/2500 [==============================] - 106s 42ms/step - reward: 0.0204\n",
      "4 episodes - episode_reward: 12.500 [0.000, 24.000] - loss: 0.009 - mae: 0.545 - mean_q: 0.655 - mean_eps: 0.493 - ale.lives: 2.203\n",
      "\n",
      "Interval 200 (497500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0112\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 499999 -> Avg=0.00, Max=0.00 -> [0.0, 0.0, 0.0, 0.0, 0.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-24.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1001510\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 499999\n",
      "    [POLICY] epsilon actual: 0.4889\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 105s 42ms/step - reward: 0.0112\n",
      "5 episodes - episode_reward: 5.000 [2.000, 11.000] - loss: 0.010 - mae: 0.538 - mean_q: 0.646 - mean_eps: 0.490 - ale.lives: 2.020\n",
      "\n",
      "Interval 201 (500000 steps performed)\n",
      "2500/2500 [==============================] - 87s 35ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 6.750 [4.000, 11.000] - loss: 0.009 - mae: 0.561 - mean_q: 0.674 - mean_eps: 0.488 - ale.lives: 1.817\n",
      "\n",
      "Interval 202 (502500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 504999 -> Avg=3.00, Max=5.00 -> [4.0, 2.0, 2.0, 5.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-21.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 95s 38ms/step - reward: 0.0132\n",
      "3 episodes - episode_reward: 12.333 [8.000, 15.000] - loss: 0.009 - mae: 0.555 - mean_q: 0.666 - mean_eps: 0.485 - ale.lives: 1.850\n",
      "\n",
      "Interval 203 (505000 steps performed)\n",
      "2500/2500 [==============================] - 97s 39ms/step - reward: 0.0164\n",
      "5 episodes - episode_reward: 9.400 [7.000, 18.000] - loss: 0.011 - mae: 0.559 - mean_q: 0.669 - mean_eps: 0.483 - ale.lives: 2.014\n",
      "\n",
      "Interval 204 (507500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0112\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 509999 -> Avg=4.40, Max=6.00 -> [3.0, 5.0, 6.0, 6.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-20.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1021542\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 509999\n",
      "    [POLICY] epsilon actual: 0.4787\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 104s 42ms/step - reward: 0.0112\n",
      "4 episodes - episode_reward: 5.750 [4.000, 7.000] - loss: 0.009 - mae: 0.551 - mean_q: 0.660 - mean_eps: 0.480 - ale.lives: 2.081\n",
      "\n",
      "Interval 205 (510000 steps performed)\n",
      "2500/2500 [==============================] - 85s 34ms/step - reward: 0.0168\n",
      "4 episodes - episode_reward: 10.500 [6.000, 19.000] - loss: 0.010 - mae: 0.574 - mean_q: 0.690 - mean_eps: 0.477 - ale.lives: 2.267\n",
      "\n",
      "Interval 206 (512500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0108\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 514999 -> Avg=0.00, Max=0.00 -> [0.0, 0.0, 0.0, 0.0, 0.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-24.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 94s 38ms/step - reward: 0.0108\n",
      "4 episodes - episode_reward: 7.250 [5.000, 11.000] - loss: 0.011 - mae: 0.574 - mean_q: 0.691 - mean_eps: 0.475 - ale.lives: 2.305\n",
      "\n",
      "Interval 207 (515000 steps performed)\n",
      "2500/2500 [==============================] - 86s 34ms/step - reward: 0.0192\n",
      "4 episodes - episode_reward: 11.000 [4.000, 17.000] - loss: 0.011 - mae: 0.578 - mean_q: 0.694 - mean_eps: 0.472 - ale.lives: 1.828\n",
      "\n",
      "Interval 208 (517500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0112\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 519999 -> Avg=7.00, Max=12.00 -> [7.0, 12.0, 0.0, 6.0, 10.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-17.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1041574\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 519999\n",
      "    [POLICY] epsilon actual: 0.4684\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 103s 41ms/step - reward: 0.0112\n",
      "4 episodes - episode_reward: 8.500 [6.000, 12.000] - loss: 0.009 - mae: 0.569 - mean_q: 0.683 - mean_eps: 0.470 - ale.lives: 1.771\n",
      "\n",
      "Interval 209 (520000 steps performed)\n",
      "2500/2500 [==============================] - 97s 39ms/step - reward: 0.0236\n",
      "3 episodes - episode_reward: 15.000 [2.000, 27.000] - loss: 0.010 - mae: 0.581 - mean_q: 0.699 - mean_eps: 0.467 - ale.lives: 2.091\n",
      "\n",
      "Interval 210 (522500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0180\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 524999 -> Avg=15.00, Max=17.00 -> [17.0, 8.0, 17.0, 17.0, 16.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-9.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 98s 39ms/step - reward: 0.0180\n",
      "3 episodes - episode_reward: 16.000 [5.000, 23.000] - loss: 0.010 - mae: 0.577 - mean_q: 0.695 - mean_eps: 0.465 - ale.lives: 2.351\n",
      "\n",
      "Interval 211 (525000 steps performed)\n",
      "2500/2500 [==============================] - 89s 36ms/step - reward: 0.0184\n",
      "4 episodes - episode_reward: 11.500 [9.000, 13.000] - loss: 0.012 - mae: 0.575 - mean_q: 0.693 - mean_eps: 0.462 - ale.lives: 2.149\n",
      "\n",
      "Interval 212 (527500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0196\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 529999 -> Avg=19.40, Max=26.00 -> [24.0, 26.0, 13.0, 16.0, 18.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-5.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1061602\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 529999\n",
      "    [POLICY] epsilon actual: 0.4582\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 100s 40ms/step - reward: 0.0200\n",
      "4 episodes - episode_reward: 13.250 [5.000, 22.000] - loss: 0.010 - mae: 0.571 - mean_q: 0.689 - mean_eps: 0.460 - ale.lives: 2.093\n",
      "\n",
      "Interval 213 (530000 steps performed)\n",
      "2500/2500 [==============================] - 86s 34ms/step - reward: 0.0204\n",
      "4 episodes - episode_reward: 12.500 [6.000, 26.000] - loss: 0.009 - mae: 0.608 - mean_q: 0.732 - mean_eps: 0.457 - ale.lives: 2.296\n",
      "\n",
      "Interval 214 (532500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0136\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 534999 -> Avg=16.00, Max=16.00 -> [16.0, 16.0, 16.0, 16.0, 16.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-8.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 102s 41ms/step - reward: 0.0136\n",
      "2 episodes - episode_reward: 12.500 [12.000, 13.000] - loss: 0.014 - mae: 0.610 - mean_q: 0.735 - mean_eps: 0.454 - ale.lives: 2.275\n",
      "\n",
      "Interval 215 (535000 steps performed)\n",
      "2500/2500 [==============================] - 86s 34ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 10.500 [6.000, 20.000] - loss: 0.010 - mae: 0.597 - mean_q: 0.715 - mean_eps: 0.452 - ale.lives: 2.277\n",
      "\n",
      "Interval 216 (537500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0136\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 539999 -> Avg=11.60, Max=16.00 -> [14.0, 16.0, 8.0, 8.0, 12.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-12.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1081630\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 539999\n",
      "    [POLICY] epsilon actual: 0.4480\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 105s 42ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 10.750 [5.000, 17.000] - loss: 0.010 - mae: 0.599 - mean_q: 0.718 - mean_eps: 0.449 - ale.lives: 1.944\n",
      "\n",
      "Interval 217 (540000 steps performed)\n",
      "2500/2500 [==============================] - 87s 35ms/step - reward: 0.0172\n",
      "4 episodes - episode_reward: 10.500 [4.000, 20.000] - loss: 0.013 - mae: 0.612 - mean_q: 0.735 - mean_eps: 0.447 - ale.lives: 2.556\n",
      "\n",
      "Interval 218 (542500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0152\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 544999 -> Avg=18.40, Max=27.00 -> [16.0, 14.0, 12.0, 23.0, 27.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-6.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 100s 40ms/step - reward: 0.0152\n",
      "3 episodes - episode_reward: 13.000 [7.000, 20.000] - loss: 0.012 - mae: 0.597 - mean_q: 0.715 - mean_eps: 0.444 - ale.lives: 1.996\n",
      "\n",
      "Interval 219 (545000 steps performed)\n",
      "2500/2500 [==============================] - 87s 35ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 6.750 [2.000, 11.000] - loss: 0.013 - mae: 0.598 - mean_q: 0.717 - mean_eps: 0.442 - ale.lives: 2.157\n",
      "\n",
      "Interval 220 (547500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0188\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 549999 -> Avg=11.60, Max=19.00 -> [10.0, 15.0, 7.0, 19.0, 7.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-12.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1101660\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 549999\n",
      "    [POLICY] epsilon actual: 0.4378\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 97s 39ms/step - reward: 0.0188\n",
      "4 episodes - episode_reward: 12.750 [8.000, 17.000] - loss: 0.009 - mae: 0.587 - mean_q: 0.708 - mean_eps: 0.439 - ale.lives: 1.948\n",
      "\n",
      "Interval 221 (550000 steps performed)\n",
      "2500/2500 [==============================] - 88s 35ms/step - reward: 0.0172\n",
      "3 episodes - episode_reward: 15.333 [7.000, 25.000] - loss: 0.012 - mae: 0.611 - mean_q: 0.733 - mean_eps: 0.437 - ale.lives: 1.943\n",
      "\n",
      "Interval 222 (552500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0204\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 554999 -> Avg=8.40, Max=9.00 -> [8.0, 9.0, 9.0, 8.0, 8.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-16.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 99s 40ms/step - reward: 0.0204\n",
      "5 episodes - episode_reward: 10.000 [4.000, 25.000] - loss: 0.009 - mae: 0.599 - mean_q: 0.719 - mean_eps: 0.434 - ale.lives: 2.219\n",
      "\n",
      "Interval 223 (555000 steps performed)\n",
      "2500/2500 [==============================] - 86s 35ms/step - reward: 0.0168\n",
      "4 episodes - episode_reward: 8.500 [5.000, 14.000] - loss: 0.011 - mae: 0.599 - mean_q: 0.718 - mean_eps: 0.431 - ale.lives: 2.004\n",
      "\n",
      "Interval 224 (557500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0140\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 559999 -> Avg=1.40, Max=2.00 -> [1.0, 2.0, 1.0, 1.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-23.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1121690\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 559999\n",
      "    [POLICY] epsilon actual: 0.4276\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 113s 45ms/step - reward: 0.0140\n",
      "3 episodes - episode_reward: 15.667 [5.000, 23.000] - loss: 0.012 - mae: 0.594 - mean_q: 0.712 - mean_eps: 0.429 - ale.lives: 2.113\n",
      "\n",
      "Interval 225 (560000 steps performed)\n",
      "2500/2500 [==============================] - 128s 51ms/step - reward: 0.0192\n",
      "4 episodes - episode_reward: 11.750 [1.000, 17.000] - loss: 0.009 - mae: 0.611 - mean_q: 0.735 - mean_eps: 0.426 - ale.lives: 1.950\n",
      "\n",
      "Interval 226 (562500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 564999 -> Avg=9.20, Max=14.00 -> [5.0, 5.0, 11.0, 14.0, 11.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-15.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 125s 50ms/step - reward: 0.0156\n",
      "3 episodes - episode_reward: 13.667 [12.000, 17.000] - loss: 0.012 - mae: 0.609 - mean_q: 0.729 - mean_eps: 0.424 - ale.lives: 2.167\n",
      "\n",
      "Interval 227 (565000 steps performed)\n",
      "2500/2500 [==============================] - 121s 48ms/step - reward: 0.0108\n",
      "4 episodes - episode_reward: 5.750 [0.000, 9.000] - loss: 0.010 - mae: 0.599 - mean_q: 0.719 - mean_eps: 0.421 - ale.lives: 2.268\n",
      "\n",
      "Interval 228 (567500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0144\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 569999 -> Avg=11.80, Max=17.00 -> [8.0, 10.0, 17.0, 13.0, 11.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-12.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1141722\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 569999\n",
      "    [POLICY] epsilon actual: 0.4173\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 149s 59ms/step - reward: 0.0144\n",
      "5 episodes - episode_reward: 7.400 [4.000, 17.000] - loss: 0.012 - mae: 0.611 - mean_q: 0.733 - mean_eps: 0.419 - ale.lives: 2.186\n",
      "\n",
      "Interval 229 (570000 steps performed)\n",
      "2500/2500 [==============================] - 127s 51ms/step - reward: 0.0192\n",
      "4 episodes - episode_reward: 11.000 [3.000, 21.000] - loss: 0.010 - mae: 0.610 - mean_q: 0.732 - mean_eps: 0.416 - ale.lives: 2.094\n",
      "\n",
      "Interval 230 (572500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0188\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 574999 -> Avg=10.60, Max=18.00 -> [13.0, 18.0, 11.0, 3.0, 8.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-13.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 138s 55ms/step - reward: 0.0188\n",
      "2 episodes - episode_reward: 27.000 [24.000, 30.000] - loss: 0.010 - mae: 0.604 - mean_q: 0.726 - mean_eps: 0.414 - ale.lives: 1.732\n",
      "\n",
      "Interval 231 (575000 steps performed)\n",
      "2500/2500 [==============================] - 95s 38ms/step - reward: 0.0208\n",
      "4 episodes - episode_reward: 11.000 [0.000, 18.000] - loss: 0.010 - mae: 0.609 - mean_q: 0.733 - mean_eps: 0.411 - ale.lives: 2.083\n",
      "\n",
      "Interval 232 (577500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0164\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 579999 -> Avg=2.60, Max=3.00 -> [2.0, 3.0, 3.0, 3.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-21.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1161746\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 579999\n",
      "    [POLICY] epsilon actual: 0.4071\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 139s 56ms/step - reward: 0.0164\n",
      "2 episodes - episode_reward: 12.000 [11.000, 13.000] - loss: 0.011 - mae: 0.613 - mean_q: 0.737 - mean_eps: 0.408 - ale.lives: 1.970\n",
      "\n",
      "Interval 233 (580000 steps performed)\n",
      "2500/2500 [==============================] - 126s 51ms/step - reward: 0.0180\n",
      "3 episodes - episode_reward: 16.667 [12.000, 25.000] - loss: 0.009 - mae: 0.618 - mean_q: 0.743 - mean_eps: 0.406 - ale.lives: 1.978\n",
      "\n",
      "Interval 234 (582500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0196\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 584999 -> Avg=3.40, Max=5.00 -> [4.0, 3.0, 5.0, 3.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-21.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 142s 57ms/step - reward: 0.0196\n",
      "3 episodes - episode_reward: 15.333 [5.000, 21.000] - loss: 0.010 - mae: 0.630 - mean_q: 0.757 - mean_eps: 0.403 - ale.lives: 2.044\n",
      "\n",
      "Interval 235 (585000 steps performed)\n",
      "2500/2500 [==============================] - 100s 40ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 13.250 [4.000, 23.000] - loss: 0.011 - mae: 0.622 - mean_q: 0.749 - mean_eps: 0.401 - ale.lives: 2.433\n",
      "\n",
      "Interval 236 (587500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0084\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 589999 -> Avg=4.20, Max=9.00 -> [5.0, 2.0, 9.0, 1.0, 4.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-20.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1181772\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 589999\n",
      "    [POLICY] epsilon actual: 0.3969\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 129s 52ms/step - reward: 0.0084\n",
      "3 episodes - episode_reward: 8.000 [5.000, 13.000] - loss: 0.009 - mae: 0.615 - mean_q: 0.744 - mean_eps: 0.398 - ale.lives: 2.051\n",
      "\n",
      "Interval 237 (590000 steps performed)\n",
      "2500/2500 [==============================] - 124s 50ms/step - reward: 0.0164\n",
      "3 episodes - episode_reward: 12.333 [5.000, 21.000] - loss: 0.012 - mae: 0.635 - mean_q: 0.763 - mean_eps: 0.396 - ale.lives: 2.062\n",
      "\n",
      "Interval 238 (592500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0160\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 594999 -> Avg=10.80, Max=14.00 -> [11.0, 9.0, 11.0, 9.0, 14.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-13.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 139s 55ms/step - reward: 0.0160\n",
      "3 episodes - episode_reward: 13.000 [11.000, 15.000] - loss: 0.010 - mae: 0.630 - mean_q: 0.759 - mean_eps: 0.393 - ale.lives: 1.949\n",
      "\n",
      "Interval 239 (595000 steps performed)\n",
      "2500/2500 [==============================] - 115s 46ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 11.500 [9.000, 18.000] - loss: 0.011 - mae: 0.628 - mean_q: 0.753 - mean_eps: 0.391 - ale.lives: 2.258\n",
      "\n",
      "Interval 240 (597500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0164\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 599999 -> Avg=13.80, Max=20.00 -> [7.0, 20.0, 7.0, 16.0, 19.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-10.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1201796\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 599999\n",
      "    [POLICY] epsilon actual: 0.3867\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 138s 55ms/step - reward: 0.0164\n",
      "2 episodes - episode_reward: 16.000 [9.000, 23.000] - loss: 0.010 - mae: 0.621 - mean_q: 0.745 - mean_eps: 0.388 - ale.lives: 2.387\n",
      "\n",
      "Interval 241 (600000 steps performed)\n",
      "2500/2500 [==============================] - 127s 51ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 10.750 [7.000, 18.000] - loss: 0.008 - mae: 0.621 - mean_q: 0.747 - mean_eps: 0.385 - ale.lives: 1.873\n",
      "\n",
      "Interval 242 (602500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0172\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 604999 -> Avg=12.40, Max=16.00 -> [12.0, 16.0, 11.0, 12.0, 11.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-12.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 139s 55ms/step - reward: 0.0172\n",
      "4 episodes - episode_reward: 9.750 [6.000, 15.000] - loss: 0.010 - mae: 0.628 - mean_q: 0.756 - mean_eps: 0.383 - ale.lives: 2.046\n",
      "\n",
      "Interval 243 (605000 steps performed)\n",
      "2500/2500 [==============================] - 99s 40ms/step - reward: 0.0184\n",
      "4 episodes - episode_reward: 9.250 [4.000, 19.000] - loss: 0.011 - mae: 0.627 - mean_q: 0.754 - mean_eps: 0.380 - ale.lives: 2.070\n",
      "\n",
      "Interval 244 (607500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0168\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 609999 -> Avg=10.80, Max=15.00 -> [15.0, 7.0, 12.0, 7.0, 13.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-13.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1221826\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 609999\n",
      "    [POLICY] epsilon actual: 0.3764\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 128s 51ms/step - reward: 0.0168\n",
      "3 episodes - episode_reward: 16.000 [9.000, 24.000] - loss: 0.010 - mae: 0.629 - mean_q: 0.759 - mean_eps: 0.378 - ale.lives: 2.138\n",
      "\n",
      "Interval 245 (610000 steps performed)\n",
      "2500/2500 [==============================] - 126s 51ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 10.250 [7.000, 14.000] - loss: 0.010 - mae: 0.644 - mean_q: 0.774 - mean_eps: 0.375 - ale.lives: 2.114\n",
      "\n",
      "Interval 246 (612500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0116\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 614999 -> Avg=1.00, Max=1.00 -> [1.0, 1.0, 1.0, 1.0, 1.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-23.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 140s 56ms/step - reward: 0.0116\n",
      "3 episodes - episode_reward: 9.333 [9.000, 10.000] - loss: 0.008 - mae: 0.634 - mean_q: 0.763 - mean_eps: 0.373 - ale.lives: 2.166\n",
      "\n",
      "Interval 247 (615000 steps performed)\n",
      "2500/2500 [==============================] - 115s 46ms/step - reward: 0.0152\n",
      "3 episodes - episode_reward: 10.000 [5.000, 18.000] - loss: 0.009 - mae: 0.637 - mean_q: 0.765 - mean_eps: 0.370 - ale.lives: 1.996\n",
      "\n",
      "Interval 248 (617500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0120\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 619999 -> Avg=0.00, Max=0.00 -> [0.0, 0.0, 0.0, 0.0, 0.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-24.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1241854\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 619999\n",
      "    [POLICY] epsilon actual: 0.3662\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 116s 46ms/step - reward: 0.0120\n",
      "4 episodes - episode_reward: 9.000 [5.000, 13.000] - loss: 0.012 - mae: 0.639 - mean_q: 0.767 - mean_eps: 0.368 - ale.lives: 2.062\n",
      "\n",
      "Interval 249 (620000 steps performed)\n",
      "2500/2500 [==============================] - 127s 51ms/step - reward: 0.0144\n",
      "3 episodes - episode_reward: 12.667 [7.000, 19.000] - loss: 0.011 - mae: 0.661 - mean_q: 0.793 - mean_eps: 0.365 - ale.lives: 1.983\n",
      "\n",
      "Interval 250 (622500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0120\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 624999 -> Avg=16.60, Max=17.00 -> [17.0, 15.0, 17.0, 17.0, 17.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-7.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 146s 58ms/step - reward: 0.0120\n",
      "3 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.011 - mae: 0.659 - mean_q: 0.790 - mean_eps: 0.362 - ale.lives: 1.698\n",
      "\n",
      "Interval 251 (625000 steps performed)\n",
      "2500/2500 [==============================] - 98s 39ms/step - reward: 0.0192\n",
      "4 episodes - episode_reward: 11.500 [5.000, 23.000] - loss: 0.011 - mae: 0.654 - mean_q: 0.787 - mean_eps: 0.360 - ale.lives: 2.071\n",
      "\n",
      "Interval 252 (627500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0184\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 629999 -> Avg=7.40, Max=9.00 -> [9.0, 6.0, 4.0, 9.0, 9.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-17.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1261882\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 629999\n",
      "    [POLICY] epsilon actual: 0.3560\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 96s 38ms/step - reward: 0.0184\n",
      "4 episodes - episode_reward: 10.750 [6.000, 18.000] - loss: 0.012 - mae: 0.662 - mean_q: 0.798 - mean_eps: 0.357 - ale.lives: 2.279\n",
      "\n",
      "Interval 253 (630000 steps performed)\n",
      "2500/2500 [==============================] - 87s 35ms/step - reward: 0.0180\n",
      "4 episodes - episode_reward: 12.500 [5.000, 26.000] - loss: 0.010 - mae: 0.669 - mean_q: 0.804 - mean_eps: 0.355 - ale.lives: 2.183\n",
      "\n",
      "Interval 254 (632500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0192\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 634999 -> Avg=3.20, Max=11.00 -> [1.0, 11.0, 0.0, 1.0, 3.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-21.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 116s 46ms/step - reward: 0.0192\n",
      "4 episodes - episode_reward: 13.000 [10.000, 18.000] - loss: 0.012 - mae: 0.676 - mean_q: 0.811 - mean_eps: 0.352 - ale.lives: 2.054\n",
      "\n",
      "Interval 255 (635000 steps performed)\n",
      "2500/2500 [==============================] - 89s 36ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 8.500 [1.000, 16.000] - loss: 0.013 - mae: 0.687 - mean_q: 0.824 - mean_eps: 0.350 - ale.lives: 2.136\n",
      "\n",
      "Interval 256 (637500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0196\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 639999 -> Avg=14.60, Max=19.00 -> [19.0, 15.0, 11.0, 18.0, 10.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-9.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1281912\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 639999\n",
      "    [POLICY] epsilon actual: 0.3458\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 99s 40ms/step - reward: 0.0196\n",
      "3 episodes - episode_reward: 16.000 [9.000, 21.000] - loss: 0.011 - mae: 0.673 - mean_q: 0.808 - mean_eps: 0.347 - ale.lives: 2.274\n",
      "\n",
      "Interval 257 (640000 steps performed)\n",
      "2500/2500 [==============================] - 93s 37ms/step - reward: 0.0188\n",
      "3 episodes - episode_reward: 11.000 [6.000, 14.000] - loss: 0.012 - mae: 0.686 - mean_q: 0.822 - mean_eps: 0.345 - ale.lives: 1.865\n",
      "\n",
      "Interval 258 (642500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0108\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 644999 -> Avg=11.40, Max=18.00 -> [14.0, 6.0, 10.0, 18.0, 9.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-13.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 113s 45ms/step - reward: 0.0112\n",
      "4 episodes - episode_reward: 11.500 [4.000, 21.000] - loss: 0.011 - mae: 0.689 - mean_q: 0.825 - mean_eps: 0.342 - ale.lives: 1.957\n",
      "\n",
      "Interval 259 (645000 steps performed)\n",
      "2500/2500 [==============================] - 86s 34ms/step - reward: 0.0180\n",
      "4 episodes - episode_reward: 11.750 [2.000, 26.000] - loss: 0.010 - mae: 0.689 - mean_q: 0.827 - mean_eps: 0.339 - ale.lives: 2.100\n",
      "\n",
      "Interval 260 (647500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0188\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 649999 -> Avg=15.80, Max=19.00 -> [15.0, 16.0, 11.0, 19.0, 18.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-8.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1301940\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 649999\n",
      "    [POLICY] epsilon actual: 0.3356\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 98s 39ms/step - reward: 0.0188\n",
      "3 episodes - episode_reward: 13.667 [7.000, 19.000] - loss: 0.012 - mae: 0.690 - mean_q: 0.828 - mean_eps: 0.337 - ale.lives: 2.405\n",
      "\n",
      "Interval 261 (650000 steps performed)\n",
      "2500/2500 [==============================] - 99s 39ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 9.750 [6.000, 11.000] - loss: 0.010 - mae: 0.710 - mean_q: 0.852 - mean_eps: 0.334 - ale.lives: 1.901\n",
      "\n",
      "Interval 262 (652500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0136\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 654999 -> Avg=11.00, Max=15.00 -> [10.0, 9.0, 15.0, 12.0, 9.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-13.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 111s 44ms/step - reward: 0.0136\n",
      "3 episodes - episode_reward: 10.667 [4.000, 18.000] - loss: 0.009 - mae: 0.712 - mean_q: 0.858 - mean_eps: 0.332 - ale.lives: 2.058\n",
      "\n",
      "Interval 263 (655000 steps performed)\n",
      "2500/2500 [==============================] - 86s 34ms/step - reward: 0.0176\n",
      "4 episodes - episode_reward: 9.500 [7.000, 11.000] - loss: 0.011 - mae: 0.711 - mean_q: 0.851 - mean_eps: 0.329 - ale.lives: 1.943\n",
      "\n",
      "Interval 264 (657500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0104\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 659999 -> Avg=1.20, Max=2.00 -> [2.0, 0.0, 1.0, 2.0, 1.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-23.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1321968\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 659999\n",
      "    [POLICY] epsilon actual: 0.3253\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 98s 39ms/step - reward: 0.0104\n",
      "3 episodes - episode_reward: 11.000 [5.000, 16.000] - loss: 0.010 - mae: 0.710 - mean_q: 0.852 - mean_eps: 0.327 - ale.lives: 2.017\n",
      "\n",
      "Interval 265 (660000 steps performed)\n",
      "2500/2500 [==============================] - 95s 38ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 7.500 [4.000, 14.000] - loss: 0.010 - mae: 0.710 - mean_q: 0.854 - mean_eps: 0.324 - ale.lives: 2.089\n",
      "\n",
      "Interval 266 (662500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 664999 -> Avg=4.40, Max=10.00 -> [2.0, 6.0, 1.0, 3.0, 10.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-20.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 107s 43ms/step - reward: 0.0132\n",
      "4 episodes - episode_reward: 10.250 [6.000, 13.000] - loss: 0.011 - mae: 0.720 - mean_q: 0.864 - mean_eps: 0.322 - ale.lives: 2.076\n",
      "\n",
      "Interval 267 (665000 steps performed)\n",
      "2500/2500 [==============================] - 86s 34ms/step - reward: 0.0156\n",
      "4 episodes - episode_reward: 6.250 [2.000, 10.000] - loss: 0.011 - mae: 0.713 - mean_q: 0.859 - mean_eps: 0.319 - ale.lives: 2.130\n",
      "\n",
      "Interval 268 (667500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0148\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 669999 -> Avg=12.40, Max=16.00 -> [12.0, 12.0, 16.0, 11.0, 11.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-12.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1341998\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 669999\n",
      "    [POLICY] epsilon actual: 0.3151\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 100s 40ms/step - reward: 0.0148\n",
      "3 episodes - episode_reward: 16.000 [9.000, 23.000] - loss: 0.010 - mae: 0.713 - mean_q: 0.856 - mean_eps: 0.316 - ale.lives: 2.400\n",
      "\n",
      "Interval 269 (670000 steps performed)\n",
      "2500/2500 [==============================] - 87s 35ms/step - reward: 0.0176\n",
      "4 episodes - episode_reward: 10.750 [5.000, 19.000] - loss: 0.012 - mae: 0.728 - mean_q: 0.875 - mean_eps: 0.314 - ale.lives: 2.529\n",
      "\n",
      "Interval 270 (672500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0136\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 674999 -> Avg=16.00, Max=16.00 -> [16.0, 16.0, 16.0, 16.0, 16.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-8.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 103s 41ms/step - reward: 0.0136\n",
      "3 episodes - episode_reward: 11.667 [8.000, 14.000] - loss: 0.010 - mae: 0.733 - mean_q: 0.881 - mean_eps: 0.311 - ale.lives: 2.053\n",
      "\n",
      "Interval 271 (675000 steps performed)\n",
      "2500/2500 [==============================] - 100s 40ms/step - reward: 0.0176\n",
      "4 episodes - episode_reward: 6.500 [4.000, 9.000] - loss: 0.012 - mae: 0.733 - mean_q: 0.879 - mean_eps: 0.309 - ale.lives: 1.928\n",
      "\n",
      "Interval 272 (677500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0196\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 679999 -> Avg=14.60, Max=19.00 -> [12.0, 10.0, 19.0, 16.0, 16.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-9.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1362026\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 679999\n",
      "    [POLICY] epsilon actual: 0.3049\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 101s 40ms/step - reward: 0.0196\n",
      "3 episodes - episode_reward: 23.333 [22.000, 25.000] - loss: 0.012 - mae: 0.728 - mean_q: 0.875 - mean_eps: 0.306 - ale.lives: 2.261\n",
      "\n",
      "Interval 273 (680000 steps performed)\n",
      "2500/2500 [==============================] - 85s 34ms/step - reward: 0.0176\n",
      "4 episodes - episode_reward: 10.500 [2.000, 17.000] - loss: 0.011 - mae: 0.739 - mean_q: 0.887 - mean_eps: 0.304 - ale.lives: 1.824\n",
      "\n",
      "Interval 274 (682500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 684999 -> Avg=23.40, Max=29.00 -> [23.0, 29.0, 24.0, 20.0, 21.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-1.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 105s 42ms/step - reward: 0.0132\n",
      "3 episodes - episode_reward: 12.333 [8.000, 15.000] - loss: 0.010 - mae: 0.737 - mean_q: 0.886 - mean_eps: 0.301 - ale.lives: 1.866\n",
      "\n",
      "Interval 275 (685000 steps performed)\n",
      "2500/2500 [==============================] - 103s 41ms/step - reward: 0.0148\n",
      "4 episodes - episode_reward: 8.750 [0.000, 23.000] - loss: 0.010 - mae: 0.736 - mean_q: 0.884 - mean_eps: 0.299 - ale.lives: 2.286\n",
      "\n",
      "Interval 276 (687500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0160\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 689999 -> Avg=0.00, Max=0.00 -> [0.0, 0.0, 0.0, 0.0, 0.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-24.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1382054\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 689999\n",
      "    [POLICY] epsilon actual: 0.2947\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 103s 41ms/step - reward: 0.0160\n",
      "3 episodes - episode_reward: 11.333 [7.000, 20.000] - loss: 0.012 - mae: 0.742 - mean_q: 0.891 - mean_eps: 0.296 - ale.lives: 2.020\n",
      "\n",
      "Interval 277 (690000 steps performed)\n",
      "2500/2500 [==============================] - 87s 35ms/step - reward: 0.0116\n",
      "4 episodes - episode_reward: 7.250 [4.000, 10.000] - loss: 0.013 - mae: 0.764 - mean_q: 0.917 - mean_eps: 0.293 - ale.lives: 1.981\n",
      "\n",
      "Interval 278 (692500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0188\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 694999 -> Avg=14.80, Max=19.00 -> [19.0, 18.0, 16.0, 13.0, 8.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-9.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 110s 44ms/step - reward: 0.0188\n",
      "3 episodes - episode_reward: 13.667 [5.000, 22.000] - loss: 0.011 - mae: 0.763 - mean_q: 0.915 - mean_eps: 0.291 - ale.lives: 2.277\n",
      "\n",
      "Interval 279 (695000 steps performed)\n",
      "2500/2500 [==============================] - 98s 39ms/step - reward: 0.0116\n",
      "5 episodes - episode_reward: 8.600 [5.000, 14.000] - loss: 0.012 - mae: 0.767 - mean_q: 0.920 - mean_eps: 0.288 - ale.lives: 2.075\n",
      "\n",
      "Interval 280 (697500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0140\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 699999 -> Avg=14.80, Max=21.00 -> [7.0, 17.0, 13.0, 21.0, 16.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-9.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1402086\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 699999\n",
      "    [POLICY] epsilon actual: 0.2844\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 99s 40ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 8.750 [4.000, 13.000] - loss: 0.012 - mae: 0.766 - mean_q: 0.918 - mean_eps: 0.286 - ale.lives: 2.152\n",
      "\n",
      "Interval 281 (700000 steps performed)\n",
      "2500/2500 [==============================] - 86s 34ms/step - reward: 0.0208\n",
      "3 episodes - episode_reward: 15.333 [0.000, 27.000] - loss: 0.012 - mae: 0.765 - mean_q: 0.919 - mean_eps: 0.283 - ale.lives: 1.885\n",
      "\n",
      "Interval 282 (702500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0204\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 704999 -> Avg=18.80, Max=32.00 -> [32.0, 8.0, 8.0, 22.0, 24.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-5.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 113s 45ms/step - reward: 0.0204\n",
      "3 episodes - episode_reward: 19.000 [18.000, 20.000] - loss: 0.011 - mae: 0.765 - mean_q: 0.918 - mean_eps: 0.281 - ale.lives: 1.660\n",
      "\n",
      "Interval 283 (705000 steps performed)\n",
      "2500/2500 [==============================] - 93s 37ms/step - reward: 0.0136\n",
      "4 episodes - episode_reward: 8.000 [0.000, 13.000] - loss: 0.011 - mae: 0.769 - mean_q: 0.925 - mean_eps: 0.278 - ale.lives: 2.002\n",
      "\n",
      "Interval 284 (707500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0224\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 709999 -> Avg=14.40, Max=18.00 -> [16.0, 18.0, 9.0, 18.0, 11.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-10.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1422112\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 709999\n",
      "    [POLICY] epsilon actual: 0.2742\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 100s 40ms/step - reward: 0.0224\n",
      "3 episodes - episode_reward: 18.667 [11.000, 26.000] - loss: 0.010 - mae: 0.760 - mean_q: 0.911 - mean_eps: 0.276 - ale.lives: 1.872\n",
      "\n",
      "Interval 285 (710000 steps performed)\n",
      "2500/2500 [==============================] - 91s 36ms/step - reward: 0.0192\n",
      "4 episodes - episode_reward: 9.500 [2.000, 15.000] - loss: 0.012 - mae: 0.776 - mean_q: 0.931 - mean_eps: 0.273 - ale.lives: 2.243\n",
      "\n",
      "Interval 286 (712500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 714999 -> Avg=2.00, Max=6.00 -> [6.0, 1.0, 1.0, 1.0, 1.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-22.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 114s 46ms/step - reward: 0.0132\n",
      "4 episodes - episode_reward: 10.750 [8.000, 14.000] - loss: 0.011 - mae: 0.779 - mean_q: 0.935 - mean_eps: 0.270 - ale.lives: 2.236\n",
      "\n",
      "Interval 287 (715000 steps performed)\n",
      "2500/2500 [==============================] - 88s 35ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 8.000 [2.000, 16.000] - loss: 0.013 - mae: 0.775 - mean_q: 0.930 - mean_eps: 0.268 - ale.lives: 2.059\n",
      "\n",
      "Interval 288 (717500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0152\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 719999 -> Avg=19.20, Max=28.00 -> [27.0, 14.0, 28.0, 6.0, 21.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-5.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1442142\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 719999\n",
      "    [POLICY] epsilon actual: 0.2640\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 101s 40ms/step - reward: 0.0152\n",
      "3 episodes - episode_reward: 14.667 [8.000, 19.000] - loss: 0.012 - mae: 0.777 - mean_q: 0.933 - mean_eps: 0.265 - ale.lives: 1.829\n",
      "\n",
      "Interval 289 (720000 steps performed)\n",
      "2500/2500 [==============================] - 87s 35ms/step - reward: 0.0132\n",
      "4 episodes - episode_reward: 7.000 [0.000, 11.000] - loss: 0.010 - mae: 0.789 - mean_q: 0.945 - mean_eps: 0.263 - ale.lives: 2.029\n",
      "\n",
      "Interval 290 (722500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0100\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 724999 -> Avg=2.80, Max=6.00 -> [1.0, 2.0, 2.0, 3.0, 6.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-21.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 113s 45ms/step - reward: 0.0100\n",
      "3 episodes - episode_reward: 7.333 [7.000, 8.000] - loss: 0.011 - mae: 0.776 - mean_q: 0.931 - mean_eps: 0.260 - ale.lives: 1.928\n",
      "\n",
      "Interval 291 (725000 steps performed)\n",
      "2500/2500 [==============================] - 99s 40ms/step - reward: 0.0176\n",
      "4 episodes - episode_reward: 11.750 [5.000, 26.000] - loss: 0.012 - mae: 0.789 - mean_q: 0.945 - mean_eps: 0.258 - ale.lives: 1.846\n",
      "\n",
      "Interval 292 (727500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0172\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 729999 -> Avg=18.60, Max=24.00 -> [23.0, 10.0, 22.0, 24.0, 14.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-5.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1462170\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 729999\n",
      "    [POLICY] epsilon actual: 0.2538\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 114s 45ms/step - reward: 0.0172\n",
      "3 episodes - episode_reward: 13.333 [11.000, 16.000] - loss: 0.010 - mae: 0.772 - mean_q: 0.926 - mean_eps: 0.255 - ale.lives: 1.851\n",
      "\n",
      "Interval 293 (730000 steps performed)\n",
      "2500/2500 [==============================] - 96s 38ms/step - reward: 0.0212\n",
      "3 episodes - episode_reward: 18.000 [8.000, 24.000] - loss: 0.012 - mae: 0.803 - mean_q: 0.963 - mean_eps: 0.253 - ale.lives: 2.197\n",
      "\n",
      "Interval 294 (732500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0168\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 734999 -> Avg=9.40, Max=15.00 -> [7.0, 6.0, 7.0, 12.0, 15.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-15.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 106s 42ms/step - reward: 0.0168\n",
      "5 episodes - episode_reward: 9.600 [6.000, 15.000] - loss: 0.010 - mae: 0.794 - mean_q: 0.955 - mean_eps: 0.250 - ale.lives: 2.130\n",
      "\n",
      "Interval 295 (735000 steps performed)\n",
      "2500/2500 [==============================] - 98s 39ms/step - reward: 0.0160\n",
      "4 episodes - episode_reward: 9.000 [1.000, 15.000] - loss: 0.011 - mae: 0.799 - mean_q: 0.959 - mean_eps: 0.247 - ale.lives: 2.110\n",
      "\n",
      "Interval 296 (737500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0140\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 739999 -> Avg=0.80, Max=2.00 -> [0.0, 1.0, 1.0, 0.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-23.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1482202\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 739999\n",
      "    [POLICY] epsilon actual: 0.2436\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 138s 55ms/step - reward: 0.0140\n",
      "4 episodes - episode_reward: 9.750 [7.000, 17.000] - loss: 0.011 - mae: 0.794 - mean_q: 0.952 - mean_eps: 0.245 - ale.lives: 2.029\n",
      "\n",
      "Interval 297 (740000 steps performed)\n",
      "2500/2500 [==============================] - 113s 45ms/step - reward: 0.0108\n",
      "3 episodes - episode_reward: 7.333 [1.000, 12.000] - loss: 0.011 - mae: 0.795 - mean_q: 0.954 - mean_eps: 0.242 - ale.lives: 1.918\n",
      "\n",
      "Interval 298 (742500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0124\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 744999 -> Avg=1.80, Max=3.00 -> [1.0, 3.0, 1.0, 2.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-22.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 120s 48ms/step - reward: 0.0128\n",
      "4 episodes - episode_reward: 8.500 [5.000, 13.000] - loss: 0.011 - mae: 0.799 - mean_q: 0.956 - mean_eps: 0.240 - ale.lives: 1.984\n",
      "\n",
      "Interval 299 (745000 steps performed)\n",
      "2500/2500 [==============================] - 115s 46ms/step - reward: 0.0176\n",
      "4 episodes - episode_reward: 11.000 [4.000, 17.000] - loss: 0.010 - mae: 0.793 - mean_q: 0.954 - mean_eps: 0.237 - ale.lives: 2.069\n",
      "\n",
      "Interval 300 (747500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0092\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 749999 -> Avg=16.00, Max=27.00 -> [17.0, 27.0, 12.0, 12.0, 12.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-8.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1502230\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 64\n",
      "    [TRAIN] Step actual: 749999\n",
      "    [POLICY] epsilon actual: 0.2333\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 127s 51ms/step - reward: 0.0092\n",
      "3 episodes - episode_reward: 8.333 [5.000, 10.000] - loss: 0.012 - mae: 0.792 - mean_q: 0.950 - mean_eps: 0.235 - ale.lives: 1.844\n",
      "\n",
      "Interval 301 (750000 steps performed)\n",
      "[BATCH] Step 750000: batch_size actualizado a 128\n",
      "2500/2500 [==============================] - 180s 72ms/step - reward: 0.0160\n",
      "4 episodes - episode_reward: 9.500 [2.000, 14.000] - loss: 0.011 - mae: 0.813 - mean_q: 0.973 - mean_eps: 0.232 - ale.lives: 2.088\n",
      "\n",
      "Interval 302 (752500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0244\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 754999 -> Avg=18.60, Max=25.00 -> [16.0, 24.0, 13.0, 15.0, 25.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-5.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 208s 83ms/step - reward: 0.0244\n",
      "3 episodes - episode_reward: 14.333 [7.000, 20.000] - loss: 0.010 - mae: 0.811 - mean_q: 0.973 - mean_eps: 0.230 - ale.lives: 1.975\n",
      "\n",
      "Interval 303 (755000 steps performed)\n",
      "2500/2500 [==============================] - 193s 77ms/step - reward: 0.0160\n",
      "5 episodes - episode_reward: 11.800 [3.000, 22.000] - loss: 0.011 - mae: 0.812 - mean_q: 0.973 - mean_eps: 0.227 - ale.lives: 2.168\n",
      "\n",
      "Interval 304 (757500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0152\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 759999 -> Avg=14.40, Max=24.00 -> [12.0, 18.0, 11.0, 7.0, 24.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-10.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1522262\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 759999\n",
      "    [POLICY] epsilon actual: 0.2231\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 210s 84ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 8.500 [5.000, 13.000] - loss: 0.011 - mae: 0.808 - mean_q: 0.969 - mean_eps: 0.224 - ale.lives: 1.832\n",
      "\n",
      "Interval 305 (760000 steps performed)\n",
      "2500/2500 [==============================] - 188s 75ms/step - reward: 0.0144\n",
      "2 episodes - episode_reward: 14.500 [7.000, 22.000] - loss: 0.011 - mae: 0.818 - mean_q: 0.982 - mean_eps: 0.222 - ale.lives: 2.092\n",
      "\n",
      "Interval 306 (762500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0188\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 764999 -> Avg=4.80, Max=8.00 -> [4.0, 3.0, 5.0, 8.0, 4.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-19.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 200s 80ms/step - reward: 0.0188\n",
      "3 episodes - episode_reward: 16.667 [13.000, 23.000] - loss: 0.010 - mae: 0.813 - mean_q: 0.974 - mean_eps: 0.219 - ale.lives: 1.836\n",
      "\n",
      "Interval 307 (765000 steps performed)\n",
      "2500/2500 [==============================] - 177s 71ms/step - reward: 0.0192\n",
      "3 episodes - episode_reward: 11.333 [10.000, 13.000] - loss: 0.011 - mae: 0.823 - mean_q: 0.985 - mean_eps: 0.217 - ale.lives: 1.925\n",
      "\n",
      "Interval 308 (767500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0124\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 769999 -> Avg=11.20, Max=12.00 -> [11.0, 12.0, 11.0, 11.0, 11.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-13.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1542284\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 769999\n",
      "    [POLICY] epsilon actual: 0.2129\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 209s 83ms/step - reward: 0.0124\n",
      "3 episodes - episode_reward: 14.000 [5.000, 31.000] - loss: 0.011 - mae: 0.817 - mean_q: 0.977 - mean_eps: 0.214 - ale.lives: 2.058\n",
      "\n",
      "Interval 309 (770000 steps performed)\n",
      "2500/2500 [==============================] - 188s 75ms/step - reward: 0.0124\n",
      "4 episodes - episode_reward: 10.250 [6.000, 14.000] - loss: 0.010 - mae: 0.824 - mean_q: 0.989 - mean_eps: 0.212 - ale.lives: 2.225\n",
      "\n",
      "Interval 310 (772500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0112\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 774999 -> Avg=12.00, Max=18.00 -> [7.0, 15.0, 18.0, 10.0, 10.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-12.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 202s 81ms/step - reward: 0.0112\n",
      "3 episodes - episode_reward: 9.000 [8.000, 11.000] - loss: 0.011 - mae: 0.829 - mean_q: 0.994 - mean_eps: 0.209 - ale.lives: 2.054\n",
      "\n",
      "Interval 311 (775000 steps performed)\n",
      "2500/2500 [==============================] - 185s 74ms/step - reward: 0.0124\n",
      "4 episodes - episode_reward: 7.500 [5.000, 9.000] - loss: 0.011 - mae: 0.820 - mean_q: 0.982 - mean_eps: 0.207 - ale.lives: 2.124\n",
      "\n",
      "Interval 312 (777500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0160\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 779999 -> Avg=0.40, Max=1.00 -> [0.0, 0.0, 1.0, 0.0, 1.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-24.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1562312\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 779999\n",
      "    [POLICY] epsilon actual: 0.2027\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 197s 79ms/step - reward: 0.0160\n",
      "3 episodes - episode_reward: 11.667 [7.000, 19.000] - loss: 0.011 - mae: 0.831 - mean_q: 0.996 - mean_eps: 0.204 - ale.lives: 2.228\n",
      "\n",
      "Interval 313 (780000 steps performed)\n",
      "2500/2500 [==============================] - 184s 74ms/step - reward: 0.0176\n",
      "3 episodes - episode_reward: 18.000 [11.000, 28.000] - loss: 0.010 - mae: 0.834 - mean_q: 1.000 - mean_eps: 0.201 - ale.lives: 1.857\n",
      "\n",
      "Interval 314 (782500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0164\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 784999 -> Avg=19.00, Max=32.00 -> [32.0, 18.0, 9.0, 19.0, 17.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-5.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 203s 81ms/step - reward: 0.0168\n",
      "3 episodes - episode_reward: 10.000 [7.000, 13.000] - loss: 0.009 - mae: 0.832 - mean_q: 0.998 - mean_eps: 0.199 - ale.lives: 2.114\n",
      "\n",
      "Interval 315 (785000 steps performed)\n",
      "2500/2500 [==============================] - 188s 75ms/step - reward: 0.0140\n",
      "5 episodes - episode_reward: 8.400 [3.000, 16.000] - loss: 0.011 - mae: 0.838 - mean_q: 1.005 - mean_eps: 0.196 - ale.lives: 2.089\n",
      "\n",
      "Interval 316 (787500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0200\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 789999 -> Avg=5.80, Max=12.00 -> [0.0, 3.0, 12.0, 3.0, 11.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-18.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1582338\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 789999\n",
      "    [POLICY] epsilon actual: 0.1924\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 194s 77ms/step - reward: 0.0200\n",
      "2 episodes - episode_reward: 24.000 [22.000, 26.000] - loss: 0.011 - mae: 0.836 - mean_q: 1.001 - mean_eps: 0.194 - ale.lives: 2.642\n",
      "\n",
      "Interval 317 (790000 steps performed)\n",
      "2500/2500 [==============================] - 189s 76ms/step - reward: 0.0176\n",
      "3 episodes - episode_reward: 13.333 [8.000, 20.000] - loss: 0.010 - mae: 0.832 - mean_q: 0.996 - mean_eps: 0.191 - ale.lives: 2.129\n",
      "\n",
      "Interval 318 (792500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0192\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 794999 -> Avg=20.40, Max=26.00 -> [26.0, 12.0, 21.0, 19.0, 24.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-4.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 212s 85ms/step - reward: 0.0192\n",
      "3 episodes - episode_reward: 18.000 [12.000, 24.000] - loss: 0.010 - mae: 0.829 - mean_q: 0.993 - mean_eps: 0.189 - ale.lives: 2.066\n",
      "\n",
      "Interval 319 (795000 steps performed)\n",
      "2500/2500 [==============================] - 186s 74ms/step - reward: 0.0148\n",
      "5 episodes - episode_reward: 8.600 [6.000, 12.000] - loss: 0.010 - mae: 0.831 - mean_q: 0.997 - mean_eps: 0.186 - ale.lives: 2.101\n",
      "\n",
      "Interval 320 (797500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0152\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 799999 -> Avg=12.60, Max=15.00 -> [12.0, 12.0, 15.0, 12.0, 12.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-11.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1602368\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 799999\n",
      "    [POLICY] epsilon actual: 0.1822\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 201s 80ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 7.750 [3.000, 11.000] - loss: 0.010 - mae: 0.832 - mean_q: 0.999 - mean_eps: 0.184 - ale.lives: 2.261\n",
      "\n",
      "Interval 321 (800000 steps performed)\n",
      "2500/2500 [==============================] - 195s 78ms/step - reward: 0.0204\n",
      "4 episodes - episode_reward: 14.500 [7.000, 20.000] - loss: 0.011 - mae: 0.854 - mean_q: 1.026 - mean_eps: 0.181 - ale.lives: 2.313\n",
      "\n",
      "Interval 322 (802500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0212\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 804999 -> Avg=10.40, Max=14.00 -> [14.0, 14.0, 2.0, 8.0, 14.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-14.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 198s 79ms/step - reward: 0.0212\n",
      "3 episodes - episode_reward: 14.667 [13.000, 17.000] - loss: 0.010 - mae: 0.846 - mean_q: 1.015 - mean_eps: 0.178 - ale.lives: 1.934\n",
      "\n",
      "Interval 323 (805000 steps performed)\n",
      "2500/2500 [==============================] - 197s 79ms/step - reward: 0.0144\n",
      "3 episodes - episode_reward: 10.000 [4.000, 17.000] - loss: 0.011 - mae: 0.849 - mean_q: 1.018 - mean_eps: 0.176 - ale.lives: 1.669\n",
      "\n",
      "Interval 324 (807500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0212\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 809999 -> Avg=11.00, Max=15.00 -> [15.0, 9.0, 10.0, 9.0, 12.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-13.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1622394\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 809999\n",
      "    [POLICY] epsilon actual: 0.1720\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 197s 79ms/step - reward: 0.0212\n",
      "3 episodes - episode_reward: 16.000 [13.000, 18.000] - loss: 0.012 - mae: 0.856 - mean_q: 1.026 - mean_eps: 0.173 - ale.lives: 2.258\n",
      "\n",
      "Interval 325 (810000 steps performed)\n",
      "2500/2500 [==============================] - 168s 67ms/step - reward: 0.0152\n",
      "3 episodes - episode_reward: 14.000 [10.000, 20.000] - loss: 0.011 - mae: 0.856 - mean_q: 1.026 - mean_eps: 0.171 - ale.lives: 1.850\n",
      "\n",
      "Interval 326 (812500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0172\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 814999 -> Avg=14.80, Max=21.00 -> [15.0, 20.0, 13.0, 5.0, 21.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-9.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 185s 74ms/step - reward: 0.0172\n",
      "4 episodes - episode_reward: 13.500 [6.000, 22.000] - loss: 0.010 - mae: 0.857 - mean_q: 1.026 - mean_eps: 0.168 - ale.lives: 2.039\n",
      "\n",
      "Interval 327 (815000 steps performed)\n",
      "2500/2500 [==============================] - 176s 70ms/step - reward: 0.0216\n",
      "3 episodes - episode_reward: 12.333 [5.000, 19.000] - loss: 0.011 - mae: 0.864 - mean_q: 1.035 - mean_eps: 0.166 - ale.lives: 2.450\n",
      "\n",
      "Interval 328 (817500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0192\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 819999 -> Avg=11.40, Max=18.00 -> [16.0, 13.0, 18.0, 5.0, 5.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-13.00, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1642420\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 819999\n",
      "    [POLICY] epsilon actual: 0.1618\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 175s 70ms/step - reward: 0.0192\n",
      "3 episodes - episode_reward: 22.333 [17.000, 32.000] - loss: 0.010 - mae: 0.863 - mean_q: 1.034 - mean_eps: 0.163 - ale.lives: 2.147\n",
      "\n",
      "Interval 329 (820000 steps performed)\n",
      "2500/2500 [==============================] - 163s 65ms/step - reward: 0.0184\n",
      "3 episodes - episode_reward: 13.000 [3.000, 27.000] - loss: 0.010 - mae: 0.864 - mean_q: 1.035 - mean_eps: 0.161 - ale.lives: 1.901\n",
      "\n",
      "Interval 330 (822500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0152\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 824999 -> Avg=9.00, Max=12.00 -> [12.0, 12.0, 11.0, 5.0, 5.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-15.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 176s 70ms/step - reward: 0.0152\n",
      "4 episodes - episode_reward: 12.000 [7.000, 16.000] - loss: 0.010 - mae: 0.863 - mean_q: 1.035 - mean_eps: 0.158 - ale.lives: 1.890\n",
      "\n",
      "Interval 331 (825000 steps performed)\n",
      "2500/2500 [==============================] - 168s 67ms/step - reward: 0.0164\n",
      "4 episodes - episode_reward: 8.750 [0.000, 16.000] - loss: 0.011 - mae: 0.863 - mean_q: 1.033 - mean_eps: 0.155 - ale.lives: 1.951\n",
      "\n",
      "Interval 332 (827500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0192\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 829999 -> Avg=15.20, Max=21.00 -> [6.0, 21.0, 20.0, 12.0, 17.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-9.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1662446\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 829999\n",
      "    [POLICY] epsilon actual: 0.1516\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 193s 77ms/step - reward: 0.0192\n",
      "2 episodes - episode_reward: 15.500 [8.000, 23.000] - loss: 0.010 - mae: 0.868 - mean_q: 1.042 - mean_eps: 0.153 - ale.lives: 1.910\n",
      "\n",
      "Interval 333 (830000 steps performed)\n",
      "2500/2500 [==============================] - 186s 74ms/step - reward: 0.0140\n",
      "3 episodes - episode_reward: 16.333 [6.000, 23.000] - loss: 0.010 - mae: 0.869 - mean_q: 1.041 - mean_eps: 0.150 - ale.lives: 2.149\n",
      "\n",
      "Interval 334 (832500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0140\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 834999 -> Avg=12.60, Max=16.00 -> [14.0, 15.0, 5.0, 13.0, 16.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-11.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 200s 80ms/step - reward: 0.0140\n",
      "3 episodes - episode_reward: 14.333 [12.000, 16.000] - loss: 0.010 - mae: 0.872 - mean_q: 1.045 - mean_eps: 0.148 - ale.lives: 1.850\n",
      "\n",
      "Interval 335 (835000 steps performed)\n",
      "2500/2500 [==============================] - 182s 73ms/step - reward: 0.0140\n",
      "3 episodes - episode_reward: 8.667 [1.000, 13.000] - loss: 0.011 - mae: 0.867 - mean_q: 1.039 - mean_eps: 0.145 - ale.lives: 1.968\n",
      "\n",
      "Interval 336 (837500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0172\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 839999 -> Avg=4.20, Max=6.00 -> [5.0, 3.0, 6.0, 5.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-20.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1682470\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 839999\n",
      "    [POLICY] epsilon actual: 0.1413\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 186s 74ms/step - reward: 0.0172\n",
      "3 episodes - episode_reward: 15.000 [8.000, 28.000] - loss: 0.011 - mae: 0.873 - mean_q: 1.046 - mean_eps: 0.143 - ale.lives: 1.984\n",
      "\n",
      "Interval 337 (840000 steps performed)\n",
      "2500/2500 [==============================] - 182s 73ms/step - reward: 0.0124\n",
      "5 episodes - episode_reward: 7.200 [3.000, 11.000] - loss: 0.011 - mae: 0.890 - mean_q: 1.068 - mean_eps: 0.140 - ale.lives: 2.086\n",
      "\n",
      "Interval 338 (842500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0184\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 844999 -> Avg=19.80, Max=22.00 -> [15.0, 22.0, 22.0, 19.0, 21.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-4.60, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 184s 74ms/step - reward: 0.0184\n",
      "2 episodes - episode_reward: 13.000 [9.000, 17.000] - loss: 0.010 - mae: 0.876 - mean_q: 1.052 - mean_eps: 0.138 - ale.lives: 1.914\n",
      "\n",
      "Interval 339 (845000 steps performed)\n",
      "2500/2500 [==============================] - 165s 66ms/step - reward: 0.0176\n",
      "4 episodes - episode_reward: 15.250 [11.000, 23.000] - loss: 0.010 - mae: 0.875 - mean_q: 1.050 - mean_eps: 0.135 - ale.lives: 1.920\n",
      "\n",
      "Interval 340 (847500 steps performed)\n",
      "2499/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 849999 -> Avg=0.00, Max=0.00 -> [0.0, 0.0, 0.0, 0.0, 0.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-24.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1702498\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 849999\n",
      "    [POLICY] epsilon actual: 0.1311\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 312s 125ms/step - reward: 0.0156\n",
      "3 episodes - episode_reward: 14.667 [11.000, 22.000] - loss: 0.012 - mae: 0.885 - mean_q: 1.061 - mean_eps: 0.132 - ale.lives: 1.777\n",
      "\n",
      "Interval 341 (850000 steps performed)\n",
      "2500/2500 [==============================] - 239s 95ms/step - reward: 0.0184\n",
      "3 episodes - episode_reward: 11.667 [1.000, 23.000] - loss: 0.010 - mae: 0.901 - mean_q: 1.081 - mean_eps: 0.130 - ale.lives: 1.860\n",
      "\n",
      "Interval 342 (852500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0144\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 854999 -> Avg=18.20, Max=21.00 -> [19.0, 21.0, 16.0, 16.0, 19.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-6.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 196s 78ms/step - reward: 0.0144\n",
      "4 episodes - episode_reward: 12.000 [8.000, 16.000] - loss: 0.010 - mae: 0.904 - mean_q: 1.084 - mean_eps: 0.127 - ale.lives: 1.713\n",
      "\n",
      "Interval 343 (855000 steps performed)\n",
      "2500/2500 [==============================] - 175s 70ms/step - reward: 0.0200\n",
      "3 episodes - episode_reward: 12.333 [0.000, 23.000] - loss: 0.012 - mae: 0.903 - mean_q: 1.083 - mean_eps: 0.125 - ale.lives: 1.972\n",
      "\n",
      "Interval 344 (857500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0176\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 859999 -> Avg=1.00, Max=1.00 -> [1.0, 1.0, 1.0, 1.0, 1.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-23.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1722524\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 859999\n",
      "    [POLICY] epsilon actual: 0.1209\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 179s 72ms/step - reward: 0.0176\n",
      "3 episodes - episode_reward: 17.000 [15.000, 19.000] - loss: 0.011 - mae: 0.905 - mean_q: 1.083 - mean_eps: 0.122 - ale.lives: 1.924\n",
      "\n",
      "Interval 345 (860000 steps performed)\n",
      "2500/2500 [==============================] - 164s 66ms/step - reward: 0.0188\n",
      "3 episodes - episode_reward: 14.000 [6.000, 19.000] - loss: 0.011 - mae: 0.917 - mean_q: 1.098 - mean_eps: 0.120 - ale.lives: 2.246\n",
      "\n",
      "Interval 346 (862500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 864999 -> Avg=4.20, Max=6.00 -> [2.0, 4.0, 6.0, 5.0, 4.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-20.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 176s 70ms/step - reward: 0.0156\n",
      "3 episodes - episode_reward: 12.000 [6.000, 15.000] - loss: 0.010 - mae: 0.921 - mean_q: 1.106 - mean_eps: 0.117 - ale.lives: 2.315\n",
      "\n",
      "Interval 347 (865000 steps performed)\n",
      "2500/2500 [==============================] - 165s 66ms/step - reward: 0.0124\n",
      "4 episodes - episode_reward: 9.250 [2.000, 15.000] - loss: 0.011 - mae: 0.915 - mean_q: 1.097 - mean_eps: 0.115 - ale.lives: 1.816\n",
      "\n",
      "Interval 348 (867500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0156\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 869999 -> Avg=2.00, Max=5.00 -> [5.0, 1.0, 2.0, 1.0, 1.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-22.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1742550\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 869999\n",
      "    [POLICY] epsilon actual: 0.1107\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 173s 69ms/step - reward: 0.0156\n",
      "3 episodes - episode_reward: 14.333 [9.000, 19.000] - loss: 0.010 - mae: 0.914 - mean_q: 1.096 - mean_eps: 0.112 - ale.lives: 2.094\n",
      "\n",
      "Interval 349 (870000 steps performed)\n",
      "2500/2500 [==============================] - 163s 65ms/step - reward: 0.0128\n",
      "2 episodes - episode_reward: 10.000 [4.000, 16.000] - loss: 0.010 - mae: 0.925 - mean_q: 1.110 - mean_eps: 0.109 - ale.lives: 2.120\n",
      "\n",
      "Interval 350 (872500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0132\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 874999 -> Avg=10.20, Max=24.00 -> [24.0, 19.0, 2.0, 3.0, 3.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-14.20, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 174s 70ms/step - reward: 0.0132\n",
      "4 episodes - episode_reward: 11.750 [10.000, 16.000] - loss: 0.010 - mae: 0.918 - mean_q: 1.101 - mean_eps: 0.107 - ale.lives: 2.137\n",
      "\n",
      "Interval 351 (875000 steps performed)\n",
      "2500/2500 [==============================] - 169s 68ms/step - reward: 0.0132\n",
      "4 episodes - episode_reward: 7.500 [2.000, 16.000] - loss: 0.010 - mae: 0.919 - mean_q: 1.103 - mean_eps: 0.104 - ale.lives: 2.134\n",
      "\n",
      "Interval 352 (877500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0184\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 879999 -> Avg=10.00, Max=15.00 -> [8.0, 11.0, 15.0, 8.0, 8.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-14.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1762576\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 879999\n",
      "    [POLICY] epsilon actual: 0.1004\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 186s 74ms/step - reward: 0.0184\n",
      "3 episodes - episode_reward: 16.667 [14.000, 21.000] - loss: 0.011 - mae: 0.926 - mean_q: 1.111 - mean_eps: 0.102 - ale.lives: 1.892\n",
      "\n",
      "Interval 353 (880000 steps performed)\n",
      "2500/2500 [==============================] - 171s 68ms/step - reward: 0.0188\n",
      "4 episodes - episode_reward: 12.000 [1.000, 28.000] - loss: 0.009 - mae: 0.933 - mean_q: 1.119 - mean_eps: 0.099 - ale.lives: 2.154\n",
      "\n",
      "Interval 354 (882500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0104\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 884999 -> Avg=15.00, Max=26.00 -> [18.0, 6.0, 20.0, 26.0, 5.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-9.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 197s 79ms/step - reward: 0.0104\n",
      "3 episodes - episode_reward: 6.667 [5.000, 8.000] - loss: 0.010 - mae: 0.926 - mean_q: 1.112 - mean_eps: 0.097 - ale.lives: 2.080\n",
      "\n",
      "Interval 355 (885000 steps performed)\n",
      "2500/2500 [==============================] - 182s 73ms/step - reward: 0.0156\n",
      "3 episodes - episode_reward: 12.333 [6.000, 24.000] - loss: 0.010 - mae: 0.925 - mean_q: 1.110 - mean_eps: 0.094 - ale.lives: 2.061\n",
      "\n",
      "Interval 356 (887500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0168\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 889999 -> Avg=11.60, Max=22.00 -> [22.0, 16.0, 6.0, 6.0, 8.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-12.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1782604\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 889999\n",
      "    [POLICY] epsilon actual: 0.0902\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 190s 76ms/step - reward: 0.0168\n",
      "4 episodes - episode_reward: 11.000 [4.000, 16.000] - loss: 0.012 - mae: 0.928 - mean_q: 1.113 - mean_eps: 0.092 - ale.lives: 1.934\n",
      "\n",
      "Interval 357 (890000 steps performed)\n",
      "2500/2500 [==============================] - 178s 71ms/step - reward: 0.0184\n",
      "3 episodes - episode_reward: 12.000 [6.000, 18.000] - loss: 0.010 - mae: 0.938 - mean_q: 1.126 - mean_eps: 0.089 - ale.lives: 1.882\n",
      "\n",
      "Interval 358 (892500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0168\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 894999 -> Avg=6.60, Max=11.00 -> [7.0, 5.0, 11.0, 8.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-17.80, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "2500/2500 [==============================] - 179s 72ms/step - reward: 0.0168\n",
      "4 episodes - episode_reward: 12.250 [4.000, 20.000] - loss: 0.010 - mae: 0.937 - mean_q: 1.124 - mean_eps: 0.086 - ale.lives: 1.988\n",
      "\n",
      "Interval 359 (895000 steps performed)\n",
      "2500/2500 [==============================] - 179s 71ms/step - reward: 0.0208\n",
      "4 episodes - episode_reward: 13.750 [9.000, 18.000] - loss: 0.010 - mae: 0.939 - mean_q: 1.125 - mean_eps: 0.084 - ale.lives: 1.875\n",
      "\n",
      "Interval 360 (897500 steps performed)\n",
      "2497/2500 [============================>.] - ETA: 0s - reward: 0.0160\n",
      ">>> 24.4\n",
      "\n",
      ">>> Step 899999 -> Avg=3.00, Max=4.00 -> [3.0, 2.0, 4.0, 4.0, 2.0] -> Mejor Actual: 24.40\n",
      "[AdaptiveElite] Δ=-21.40, elite_fraction=0.15\n",
      "[AdaptiveElite] Estancamiento → Reduzco elite_fraction a 0.15\n",
      "\n",
      ">>> Memoria e Hiperparametros:\n",
      "    [MEMORY] Memory Size: 1802630\n",
      "    [MEMORY] Elite Fraction: 0.15\n",
      "    [TRAIN] Batch Size: 128\n",
      "    [TRAIN] Step actual: 899999\n",
      "    [POLICY] epsilon actual: 0.0800\n",
      "    [LR] Learning rate actual: 0.010000\n",
      "2500/2500 [==============================] - 189s 75ms/step - reward: 0.0160\n",
      "2 episodes - episode_reward: 16.000 [15.000, 17.000] - loss: 0.009 - mae: 0.927 - mean_q: 1.113 - mean_eps: 0.081 - ale.lives: 2.256\n",
      "\n",
      "Interval 361 (900000 steps performed)\n",
      " 809/2500 [========>.....................] - ETA: 2:00 - reward: 0.0136done, took 36400.260 seconds\n"
     ]
    }
   ],
   "source": [
    "nombre_modelo = 'Agente_1M_GPU_Local'\n",
    "\n",
    "# entrenamiento del agent\n",
    "#weights_filename = 'apr_g9_dqn_{}_weights.h5f'.format(env_name)\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "checkpoint_weights_filename = nombre_modelo + '/apr_g9_dqn_' + env_name + '_weights_{step}_1M_GPU_Local.h5f'\n",
    "log_filename = nombre_modelo + '/apr_g9_dqn_{}_1M_GPU_Local_log.json'.format(env_name)\n",
    "total_steps = 1000000\n",
    "\n",
    "callbacks = [\n",
    "    ModelIntervalCheckpoint(checkpoint_weights_filename, interval=total_steps/100),\n",
    "    FileLogger(log_filename, interval=total_steps/100),\n",
    "    #SaveBestRewardCallback(env, filename='best_model_weights_6M.h5f', test_episodes=5, test_interval=500),\n",
    "    #AdaptiveEliteFractionCallback(env, monitor_interval=100, improve_threshold=1.0, decay_threshold=0.1, decay_factor=0.7, grow_factor=0.7, min_elite=0.2, max_elite=0.8),\n",
    "    AdaptiveEliteTuningAndSaveBestModelCallback(env, filename=nombre_modelo + '/best_model_weights_1M_GPU_Local.h5f', test_episodes=5, \n",
    "                                                warmup_steps=dqn.nb_steps_warmup, monitor_interval=5000, \n",
    "                                                improve_threshold=1.0, decay_threshold=0.1, decay_factor=0.9, \n",
    "                                                grow_factor=1.1, min_elite=0.15, max_elite=0.85),\n",
    "    BatchSizeSchedulerCallback(total_steps=total_steps, initial_bs=32, schedule=[(1/3, 2), (3/4, 2)]),\n",
    "    HyperparameterMonitorCallback(interval=total_steps/100)\n",
    "]\n",
    "entrenamiento = dqn.fit(env, callbacks=callbacks, nb_steps=total_steps, log_interval=2500, visualize=False, verbose=1)\n",
    "\n",
    "# se graban los pesos finales luego de finalizar el entrenamiento\n",
    "#dqn.save_weights('apr_g9_dqn_{}_weights.h5f'.format(env_name), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OHYryKd1Gb2b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 27.000, steps: 1163\n",
      "Episode 2: reward: 14.000, steps: 816\n",
      "Episode 3: reward: 19.000, steps: 1064\n",
      "Episode 4: reward: 16.000, steps: 940\n",
      "Episode 5: reward: 21.000, steps: 1149\n",
      "Episode 6: reward: 21.000, steps: 925\n",
      "Episode 7: reward: 17.000, steps: 851\n",
      "Episode 8: reward: 22.000, steps: 937\n",
      "Episode 9: reward: 19.000, steps: 864\n",
      "Episode 10: reward: 11.000, steps: 676\n",
      "Episode 11: reward: 19.000, steps: 1063\n",
      "Episode 12: reward: 14.000, steps: 817\n",
      "Episode 13: reward: 24.000, steps: 1106\n",
      "Episode 14: reward: 22.000, steps: 1073\n",
      "Episode 15: reward: 24.000, steps: 1169\n",
      "Episode 16: reward: 24.000, steps: 1478\n",
      "Episode 17: reward: 16.000, steps: 802\n",
      "Episode 18: reward: 16.000, steps: 699\n",
      "Episode 19: reward: 15.000, steps: 875\n",
      "Episode 20: reward: 20.000, steps: 797\n",
      "Episode 21: reward: 23.000, steps: 1259\n",
      "Episode 22: reward: 15.000, steps: 818\n",
      "Episode 23: reward: 18.000, steps: 878\n",
      "Episode 24: reward: 16.000, steps: 634\n",
      "Episode 25: reward: 17.000, steps: 757\n",
      "Episode 26: reward: 18.000, steps: 1130\n",
      "Episode 27: reward: 19.000, steps: 1006\n",
      "Episode 28: reward: 19.000, steps: 1150\n",
      "Episode 29: reward: 20.000, steps: 1027\n",
      "Episode 30: reward: 14.000, steps: 656\n",
      "Episode 31: reward: 20.000, steps: 974\n",
      "Episode 32: reward: 21.000, steps: 1099\n",
      "Episode 33: reward: 18.000, steps: 1094\n",
      "Episode 34: reward: 12.000, steps: 1230\n",
      "Episode 35: reward: 16.000, steps: 772\n",
      "Episode 36: reward: 29.000, steps: 1352\n",
      "Episode 37: reward: 22.000, steps: 1197\n",
      "Episode 38: reward: 12.000, steps: 765\n",
      "Episode 39: reward: 16.000, steps: 805\n",
      "Episode 40: reward: 12.000, steps: 679\n",
      "Episode 41: reward: 25.000, steps: 1094\n",
      "Episode 42: reward: 15.000, steps: 792\n",
      "Episode 43: reward: 32.000, steps: 1728\n",
      "Episode 44: reward: 20.000, steps: 1097\n",
      "Episode 45: reward: 26.000, steps: 1334\n",
      "Episode 46: reward: 19.000, steps: 942\n",
      "Episode 47: reward: 19.000, steps: 1134\n",
      "Episode 48: reward: 23.000, steps: 1398\n",
      "Episode 49: reward: 23.000, steps: 1252\n",
      "Episode 50: reward: 22.000, steps: 1124\n",
      "Episode 51: reward: 19.000, steps: 1329\n",
      "Episode 52: reward: 16.000, steps: 790\n",
      "Episode 53: reward: 19.000, steps: 979\n",
      "Episode 54: reward: 15.000, steps: 734\n",
      "Episode 55: reward: 17.000, steps: 1179\n",
      "Episode 56: reward: 17.000, steps: 1276\n",
      "Episode 57: reward: 19.000, steps: 1028\n",
      "Episode 58: reward: 22.000, steps: 1018\n",
      "Episode 59: reward: 10.000, steps: 738\n",
      "Episode 60: reward: 30.000, steps: 1538\n",
      "Episode 61: reward: 24.000, steps: 1246\n",
      "Episode 62: reward: 22.000, steps: 982\n",
      "Episode 63: reward: 15.000, steps: 694\n",
      "Episode 64: reward: 11.000, steps: 667\n",
      "Episode 65: reward: 11.000, steps: 654\n",
      "Episode 66: reward: 26.000, steps: 1110\n",
      "Episode 67: reward: 25.000, steps: 1963\n",
      "Episode 68: reward: 24.000, steps: 1073\n",
      "Episode 69: reward: 20.000, steps: 1174\n",
      "Episode 70: reward: 14.000, steps: 812\n",
      "Episode 71: reward: 14.000, steps: 939\n",
      "Episode 72: reward: 25.000, steps: 1114\n",
      "Episode 73: reward: 17.000, steps: 920\n",
      "Episode 74: reward: 22.000, steps: 1124\n",
      "Episode 75: reward: 15.000, steps: 680\n",
      "Episode 76: reward: 15.000, steps: 652\n",
      "Episode 77: reward: 26.000, steps: 1240\n",
      "Episode 78: reward: 22.000, steps: 1173\n",
      "Episode 79: reward: 16.000, steps: 699\n",
      "Episode 80: reward: 23.000, steps: 1611\n",
      "Episode 81: reward: 8.000, steps: 510\n",
      "Episode 82: reward: 21.000, steps: 744\n",
      "Episode 83: reward: 22.000, steps: 1289\n",
      "Episode 84: reward: 16.000, steps: 853\n",
      "Episode 85: reward: 25.000, steps: 1042\n",
      "Episode 86: reward: 27.000, steps: 980\n",
      "Episode 87: reward: 26.000, steps: 1187\n",
      "Episode 88: reward: 26.000, steps: 1501\n",
      "Episode 89: reward: 14.000, steps: 934\n",
      "Episode 90: reward: 21.000, steps: 1143\n",
      "Episode 91: reward: 16.000, steps: 1297\n",
      "Episode 92: reward: 15.000, steps: 808\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import Monitor\n",
    "import numpy as np\n",
    "\n",
    "# cargar mejor modelo\n",
    "dqn.load_weights(nombre_modelo + '/best_model_weights_1M_GPU_Local.h5f')\n",
    "\n",
    "# test normal + guardar recompensas\n",
    "history = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "episode_rewards = history.history['episode_reward']\n",
    "np.save(nombre_modelo + '/test_rewards.npy', episode_rewards)\n",
    "\n",
    "# Calcular métricas\n",
    "promedio = np.mean(episode_rewards)\n",
    "desviacion = np.std(episode_rewards)\n",
    "mejor_reward = np.max(episode_rewards)\n",
    "mejor_epi = np.argmax(episode_rewards) + 1\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"\\nPromedio de recompensas: {promedio:.2f}\")\n",
    "print(f\"Desviación estándar: {desviacion:.2f}\")\n",
    "print(f\"Mejor Episodio {mejor_epi} con Recompensa = {mejor_reward}\")\n",
    "\n",
    "# Guardar la mejor recompensa en un archivo\n",
    "with open(nombre_modelo + '/mejor_ejecucion.txt', 'w') as f:\n",
    "    f.write(f\"Mejor episodio: {mejor_epi} con Recompensa: {mejor_reward}\\n\")\n",
    "    f.write(f\"Promedio: {promedio:.2f}, Desviación estándar: {desviacion:.2f}\\n\")\n",
    "\n",
    "# correr episodio con video\n",
    "# video_env = Monitor(gym.make(env_name), './video', force=True)\n",
    "# dqn.test(video_env, nb_episodes=1, visualize=False)\n",
    "# video_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "rewards = np.load(nombre_modelo + '/test_rewards.npy')\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Recompensa por episodio\")\n",
    "plt.xlabel(\"Episodio\")\n",
    "plt.ylabel(\"Recompensa\")\n",
    "plt.grid()\n",
    "plt.savefig(nombre_modelo + '/grafico_recompensas.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "plt.hist(rewards, bins=range(int(min(rewards)), int(max(rewards)) + 2), edgecolor='black')\n",
    "plt.title('Histograma de Recompensas')\n",
    "plt.xlabel('Recompensa por Episodio')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego de muchas pruebas y experimentos, llegamos a este agente que consiguió un promedio de 20 puntos, con un máximo de 32. Otros agentes previos llegaron a 40 y 41 puntos, pero con un promedio de recompensas menor entre 15 y 16 puntos. Por lo que valoramos más el promedio de los episodios por sobre un resultado individual que pueda ser superior. Asimismo, el actual modelo obtuvo una desviación estándar de X, mientras que otros modelos con mejor puntaje máximo mostraron mayor varianza, lo que los hace menos confiables.\n",
    "\n",
    "A lo largo de estas semanas fuimos haciendo cambios y agregando complejidad al modelo. \n",
    "Lo más relevante posiblemente sea pasar de una SequentialMemory que provee keras-rl a otros tipos de memoria. En primer lugar se desarrolló una PrioritizedMemory, donde las experiencias se priorizaron para luego utilizar las mejores durante el muestreo. Esto no tenía mucho sentido, ya que las recompensas son discretas, es decir, una acción puede no dar recompensa, o si lo hace, da 1, no hay valores intermedios y la priorización carecía de sentido. Además, el agente también aprende de experiencias de acciones que no dieron recompensas. \n",
    "Por lo dicho, implementamos una memoria basada en la idea de elitismo, donde a la hora de tomar un muestreo de experiencias lo hacemos con un porcentaje de experiencias positivas, para favorecer la explotación, y el resto de experiencias aleatorias hasta completar el batch_size. Cuando más bajo sea dicho porcentaje, más se favorece la exploración para poder salir de óptimos locales.\n",
    "La idea es que este porcentaje que mide el nivel de elitismo sea dinámico, por eso mediante un callback estamos evaluando si el modelo esta estancado o no, en caso de estarlo se va disminuyendo su valor, o aumentando en caso contrario. \n",
    "El actual modelo comienza con un valor del 60% de elitismo, y se ve como el mismo fue disminuyendo hasta llegar al mínimo de 15%.\n",
    "Además, la memoria tiene una capacidad del doble de los steps del entrenamiento, ya que vimos que se llena por encima del valor de los steps. \n",
    "\n",
    "Respecto al batch_size, también es dinámico. La idea fue ir incrementandolo a medida que avanzaba el entrenamiento para tener resultados más estables hacia el final. Se comienza con un batch size de 32, que es el valor estándar, y mediante un callback (BatchSizeSchedulerCallback) se incremente al doble cuando llegamos al 33% del entrenamiento, y otra vez al doble al llegar al 75% del mismo. \n",
    "\n",
    "Para la Policy se usó un LinearAnnealedPolicy con el valor del epsilon que va decayendo desde 1 hasta 0.08 a lo largo del 90% del total de steps, en este caso, 900.000.\n",
    "\n",
    "El agente DQN se compiló con un optimizador Adam con learning rate dinámico usando ExponentialDecay, el cual comienza en 0.0002 y cada 100.000 pasos se multiplica por 0.96, haciendo que disminuya su valor.\n",
    "\n",
    "Este último agente se entrenó con 1M de pasos, lo que le alcanzó para superar en promedio de recompensas a otros agentes entrenados en 4, 5 y 6 millones de steps, pero con menor complejidad. Mediante el AdaptiveEliteTuningAndSaveBestModelCallback se evalúa el modelo cada 5000 pasos para quedarnos con los mejores pesos si al evaluarlo en 5 episodios, el promedio supera al máximo encontrado hasta ese momento.\n",
    "\n",
    "Por último, como ayuda al modelo, en el AtariProcessor estamos haciendo un crop de las observaciones para eliminar los bordes laterales, y la información extra como el puntaje obtenido y la cantidad de vidas. También se agregó inicialización HeNormal a cada capa de la red neuronal así como un Dropout de 0.3 a las capas Dense, para ayudar a estabilizar el entrenamiento junto al batch_size elevado.\n",
    "\n",
    "Además, cada 10.000 steps, monitoreamos el estado de la memoria y el valor de estos hiperparametros dinámicos con otro callback (HyperparameterMonitorCallback).\n",
    "\n",
    "El diseño progresivo del agente priorizó la estabilidad y adaptabilidad. A pesar de entrenar con menos pasos que otros agentes previos, alcanzó un desempeño más confiable, demostrando que la calidad de las experiencias y la dinámica de los hiperparámetros son más relevantes que la cantidad de iteraciones.\n",
    "\n",
    "El mejor modelo durante el entrenamiento se encontró cerca de la mitad del mismo, en el step 455.000, si bien luego se encontraron otros modelos cercanos a este, ninguno logró superar su promedio de recompensas, lo cual indica que podria implementarse un mecanismo para frenar el entrenamiento si pasa una cantidad determinada de episodios sin mejoras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---\n",
    "*   Alumno 1: Granizo, Mateo\n",
    "*   Alumno 2: Maiolo, Pablo\n",
    "*   Alumno 3: Miglino, Diego\n",
    "  \n",
    "https://github.com/dmiglino/SpaceInvaders_ReinforcementLearning/tree/entrega"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env_apr_g9",
   "language": "python",
   "name": "env_apr_g9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
