{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUehXgCyIRdq"
   },
   "source": [
    "# Actividad - Proyecto práctico\n",
    "\n",
    "\n",
    "> La actividad se desarrollará en grupos pre-definidos de 2-3 alumnos. Se debe indicar los nombres en orden alfabético (de apellidos). Recordad que esta actividad se corresponde con un 30% de la nota final de la asignatura. Se debe entregar entregar el trabajo en la presente notebook.\n",
    "*   Alumno 1: Granizo, Mateo\n",
    "*   Alumno 2: Maiolo, Pablo\n",
    "*   Alumno 3: Miglino, Diego\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwpYlnjWJhS9"
   },
   "source": [
    "---\n",
    "## **PARTE 1** - Instalación y requisitos previos\n",
    "\n",
    "> Las prácticas han sido preparadas para poder realizarse en el entorno de trabajo de Google Colab. Sin embargo, esta plataforma presenta ciertas incompatibilidades a la hora de visualizar la renderización en gym. Por ello, para obtener estas visualizaciones, se deberá trasladar el entorno de trabajo a local. Por ello, el presente dosier presenta instrucciones para poder trabajar en ambos entornos. Siga los siguientes pasos para un correcto funcionamiento:\n",
    "1.   **LOCAL:** Preparar el enviroment, siguiendo las intrucciones detalladas en la sección *1.1.Preparar enviroment*.\n",
    "2.  **AMBOS:** Modificar las variables \"mount\" y \"drive_mount\" a la carpeta de trabajo en drive en el caso de estar en Colab, y ejecturar la celda *1.2.Localizar entorno de trabajo*.\n",
    "3. **COLAB:** se deberá ejecutar las celdas correspondientes al montaje de la carpeta de trabajo en Drive. Esta corresponde a la sección *1.3.Montar carpeta de datos local*.\n",
    "4.  **AMBOS:** Instalar las librerías necesarias, siguiendo la sección *1.4.Instalar librerías necesarias*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU2BPrK2JkP0"
   },
   "source": [
    "---\n",
    "### 1.1. Preparar enviroment (solo local)\n",
    "\n",
    "\n",
    "\n",
    "> Para preparar el entorno de trabajo en local, se han seguido los siguientes pasos:\n",
    "1. En Windows, puede ser necesario instalar las C++ Build Tools. Para ello, siga los siguientes pasos: https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30.\n",
    "2. Instalar Anaconda\n",
    "3. Siguiendo el código que se presenta comentado en la próxima celda: Crear un enviroment, cambiar la ruta de trabajo, e instalar librerías básicas.\n",
    "\n",
    "\n",
    "```\n",
    "conda create --name miar_rl python=3.8\n",
    "conda activate miar_rl\n",
    "cd \"PATH_TO_FOLDER\"\n",
    "conda install git\n",
    "pip install jupyter\n",
    "```\n",
    "\n",
    "\n",
    "4. Abrir la notebook con *jupyter-notebook*.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "jupyter-notebook\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-kixNPiJqTc"
   },
   "source": [
    "---\n",
    "### 1.2. Localizar entorno de trabajo: Google colab o local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_YDFwZ-JscI",
    "outputId": "01a99aa0-3d4e-4cd1-b1bc-309aef65070a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# ATENCIÓN!! Modificar ruta relativa a la práctica si es distinta (drive_root)\n",
    "mount='/content/gdrive'\n",
    "drive_root = mount + \"/My Drive/08_MIAR/actividades/TP_Grupal\"\n",
    "mount='./'\n",
    "\n",
    "try:\n",
    "  from google.colab import drive\n",
    "  IN_COLAB=True\n",
    "except:\n",
    "  IN_COLAB=False\n",
    "print(IN_COLAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Dp_a1iBJ0tf"
   },
   "source": [
    "---\n",
    "### 1.3. Montar carpeta de datos local (solo Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6n7MIefJ21i",
    "outputId": "9a6fc610-9fb0-4f63-8562-46754d7d75fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos en el directorio: \n",
      "['.git', '.ipynb_checkpoints', 'anaconda_projects', 'apr_g9_dqn_SpaceInvaders-v0_log.json', 'apr_g9_dqn_SpaceInvaders-v0_weights.h5f.data-00000-of-00001', 'apr_g9_dqn_SpaceInvaders-v0_weights.h5f.index', 'APR_Grupo_9.ipynb', 'checkpoint', 'README.md']\n"
     ]
    }
   ],
   "source": [
    "# Switch to the directory on the Google Drive that you want to use\n",
    "import os\n",
    "if IN_COLAB:\n",
    "  print(\"We're running Colab\")\n",
    "\n",
    "  if IN_COLAB:\n",
    "    # Mount the Google Drive at mount\n",
    "    print(\"Colab: mounting Google drive on \", mount)\n",
    "\n",
    "    drive.mount(mount)\n",
    "\n",
    "    # Create drive_root if it doesn't exist\n",
    "    create_drive_root = True\n",
    "    if create_drive_root:\n",
    "      print(\"\\nColab: making sure \", drive_root, \" exists.\")\n",
    "      os.makedirs(drive_root, exist_ok=True)\n",
    "\n",
    "    # Change to the directory\n",
    "    print(\"\\nColab: Changing directory to \", drive_root)\n",
    "    %cd $drive_root\n",
    "# Verify we're in the correct working directory\n",
    "%pwd\n",
    "print(\"Archivos en el directorio: \")\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1ZSL5bpJ560"
   },
   "source": [
    "---\n",
    "### 1.4. Instalar librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UbVRjvHCJ8UF",
    "outputId": "fe539761-ae1b-4e9f-95de-9a3cfa9ae8bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting gym==0.17.3\n",
      "  Using cached gym-0.17.3.tar.gz (1.6 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting scipy (from gym==0.17.3)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl.metadata (58 kB)\n",
      "Collecting numpy>=1.10.4 (from gym==0.17.3)\n",
      "  Downloading numpy-1.24.4-cp38-cp38-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
      "  Using cached pyglet-1.5.0-py2.py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
      "  Using cached cloudpickle-1.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting future (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3)\n",
      "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Using cached cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
      "Downloading numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.8/14.9 MB 7.7 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 12.8/14.9 MB 27.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.9/14.9 MB 30.2 MB/s eta 0:00:00\n",
      "Using cached pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
      "Downloading scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "   ---------------------------------------- 0.0/42.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 4.7/42.2 MB 22.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 5.0/42.2 MB 12.6 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.8/42.2 MB 10.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 9.7/42.2 MB 11.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 14.7/42.2 MB 13.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 20.4/42.2 MB 16.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 21.0/42.2 MB 13.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 22.0/42.2 MB 13.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 26.5/42.2 MB 13.5 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 30.4/42.2 MB 14.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 35.7/42.2 MB 15.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 42.2/42.2 MB 17.0 MB/s eta 0:00:00\n",
      "Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (setup.py): started\n",
      "  Building wheel for gym (setup.py): finished with status 'done'\n",
      "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654624 sha256=34f25f3a9182dac46aefb231e60c140b31cfd2c189e4d60df2d75261ff801181\n",
      "  Stored in directory: c:\\users\\mateo\\appdata\\local\\pip\\cache\\wheels\\84\\40\\e7\\14efb9870cfc92ac236d78cb721dce614ddec9666c8a5e0a35\n",
      "Successfully built gym\n",
      "Installing collected packages: numpy, future, cloudpickle, scipy, pyglet, gym\n",
      "Successfully installed cloudpickle-1.6.0 future-1.0.0 gym-0.17.3 numpy-1.24.4 pyglet-1.5.0 scipy-1.10.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts futurize.exe and pasteurize.exe are installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/Kojoley/atari-py.git\n",
      "  Cloning https://github.com/Kojoley/atari-py.git to c:\\users\\mateo\\appdata\\local\\temp\\pip-req-build-n3avqp0u\n",
      "  Resolved https://github.com/Kojoley/atari-py.git to commit 86a1e05c0a95e9e6233c3a413521fdb34ca8a089\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from atari-py==1.2.2) (1.24.4)\n",
      "Building wheels for collected packages: atari-py\n",
      "  Building wheel for atari-py (setup.py): started\n",
      "  Building wheel for atari-py (setup.py): still running...\n",
      "  Building wheel for atari-py (setup.py): finished with status 'done'\n",
      "  Created wheel for atari-py: filename=atari_py-1.2.2-cp38-cp38-win_amd64.whl size=556174 sha256=736c3afd0c86cceefdc8f970c25510f8578777de8d65b4034a23b0ca96dadb2c\n",
      "  Stored in directory: C:\\Users\\mateo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-s55ykek4\\wheels\\25\\11\\0c\\d37ea19ecec588ab95c4199a485d3a4de5284e9a08b89c8f3f\n",
      "Successfully built atari-py\n",
      "Installing collected packages: atari-py\n",
      "Successfully installed atari-py-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/Kojoley/atari-py.git 'C:\\Users\\mateo\\AppData\\Local\\Temp\\pip-req-build-n3avqp0u'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyglet==1.5.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: future in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from pyglet==1.5.0) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting h5py==3.1.0\n",
      "  Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17.5 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from h5py==3.1.0) (1.24.4)\n",
      "Downloading h5py-3.1.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 0.8/2.7 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.8/2.7 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: h5py\n",
      "Successfully installed h5py-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting Pillow==9.5.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-win_amd64.whl.metadata (9.7 kB)\n",
      "Downloading Pillow-9.5.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.6/2.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 MB 9.1 MB/s eta 0:00:00\n",
      "Installing collected packages: Pillow\n",
      "Successfully installed Pillow-9.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting keras-rl2==1.0.5Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script normalizer.exe is installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script markdown_py.exe is installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached keras_rl2-1.0.5-py3-none-any.whl.metadata (304 bytes)\n",
      "Collecting tensorflow (from keras-rl2==1.0.5)\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tensorflow-2.13.0-cp38-cp38-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading tensorflow_intel-2.13.0-cp38-cp38-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5) (3.1.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading numpy-1.24.3-cp38-cp38-win_amd64.whl.metadata (5.6 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading protobuf-4.25.8-cp38-cp38-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5) (75.3.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading wrapt-1.17.2-cp38-cp38-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading grpcio-1.70.0-cp38-cp38-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5) (0.44.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting requests<3,>=2.21.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5) (8.5.0)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading charset_normalizer-3.4.2-cp38-cp38-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Using cached MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5) (3.20.2)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow->keras-rl2==1.0.5)\n",
      "  Downloading oauthlib-3.3.1-py3-none-any.whl.metadata (7.9 kB)\n",
      "Using cached keras_rl2-1.0.5-py3-none-any.whl (52 kB)\n",
      "Downloading tensorflow-2.13.0-cp38-cp38-win_amd64.whl (1.9 kB)\n",
      "Downloading tensorflow_intel-2.13.0-cp38-cp38-win_amd64.whl (276.5 MB)\n",
      "   ---------------------------------------- 0.0/276.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/276.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/276.5 MB 7.7 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 9.7/276.5 MB 22.4 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 20.2/276.5 MB 34.5 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 29.4/276.5 MB 35.1 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 36.7/276.5 MB 37.0 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 38.0/276.5 MB 30.6 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 40.4/276.5 MB 28.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 46.4/276.5 MB 27.6 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 47.7/276.5 MB 26.0 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 68.4/276.5 MB 32.3 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 88.6/276.5 MB 37.9 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 102.0/276.5 MB 41.5 MB/s eta 0:00:05\n",
      "   -------------- ------------------------ 104.3/276.5 MB 38.5 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 108.5/276.5 MB 36.5 MB/s eta 0:00:05\n",
      "   --------------- ----------------------- 109.8/276.5 MB 35.1 MB/s eta 0:00:05\n",
      "   ---------------- ---------------------- 118.5/276.5 MB 34.9 MB/s eta 0:00:05\n",
      "   ------------------ -------------------- 133.4/276.5 MB 36.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 148.6/276.5 MB 38.8 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 163.6/276.5 MB 40.4 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 179.6/276.5 MB 42.2 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 194.8/276.5 MB 43.5 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 210.5/276.5 MB 44.8 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 226.0/276.5 MB 46.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 241.7/276.5 MB 47.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 257.9/276.5 MB 48.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  272.6/276.5 MB 51.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  276.3/276.5 MB 51.1 MB/s eta 0:00:01\n",
      "   --------------------------------------- 276.5/276.5 MB 48.6 MB/s eta 0:00:00\n",
      "Using cached absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp38-cp38-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.3/4.3 MB 86.1 MB/s eta 0:00:00\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 46.5 MB/s eta 0:00:00\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading numpy-1.24.3-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "   ---------------------------------------- 0.0/14.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 14.9/14.9 MB 71.9 MB/s eta 0:00:00\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-4.25.8-cp38-cp38-win_amd64.whl (413 kB)\n",
      "Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.6/5.6 MB 85.6 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 81.6 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Downloading wrapt-1.17.2-cp38-cp38-win_amd64.whl (38 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp38-cp38-win_amd64.whl (105 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl (17 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading oauthlib-3.3.1-py3-none-any.whl (160 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, oauthlib, numpy, MarkupSafe, keras, idna, grpcio, google-pasta, gast, charset_normalizer, certifi, cachetools, astunparse, absl-py, werkzeug, rsa, requests, pyasn1-modules, markdown, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow, keras-rl2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.13.2\n",
      "    Uninstalling typing_extensions-4.13.2:\n",
      "      Successfully uninstalled typing_extensions-4.13.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed MarkupSafe-2.1.5 absl-py-2.3.0 astunparse-1.6.3 cachetools-5.5.2 certifi-2025.6.15 charset_normalizer-3.4.2 flatbuffers-25.2.10 gast-0.4.0 google-auth-2.40.3 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.70.0 idna-3.10 keras-2.13.1 keras-rl2-1.0.5 libclang-18.1.1 markdown-3.7 numpy-1.24.3 oauthlib-3.3.1 opt-einsum-3.4.0 protobuf-4.25.8 pyasn1-0.6.1 pyasn1-modules-0.4.2 requests-2.32.4 requests-oauthlib-2.0.0 rsa-4.9.1 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 typing-extensions-4.5.0 urllib3-2.2.3 werkzeug-3.0.6 wrapt-1.17.2\n",
      "Collecting Keras==2.2.4\n",
      "  Using cached Keras-2.2.4-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from Keras==2.2.4) (1.24.3)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from Keras==2.2.4) (1.10.1)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from Keras==2.2.4) (1.17.0)\n",
      "Collecting pyyaml (from Keras==2.2.4)\n",
      "  Downloading PyYAML-6.0.2-cp38-cp38-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: h5py in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from Keras==2.2.4) (3.1.0)\n",
      "Collecting keras-applications>=1.0.6 (from Keras==2.2.4)\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting keras-preprocessing>=1.0.5 (from Keras==2.2.4)\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Using cached Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
      "Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Downloading PyYAML-6.0.2-cp38-cp38-win_amd64.whl (162 kB)\n",
      "Installing collected packages: pyyaml, keras-preprocessing, keras-applications, Keras\n",
      "  Attempting uninstall: Keras\n",
      "    Found existing installation: keras 2.13.1\n",
      "    Uninstalling keras-2.13.1:\n",
      "      Successfully uninstalled keras-2.13.1\n",
      "Successfully installed Keras-2.2.4 keras-applications-1.0.8 keras-preprocessing-1.1.2 pyyaml-6.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires keras<2.14,>=2.13.1, but you have keras 2.2.4 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.5.3Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading tensorflow-2.5.3-cp38-cp38-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting numpy~=1.19.2 (from tensorflow==2.5.3)\n",
      "  Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting absl-py~=0.10 (from tensorflow==2.5.3)\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow==2.5.3) (1.6.3)\n",
      "Collecting flatbuffers~=1.12.0 (from tensorflow==2.5.3)\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl.metadata (872 bytes)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow==2.5.3) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow==2.5.3) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow==2.5.3) (1.1.2)\n",
      "Collecting opt-einsum~=3.3.0 (from tensorflow==2.5.3)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow==2.5.3) (4.25.8)\n",
      "Collecting six~=1.15.0 (from tensorflow==2.5.3)\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting termcolor~=1.1.0 (from tensorflow==2.5.3)\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting typing-extensions~=3.7.4 (from tensorflow==2.5.3)\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow==2.5.3) (0.44.0)\n",
      "Collecting wrapt~=1.12.1 (from tensorflow==2.5.3)\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow==2.5.3) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow==2.5.3) (2.13.0)\n",
      "Collecting tensorflow-estimator<2.6.0,>=2.5.0 (from tensorflow==2.5.3)\n",
      "  Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting keras-nightly~=2.5.0.dev (from tensorflow==2.5.3)\n",
      "  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting grpcio~=1.34.0 (from tensorflow==2.5.3)\n",
      "  Downloading grpcio-1.34.1-cp38-cp38-win_amd64.whl.metadata (4.0 kB)\n",
      "INFO: pip is looking at multiple versions of tensorboard to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorboard~=2.5 (from tensorflow==2.5.3)\n",
      "  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading tensorboard-2.12.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading tensorboard-2.12.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading tensorboard-2.12.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.40.3)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard~=2.5->tensorflow==2.5.3)\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (3.7)\n",
      "Collecting protobuf>=3.9.2 (from tensorflow==2.5.3)\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-win_amd64.whl.metadata (699 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (75.3.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard~=2.5->tensorflow==2.5.3)\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard~=2.5->tensorflow==2.5.3)\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl.metadata (873 bytes)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.3) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.3) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.3) (8.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.3) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow==2.5.3) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.3) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.3) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.3) (3.3.1)\n",
      "Downloading tensorflow-2.5.3-cp38-cp38-win_amd64.whl (428.2 MB)\n",
      "   ---------------------------------------- 0.0/428.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/428.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/428.2 MB 8.4 MB/s eta 0:00:51\n",
      "   ---------------------------------------- 2.1/428.2 MB 7.8 MB/s eta 0:00:55\n",
      "   ---------------------------------------- 3.1/428.2 MB 5.8 MB/s eta 0:01:14\n",
      "   ---------------------------------------- 4.2/428.2 MB 5.1 MB/s eta 0:01:23\n",
      "   ---------------------------------------- 4.2/428.2 MB 5.1 MB/s eta 0:01:23\n",
      "   ---------------------------------------- 5.2/428.2 MB 4.6 MB/s eta 0:01:32\n",
      "    --------------------------------------- 6.3/428.2 MB 4.3 MB/s eta 0:01:39\n",
      "    --------------------------------------- 6.6/428.2 MB 3.7 MB/s eta 0:01:54\n",
      "    --------------------------------------- 7.3/428.2 MB 4.2 MB/s eta 0:01:42\n",
      "    --------------------------------------- 8.4/428.2 MB 3.9 MB/s eta 0:01:47\n",
      "    --------------------------------------- 9.4/428.2 MB 4.2 MB/s eta 0:01:41\n",
      "   - -------------------------------------- 11.5/428.2 MB 4.4 MB/s eta 0:01:35\n",
      "   - -------------------------------------- 11.5/428.2 MB 4.4 MB/s eta 0:01:35\n",
      "   - -------------------------------------- 13.6/428.2 MB 4.5 MB/s eta 0:01:32\n",
      "   - -------------------------------------- 15.7/428.2 MB 4.9 MB/s eta 0:01:25\n",
      "   - -------------------------------------- 17.8/428.2 MB 5.2 MB/s eta 0:01:19\n",
      "   - -------------------------------------- 18.9/428.2 MB 5.4 MB/s eta 0:01:16\n",
      "   - -------------------------------------- 19.9/428.2 MB 5.2 MB/s eta 0:01:19\n",
      "   -- ------------------------------------- 22.0/428.2 MB 5.4 MB/s eta 0:01:15\n",
      "   -- ------------------------------------- 22.0/428.2 MB 5.4 MB/s eta 0:01:15\n",
      "   -- ------------------------------------- 25.2/428.2 MB 5.6 MB/s eta 0:01:13\n",
      "   -- ------------------------------------- 26.2/428.2 MB 5.7 MB/s eta 0:01:11\n",
      "   -- ------------------------------------- 26.2/428.2 MB 5.7 MB/s eta 0:01:11\n",
      "   -- ------------------------------------- 27.3/428.2 MB 5.4 MB/s eta 0:01:15\n",
      "   -- ------------------------------------- 27.3/428.2 MB 5.4 MB/s eta 0:01:15\n",
      "   -- ------------------------------------- 27.3/428.2 MB 5.4 MB/s eta 0:01:15\n",
      "   -- ------------------------------------- 28.3/428.2 MB 4.9 MB/s eta 0:01:22\n",
      "   -- ------------------------------------- 28.3/428.2 MB 4.9 MB/s eta 0:01:22\n",
      "   -- ------------------------------------- 30.4/428.2 MB 4.9 MB/s eta 0:01:21\n",
      "   --- ------------------------------------ 32.5/428.2 MB 5.1 MB/s eta 0:01:18\n",
      "   --- ------------------------------------ 34.6/428.2 MB 5.2 MB/s eta 0:01:16\n",
      "   --- ------------------------------------ 35.7/428.2 MB 5.2 MB/s eta 0:01:16\n",
      "   --- ------------------------------------ 36.7/428.2 MB 5.2 MB/s eta 0:01:16\n",
      "   --- ------------------------------------ 37.7/428.2 MB 5.2 MB/s eta 0:01:16\n",
      "   --- ------------------------------------ 38.8/428.2 MB 5.2 MB/s eta 0:01:16\n",
      "   --- ------------------------------------ 39.8/428.2 MB 5.3 MB/s eta 0:01:14\n",
      "   --- ------------------------------------ 40.9/428.2 MB 5.3 MB/s eta 0:01:14\n",
      "   ---- ----------------------------------- 43.0/428.2 MB 5.3 MB/s eta 0:01:13\n",
      "   ---- ----------------------------------- 43.0/428.2 MB 5.3 MB/s eta 0:01:13\n",
      "   ---- ----------------------------------- 44.0/428.2 MB 5.2 MB/s eta 0:01:14\n",
      "   ---- ----------------------------------- 45.1/428.2 MB 5.2 MB/s eta 0:01:15\n",
      "   ---- ----------------------------------- 45.1/428.2 MB 5.2 MB/s eta 0:01:15\n",
      "   ---- ----------------------------------- 46.1/428.2 MB 5.1 MB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 47.2/428.2 MB 5.0 MB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 47.2/428.2 MB 5.0 MB/s eta 0:01:16\n",
      "   ---- ----------------------------------- 48.2/428.2 MB 5.0 MB/s eta 0:01:17\n",
      "   ---- ----------------------------------- 49.3/428.2 MB 4.9 MB/s eta 0:01:17\n",
      "   ---- ----------------------------------- 50.3/428.2 MB 4.9 MB/s eta 0:01:17\n",
      "   ---- ----------------------------------- 52.4/428.2 MB 5.0 MB/s eta 0:01:15\n",
      "   ---- ----------------------------------- 52.4/428.2 MB 5.0 MB/s eta 0:01:15\n",
      "   ---- ----------------------------------- 53.5/428.2 MB 5.0 MB/s eta 0:01:16\n",
      "   ----- ---------------------------------- 54.5/428.2 MB 4.9 MB/s eta 0:01:17\n",
      "   ----- ---------------------------------- 56.6/428.2 MB 5.0 MB/s eta 0:01:15\n",
      "   ----- ---------------------------------- 56.6/428.2 MB 5.0 MB/s eta 0:01:15\n",
      "   ----- ---------------------------------- 58.7/428.2 MB 5.0 MB/s eta 0:01:14\n",
      "   ----- ---------------------------------- 58.7/428.2 MB 5.0 MB/s eta 0:01:14\n",
      "   ----- ---------------------------------- 60.8/428.2 MB 5.0 MB/s eta 0:01:14\n",
      "   ----- ---------------------------------- 60.8/428.2 MB 5.0 MB/s eta 0:01:14\n",
      "   ----- ---------------------------------- 61.9/428.2 MB 5.0 MB/s eta 0:01:14\n",
      "   ----- ---------------------------------- 62.9/428.2 MB 4.9 MB/s eta 0:01:15\n",
      "   ----- ---------------------------------- 62.9/428.2 MB 4.9 MB/s eta 0:01:15\n",
      "   ------ --------------------------------- 64.7/428.2 MB 4.9 MB/s eta 0:01:15\n",
      "   ------ --------------------------------- 65.0/428.2 MB 4.9 MB/s eta 0:01:15\n",
      "   ------ --------------------------------- 66.1/428.2 MB 4.9 MB/s eta 0:01:15\n",
      "   ------ --------------------------------- 68.2/428.2 MB 4.9 MB/s eta 0:01:14\n",
      "   ------ --------------------------------- 69.2/428.2 MB 4.9 MB/s eta 0:01:13\n",
      "   ------ --------------------------------- 70.3/428.2 MB 4.9 MB/s eta 0:01:13\n",
      "   ------ --------------------------------- 73.4/428.2 MB 5.1 MB/s eta 0:01:10\n",
      "   ------- -------------------------------- 75.5/428.2 MB 5.1 MB/s eta 0:01:09\n",
      "   ------- -------------------------------- 76.5/428.2 MB 5.2 MB/s eta 0:01:09\n",
      "   ------- -------------------------------- 78.6/428.2 MB 5.2 MB/s eta 0:01:08\n",
      "   ------- -------------------------------- 79.7/428.2 MB 5.2 MB/s eta 0:01:07\n",
      "   ------- -------------------------------- 79.7/428.2 MB 5.2 MB/s eta 0:01:07\n",
      "   ------- -------------------------------- 80.7/428.2 MB 5.1 MB/s eta 0:01:08\n",
      "   ------- -------------------------------- 80.7/428.2 MB 5.1 MB/s eta 0:01:08\n",
      "   ------- -------------------------------- 81.8/428.2 MB 5.0 MB/s eta 0:01:09\n",
      "   ------- -------------------------------- 82.8/428.2 MB 5.1 MB/s eta 0:01:09\n",
      "   ------- -------------------------------- 83.9/428.2 MB 5.0 MB/s eta 0:01:09\n",
      "   -------- ------------------------------- 87.0/428.2 MB 5.2 MB/s eta 0:01:07\n",
      "   -------- ------------------------------- 88.1/428.2 MB 5.2 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 88.1/428.2 MB 5.2 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 90.2/428.2 MB 5.1 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 91.2/428.2 MB 5.2 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 92.3/428.2 MB 5.1 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 92.3/428.2 MB 5.1 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 93.3/428.2 MB 5.1 MB/s eta 0:01:07\n",
      "   -------- ------------------------------- 94.4/428.2 MB 5.1 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 95.4/428.2 MB 5.1 MB/s eta 0:01:06\n",
      "   -------- ------------------------------- 95.4/428.2 MB 5.1 MB/s eta 0:01:06\n",
      "   --------- ------------------------------ 97.5/428.2 MB 5.1 MB/s eta 0:01:06\n",
      "   --------- ------------------------------ 97.5/428.2 MB 5.1 MB/s eta 0:01:06\n",
      "   --------- ------------------------------ 97.5/428.2 MB 5.1 MB/s eta 0:01:06\n",
      "   --------- ------------------------------ 98.6/428.2 MB 5.0 MB/s eta 0:01:07\n",
      "   --------- ------------------------------ 98.6/428.2 MB 5.0 MB/s eta 0:01:07\n",
      "   --------- ------------------------------ 100.7/428.2 MB 5.0 MB/s eta 0:01:07\n",
      "   --------- ------------------------------ 102.8/428.2 MB 5.0 MB/s eta 0:01:06\n",
      "   --------- ------------------------------ 102.8/428.2 MB 5.0 MB/s eta 0:01:06\n",
      "   --------- ------------------------------ 103.8/428.2 MB 5.0 MB/s eta 0:01:06\n",
      "   --------- ------------------------------ 105.9/428.2 MB 5.0 MB/s eta 0:01:05\n",
      "   --------- ------------------------------ 107.0/428.2 MB 5.0 MB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 108.0/428.2 MB 5.0 MB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 109.1/428.2 MB 5.0 MB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 110.1/428.2 MB 5.0 MB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 111.1/428.2 MB 5.0 MB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 111.1/428.2 MB 5.0 MB/s eta 0:01:04\n",
      "   ---------- ----------------------------- 114.3/428.2 MB 5.0 MB/s eta 0:01:03\n",
      "   ---------- ----------------------------- 115.3/428.2 MB 5.1 MB/s eta 0:01:02\n",
      "   ---------- ----------------------------- 117.4/428.2 MB 5.1 MB/s eta 0:01:02\n",
      "   ----------- ---------------------------- 118.5/428.2 MB 5.1 MB/s eta 0:01:01\n",
      "   ----------- ---------------------------- 119.5/428.2 MB 5.1 MB/s eta 0:01:01\n",
      "   ----------- ---------------------------- 119.5/428.2 MB 5.1 MB/s eta 0:01:01\n",
      "   ----------- ---------------------------- 120.6/428.2 MB 5.1 MB/s eta 0:01:01\n",
      "   ----------- ---------------------------- 121.6/428.2 MB 5.1 MB/s eta 0:01:01\n",
      "   ----------- ---------------------------- 122.7/428.2 MB 5.1 MB/s eta 0:01:01\n",
      "   ----------- ---------------------------- 124.8/428.2 MB 5.1 MB/s eta 0:01:00\n",
      "   ----------- ---------------------------- 125.8/428.2 MB 5.1 MB/s eta 0:01:00\n",
      "   ----------- ---------------------------- 125.8/428.2 MB 5.1 MB/s eta 0:01:00\n",
      "   ------------ --------------------------- 129.0/428.2 MB 5.1 MB/s eta 0:00:59\n",
      "   ------------ --------------------------- 131.1/428.2 MB 5.2 MB/s eta 0:00:58\n",
      "   ------------ --------------------------- 131.1/428.2 MB 5.2 MB/s eta 0:00:58\n",
      "   ------------ --------------------------- 133.2/428.2 MB 5.2 MB/s eta 0:00:58\n",
      "   ------------ --------------------------- 134.2/428.2 MB 5.2 MB/s eta 0:00:57\n",
      "   ------------ --------------------------- 135.3/428.2 MB 5.2 MB/s eta 0:00:57\n",
      "   ------------ --------------------------- 136.1/428.2 MB 5.1 MB/s eta 0:00:57\n",
      "   ------------ --------------------------- 136.3/428.2 MB 5.1 MB/s eta 0:00:57\n",
      "   ------------ --------------------------- 137.4/428.2 MB 5.1 MB/s eta 0:00:57\n",
      "   ------------ --------------------------- 138.4/428.2 MB 5.1 MB/s eta 0:00:57\n",
      "   ------------- -------------------------- 139.5/428.2 MB 5.1 MB/s eta 0:00:57\n",
      "   ------------- -------------------------- 140.5/428.2 MB 5.1 MB/s eta 0:00:57\n",
      "   ------------- -------------------------- 141.6/428.2 MB 5.1 MB/s eta 0:00:57\n",
      "   ------------- -------------------------- 142.6/428.2 MB 5.1 MB/s eta 0:00:56\n",
      "   ------------- -------------------------- 143.7/428.2 MB 5.1 MB/s eta 0:00:56\n",
      "   ------------- -------------------------- 145.8/428.2 MB 5.1 MB/s eta 0:00:56\n",
      "   ------------- -------------------------- 145.8/428.2 MB 5.1 MB/s eta 0:00:56\n",
      "   ------------- -------------------------- 147.8/428.2 MB 5.1 MB/s eta 0:00:55\n",
      "   ------------- -------------------------- 147.8/428.2 MB 5.1 MB/s eta 0:00:55\n",
      "   ------------- -------------------------- 148.9/428.2 MB 5.1 MB/s eta 0:00:55\n",
      "   -------------- ------------------------- 149.9/428.2 MB 5.1 MB/s eta 0:00:55\n",
      "   -------------- ------------------------- 149.9/428.2 MB 5.1 MB/s eta 0:00:55\n",
      "   -------------- ------------------------- 151.0/428.2 MB 5.1 MB/s eta 0:00:55\n",
      "   -------------- ------------------------- 152.0/428.2 MB 5.1 MB/s eta 0:00:55\n",
      "   -------------- ------------------------- 154.1/428.2 MB 5.1 MB/s eta 0:00:54\n",
      "   -------------- ------------------------- 154.1/428.2 MB 5.1 MB/s eta 0:00:54\n",
      "   -------------- ------------------------- 155.2/428.2 MB 5.1 MB/s eta 0:00:54\n",
      "   -------------- ------------------------- 155.2/428.2 MB 5.1 MB/s eta 0:00:54\n",
      "   -------------- ------------------------- 158.3/428.2 MB 5.1 MB/s eta 0:00:53\n",
      "   --------------- ------------------------ 161.5/428.2 MB 5.2 MB/s eta 0:00:52\n",
      "   --------------- ------------------------ 161.5/428.2 MB 5.2 MB/s eta 0:00:52\n",
      "   --------------- ------------------------ 163.6/428.2 MB 5.2 MB/s eta 0:00:51\n",
      "   --------------- ------------------------ 164.6/428.2 MB 5.3 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 165.7/428.2 MB 5.2 MB/s eta 0:00:51\n",
      "   --------------- ------------------------ 166.7/428.2 MB 5.3 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 166.7/428.2 MB 5.3 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 167.8/428.2 MB 5.2 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 168.8/428.2 MB 5.2 MB/s eta 0:00:50\n",
      "   --------------- ------------------------ 169.9/428.2 MB 5.1 MB/s eta 0:00:51\n",
      "   --------------- ------------------------ 170.9/428.2 MB 5.1 MB/s eta 0:00:51\n",
      "   ---------------- ----------------------- 171.7/428.2 MB 5.1 MB/s eta 0:00:51\n",
      "   ---------------- ----------------------- 172.0/428.2 MB 5.1 MB/s eta 0:00:51\n",
      "   ---------------- ----------------------- 173.0/428.2 MB 5.0 MB/s eta 0:00:51\n",
      "   ---------------- ----------------------- 174.1/428.2 MB 5.0 MB/s eta 0:00:51\n",
      "   ---------------- ----------------------- 176.2/428.2 MB 5.0 MB/s eta 0:00:51\n",
      "   ---------------- ----------------------- 176.2/428.2 MB 5.0 MB/s eta 0:00:51\n",
      "   ---------------- ----------------------- 176.2/428.2 MB 5.0 MB/s eta 0:00:51\n",
      "   ---------------- ----------------------- 178.3/428.2 MB 5.1 MB/s eta 0:00:49\n",
      "   ---------------- ----------------------- 178.8/428.2 MB 5.1 MB/s eta 0:00:50\n",
      "   ---------------- ----------------------- 179.3/428.2 MB 5.1 MB/s eta 0:00:49\n",
      "   ---------------- ----------------------- 180.4/428.2 MB 5.1 MB/s eta 0:00:49\n",
      "   ---------------- ----------------------- 181.4/428.2 MB 5.1 MB/s eta 0:00:49\n",
      "   ---------------- ----------------------- 181.4/428.2 MB 5.1 MB/s eta 0:00:49\n",
      "   ----------------- ---------------------- 183.5/428.2 MB 5.0 MB/s eta 0:00:49\n",
      "   ----------------- ---------------------- 184.5/428.2 MB 5.0 MB/s eta 0:00:49\n",
      "   ----------------- ---------------------- 185.6/428.2 MB 5.0 MB/s eta 0:00:49\n",
      "   ----------------- ---------------------- 186.6/428.2 MB 5.0 MB/s eta 0:00:49\n",
      "   ----------------- ---------------------- 188.7/428.2 MB 5.0 MB/s eta 0:00:48\n",
      "   ----------------- ---------------------- 189.8/428.2 MB 5.1 MB/s eta 0:00:48\n",
      "   ----------------- ---------------------- 191.9/428.2 MB 5.1 MB/s eta 0:00:47\n",
      "   ------------------ --------------------- 192.9/428.2 MB 5.1 MB/s eta 0:00:47\n",
      "   ------------------ --------------------- 193.7/428.2 MB 5.0 MB/s eta 0:00:47\n",
      "   ------------------ --------------------- 195.0/428.2 MB 5.1 MB/s eta 0:00:46\n",
      "   ------------------ --------------------- 197.1/428.2 MB 5.1 MB/s eta 0:00:46\n",
      "   ------------------ --------------------- 198.2/428.2 MB 5.1 MB/s eta 0:00:45\n",
      "   ------------------ --------------------- 199.2/428.2 MB 5.2 MB/s eta 0:00:45\n",
      "   ------------------ --------------------- 200.3/428.2 MB 5.1 MB/s eta 0:00:45\n",
      "   ------------------ --------------------- 202.4/428.2 MB 5.2 MB/s eta 0:00:44\n",
      "   ------------------- -------------------- 204.5/428.2 MB 5.2 MB/s eta 0:00:43\n",
      "   ------------------- -------------------- 206.6/428.2 MB 5.3 MB/s eta 0:00:42\n",
      "   ------------------- -------------------- 209.5/428.2 MB 5.3 MB/s eta 0:00:42\n",
      "   ------------------- -------------------- 209.7/428.2 MB 5.3 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 210.8/428.2 MB 5.3 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 211.8/428.2 MB 5.3 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 212.9/428.2 MB 5.3 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 212.9/428.2 MB 5.3 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 213.9/428.2 MB 5.3 MB/s eta 0:00:41\n",
      "   ------------------- -------------------- 213.9/428.2 MB 5.3 MB/s eta 0:00:41\n",
      "   -------------------- ------------------- 215.0/428.2 MB 5.2 MB/s eta 0:00:41\n",
      "   -------------------- ------------------- 216.0/428.2 MB 5.2 MB/s eta 0:00:41\n",
      "   -------------------- ------------------- 217.1/428.2 MB 5.2 MB/s eta 0:00:41\n",
      "   -------------------- ------------------- 218.1/428.2 MB 5.2 MB/s eta 0:00:41\n",
      "   -------------------- ------------------- 219.2/428.2 MB 5.2 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 219.2/428.2 MB 5.2 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 221.2/428.2 MB 5.2 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 222.3/428.2 MB 5.3 MB/s eta 0:00:39\n",
      "   -------------------- ------------------- 222.3/428.2 MB 5.3 MB/s eta 0:00:39\n",
      "   -------------------- ------------------- 223.3/428.2 MB 5.2 MB/s eta 0:00:40\n",
      "   -------------------- ------------------- 224.4/428.2 MB 5.2 MB/s eta 0:00:39\n",
      "   --------------------- ------------------ 226.0/428.2 MB 5.2 MB/s eta 0:00:39\n",
      "   --------------------- ------------------ 227.5/428.2 MB 5.2 MB/s eta 0:00:39\n",
      "   --------------------- ------------------ 228.9/428.2 MB 5.1 MB/s eta 0:00:39\n",
      "   --------------------- ------------------ 231.7/428.2 MB 5.2 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 232.8/428.2 MB 5.2 MB/s eta 0:00:38\n",
      "   --------------------- ------------------ 234.9/428.2 MB 5.2 MB/s eta 0:00:38\n",
      "   ---------------------- ----------------- 235.9/428.2 MB 5.3 MB/s eta 0:00:37\n",
      "   ---------------------- ----------------- 237.0/428.2 MB 5.3 MB/s eta 0:00:37\n",
      "   ---------------------- ----------------- 238.0/428.2 MB 5.3 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 238.0/428.2 MB 5.3 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 239.1/428.2 MB 5.2 MB/s eta 0:00:37\n",
      "   ---------------------- ----------------- 241.2/428.2 MB 5.3 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 243.3/428.2 MB 5.2 MB/s eta 0:00:36\n",
      "   ---------------------- ----------------- 244.3/428.2 MB 5.2 MB/s eta 0:00:36\n",
      "   ----------------------- ---------------- 246.4/428.2 MB 5.3 MB/s eta 0:00:35\n",
      "   ----------------------- ---------------- 247.5/428.2 MB 5.3 MB/s eta 0:00:35\n",
      "   ----------------------- ---------------- 247.5/428.2 MB 5.3 MB/s eta 0:00:35\n",
      "   ----------------------- ---------------- 248.5/428.2 MB 5.3 MB/s eta 0:00:35\n",
      "   ----------------------- ---------------- 248.5/428.2 MB 5.3 MB/s eta 0:00:35\n",
      "   ----------------------- ---------------- 249.6/428.2 MB 5.3 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 250.6/428.2 MB 5.2 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 252.7/428.2 MB 5.3 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 252.7/428.2 MB 5.3 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 253.8/428.2 MB 5.2 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 253.8/428.2 MB 5.2 MB/s eta 0:00:34\n",
      "   ----------------------- ---------------- 255.9/428.2 MB 5.3 MB/s eta 0:00:33\n",
      "   ------------------------ --------------- 257.9/428.2 MB 5.4 MB/s eta 0:00:32\n",
      "   ------------------------ --------------- 259.0/428.2 MB 5.4 MB/s eta 0:00:32\n",
      "   ------------------------ --------------- 260.0/428.2 MB 5.4 MB/s eta 0:00:32\n",
      "   ------------------------ --------------- 261.1/428.2 MB 5.3 MB/s eta 0:00:32\n",
      "   ------------------------ --------------- 263.2/428.2 MB 5.4 MB/s eta 0:00:31\n",
      "   ------------------------ --------------- 264.2/428.2 MB 5.4 MB/s eta 0:00:31\n",
      "   ------------------------ --------------- 267.4/428.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ------------------------- -------------- 268.7/428.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ------------------------- -------------- 269.5/428.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ------------------------- -------------- 269.5/428.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ------------------------- -------------- 270.5/428.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ------------------------- -------------- 271.6/428.2 MB 5.4 MB/s eta 0:00:30\n",
      "   ------------------------- -------------- 274.7/428.2 MB 5.5 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 274.7/428.2 MB 5.5 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 275.8/428.2 MB 5.4 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 275.8/428.2 MB 5.4 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 276.8/428.2 MB 5.3 MB/s eta 0:00:29\n",
      "   ------------------------- -------------- 277.3/428.2 MB 5.3 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 278.9/428.2 MB 5.3 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 278.9/428.2 MB 5.3 MB/s eta 0:00:29\n",
      "   -------------------------- ------------- 280.0/428.2 MB 5.3 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 281.0/428.2 MB 5.3 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 282.1/428.2 MB 5.3 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 282.1/428.2 MB 5.3 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 283.1/428.2 MB 5.3 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 284.2/428.2 MB 5.3 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 285.2/428.2 MB 5.2 MB/s eta 0:00:28\n",
      "   -------------------------- ------------- 287.3/428.2 MB 5.2 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 287.3/428.2 MB 5.2 MB/s eta 0:00:27\n",
      "   -------------------------- ------------- 288.4/428.2 MB 5.2 MB/s eta 0:00:28\n",
      "   --------------------------- ------------ 290.5/428.2 MB 5.2 MB/s eta 0:00:27\n",
      "   --------------------------- ------------ 292.6/428.2 MB 5.3 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 293.6/428.2 MB 5.3 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 293.6/428.2 MB 5.3 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 293.6/428.2 MB 5.3 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 293.6/428.2 MB 5.3 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 293.6/428.2 MB 5.3 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 293.6/428.2 MB 5.3 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 294.6/428.2 MB 5.1 MB/s eta 0:00:27\n",
      "   --------------------------- ------------ 295.7/428.2 MB 5.1 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 295.7/428.2 MB 5.1 MB/s eta 0:00:26\n",
      "   --------------------------- ------------ 297.8/428.2 MB 5.1 MB/s eta 0:00:26\n",
      "   ---------------------------- ----------- 299.9/428.2 MB 5.1 MB/s eta 0:00:26\n",
      "   ---------------------------- ----------- 302.0/428.2 MB 5.2 MB/s eta 0:00:25\n",
      "   ---------------------------- ----------- 303.0/428.2 MB 5.2 MB/s eta 0:00:25\n",
      "   ---------------------------- ----------- 304.1/428.2 MB 5.2 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 306.2/428.2 MB 5.2 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 306.2/428.2 MB 5.2 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 307.2/428.2 MB 5.2 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 307.2/428.2 MB 5.2 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 308.3/428.2 MB 5.2 MB/s eta 0:00:24\n",
      "   ---------------------------- ----------- 309.3/428.2 MB 5.2 MB/s eta 0:00:23\n",
      "   ----------------------------- ---------- 311.4/428.2 MB 5.2 MB/s eta 0:00:23\n",
      "   ----------------------------- ---------- 311.4/428.2 MB 5.2 MB/s eta 0:00:23\n",
      "   ----------------------------- ---------- 314.6/428.2 MB 5.1 MB/s eta 0:00:23\n",
      "   ----------------------------- ---------- 314.6/428.2 MB 5.1 MB/s eta 0:00:23\n",
      "   ----------------------------- ---------- 315.6/428.2 MB 5.1 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 316.7/428.2 MB 5.1 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 317.7/428.2 MB 5.1 MB/s eta 0:00:22\n",
      "   ----------------------------- ---------- 320.9/428.2 MB 5.2 MB/s eta 0:00:21\n",
      "   ----------------------------- ---------- 320.9/428.2 MB 5.2 MB/s eta 0:00:21\n",
      "   ------------------------------ --------- 321.9/428.2 MB 5.1 MB/s eta 0:00:21\n",
      "   ------------------------------ --------- 323.0/428.2 MB 5.2 MB/s eta 0:00:21\n",
      "   ------------------------------ --------- 324.0/428.2 MB 5.2 MB/s eta 0:00:21\n",
      "   ------------------------------ --------- 325.1/428.2 MB 5.2 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 326.1/428.2 MB 5.2 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 326.1/428.2 MB 5.2 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 327.2/428.2 MB 5.2 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 328.2/428.2 MB 5.2 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 328.2/428.2 MB 5.2 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 329.3/428.2 MB 5.2 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 329.3/428.2 MB 5.2 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 329.3/428.2 MB 5.2 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 330.3/428.2 MB 5.1 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 331.4/428.2 MB 5.1 MB/s eta 0:00:20\n",
      "   ------------------------------ --------- 331.4/428.2 MB 5.1 MB/s eta 0:00:20\n",
      "   ------------------------------- -------- 332.4/428.2 MB 5.1 MB/s eta 0:00:19\n",
      "   ------------------------------- -------- 333.4/428.2 MB 5.1 MB/s eta 0:00:19\n",
      "   ------------------------------- -------- 334.5/428.2 MB 5.1 MB/s eta 0:00:19\n",
      "   ------------------------------- -------- 335.5/428.2 MB 5.0 MB/s eta 0:00:19\n",
      "   ------------------------------- -------- 335.5/428.2 MB 5.0 MB/s eta 0:00:19\n",
      "   ------------------------------- -------- 337.6/428.2 MB 5.1 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 338.7/428.2 MB 5.1 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 339.7/428.2 MB 5.0 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 340.8/428.2 MB 5.0 MB/s eta 0:00:18\n",
      "   ------------------------------- -------- 341.8/428.2 MB 5.0 MB/s eta 0:00:18\n",
      "   -------------------------------- ------- 342.9/428.2 MB 5.0 MB/s eta 0:00:18\n",
      "   -------------------------------- ------- 343.9/428.2 MB 5.0 MB/s eta 0:00:17\n",
      "   -------------------------------- ------- 345.2/428.2 MB 5.0 MB/s eta 0:00:17\n",
      "   -------------------------------- ------- 347.1/428.2 MB 5.0 MB/s eta 0:00:17\n",
      "   -------------------------------- ------- 348.1/428.2 MB 5.0 MB/s eta 0:00:17\n",
      "   -------------------------------- ------- 349.2/428.2 MB 5.0 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 350.2/428.2 MB 5.0 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 350.2/428.2 MB 5.0 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 351.3/428.2 MB 4.9 MB/s eta 0:00:16\n",
      "   -------------------------------- ------- 352.3/428.2 MB 4.9 MB/s eta 0:00:16\n",
      "   --------------------------------- ------ 353.4/428.2 MB 4.8 MB/s eta 0:00:16\n",
      "   --------------------------------- ------ 353.4/428.2 MB 4.8 MB/s eta 0:00:16\n",
      "   --------------------------------- ------ 355.5/428.2 MB 4.8 MB/s eta 0:00:16\n",
      "   --------------------------------- ------ 356.5/428.2 MB 4.8 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 357.6/428.2 MB 4.9 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 357.6/428.2 MB 4.9 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 358.6/428.2 MB 4.9 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 359.7/428.2 MB 4.9 MB/s eta 0:00:15\n",
      "   --------------------------------- ------ 360.7/428.2 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 361.8/428.2 MB 4.9 MB/s eta 0:00:14\n",
      "   --------------------------------- ------ 362.8/428.2 MB 4.9 MB/s eta 0:00:14\n",
      "   ---------------------------------- ----- 366.0/428.2 MB 4.9 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 367.0/428.2 MB 5.0 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 367.0/428.2 MB 5.0 MB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 369.4/428.2 MB 5.0 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 371.2/428.2 MB 5.0 MB/s eta 0:00:12\n",
      "   ---------------------------------- ----- 372.2/428.2 MB 5.0 MB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 375.4/428.2 MB 5.1 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 375.4/428.2 MB 5.1 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 376.4/428.2 MB 5.0 MB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 378.5/428.2 MB 5.0 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 380.6/428.2 MB 5.0 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 380.6/428.2 MB 5.0 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 382.7/428.2 MB 5.0 MB/s eta 0:00:10\n",
      "   ----------------------------------- ---- 383.8/428.2 MB 4.9 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 383.8/428.2 MB 4.9 MB/s eta 0:00:09\n",
      "   ----------------------------------- ---- 384.8/428.2 MB 5.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 385.9/428.2 MB 5.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 386.9/428.2 MB 5.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 386.9/428.2 MB 5.0 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 388.0/428.2 MB 4.9 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 391.1/428.2 MB 4.9 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 392.2/428.2 MB 4.9 MB/s eta 0:00:08\n",
      "   ------------------------------------ --- 394.3/428.2 MB 4.9 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 395.3/428.2 MB 4.9 MB/s eta 0:00:07\n",
      "   ------------------------------------ --- 395.3/428.2 MB 4.9 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 396.4/428.2 MB 5.0 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 397.4/428.2 MB 5.0 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 397.4/428.2 MB 5.0 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 398.5/428.2 MB 4.9 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 400.6/428.2 MB 5.0 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 400.6/428.2 MB 5.0 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 402.7/428.2 MB 5.0 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 403.7/428.2 MB 5.0 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 404.8/428.2 MB 4.9 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 404.8/428.2 MB 4.9 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 406.8/428.2 MB 4.9 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 407.9/428.2 MB 5.0 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 410.0/428.2 MB 4.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 410.0/428.2 MB 4.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 411.0/428.2 MB 4.9 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 412.1/428.2 MB 4.8 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 413.1/428.2 MB 4.8 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 415.2/428.2 MB 4.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 417.3/428.2 MB 4.9 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 417.3/428.2 MB 4.9 MB/s eta 0:00:03\n",
      "   ---------------------------------------  418.4/428.2 MB 4.9 MB/s eta 0:00:03\n",
      "   ---------------------------------------  419.4/428.2 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  420.5/428.2 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  422.6/428.2 MB 4.9 MB/s eta 0:00:02\n",
      "   ---------------------------------------  423.6/428.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  423.6/428.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  424.7/428.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  425.7/428.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  426.8/428.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  426.8/428.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  428.1/428.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  428.1/428.2 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 428.2/428.2 MB 4.9 MB/s eta 0:00:00\n",
      "Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Downloading grpcio-1.34.1-cp38-cp38-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 15.5 MB/s eta 0:00:00\n",
      "Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 61.5 MB/s eta 0:00:00\n",
      "Downloading numpy-1.19.5-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 10.7/13.3 MB 83.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.3/13.3 MB 30.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.1/13.3 MB 19.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.8/13.3 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 13.0 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "   ---------------------------------------- 0.0/6.0 MB ? eta -:--:--\n",
      "   -------------------------------------- - 5.8/6.0 MB 117.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.0/6.0 MB 19.3 MB/s eta 0:00:00\n",
      "Downloading protobuf-3.20.3-cp38-cp38-win_amd64.whl (904 kB)\n",
      "   ---------------------------------------- 0.0/904.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 904.4/904.4 kB ? eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.5.0-py2.py3-none-any.whl (462 kB)\n",
      "Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "   ---------------------------------------- 0.0/781.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 781.3/781.3 kB ? eta 0:00:00\n",
      "Building wheels for collected packages: termcolor, wrapt\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4855 sha256=112f56d068ebaa81d55520478914ee55a00f487bb666299d79b686abe71682b5\n",
      "  Stored in directory: c:\\users\\mateo\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp38-cp38-win_amd64.whl size=33211 sha256=cb7bd127192dac5e2a24dc0ac54b1c448a1c5c66a55890a46f6ac7b73f21486d\n",
      "  Stored in directory: c:\\users\\mateo\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\n",
      "Successfully built termcolor wrapt\n",
      "Installing collected packages: wrapt, typing-extensions, termcolor, tensorflow-estimator, tensorboard-plugin-wit, keras-nightly, flatbuffers, tensorboard-data-server, six, protobuf, numpy, opt-einsum, grpcio, absl-py, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.17.2\n",
      "    Uninstalling wrapt-1.17.2:\n",
      "      Successfully uninstalled wrapt-1.17.2\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "  Attempting uninstall: termcolor\n",
      "    Found existing installation: termcolor 2.4.0\n",
      "    Uninstalling termcolor-2.4.0:\n",
      "      Successfully uninstalled termcolor-2.4.0\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.13.0\n",
      "    Uninstalling tensorflow-estimator-2.13.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.13.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 25.2.10\n",
      "    Uninstalling flatbuffers-25.2.10:\n",
      "      Successfully uninstalled flatbuffers-25.2.10\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.2\n",
      "    Uninstalling tensorboard-data-server-0.7.2:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.17.0\n",
      "    Uninstalling six-1.17.0:\n",
      "      Successfully uninstalled six-1.17.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.8\n",
      "    Uninstalling protobuf-4.25.8:\n",
      "      Successfully uninstalled protobuf-4.25.8\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "  Attempting uninstall: opt-einsum\n",
      "    Found existing installation: opt_einsum 3.4.0\n",
      "    Uninstalling opt_einsum-3.4.0:\n",
      "      Successfully uninstalled opt_einsum-3.4.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.70.0\n",
      "    Uninstalling grpcio-1.70.0:\n",
      "      Successfully uninstalled grpcio-1.70.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 2.3.0\n",
      "    Uninstalling absl-py-2.3.0:\n",
      "      Successfully uninstalled absl-py-2.3.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.13.0\n",
      "    Uninstalling tensorboard-2.13.0:\n",
      "      Successfully uninstalled tensorboard-2.13.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.13.0\n",
      "    Uninstalling tensorflow-2.13.0:\n",
      "      Successfully uninstalled tensorflow-2.13.0\n",
      "Successfully installed absl-py-0.15.0 flatbuffers-1.12 google-auth-oauthlib-0.4.6 grpcio-1.34.1 keras-nightly-2.5.0.dev2021032900 numpy-1.19.5 opt-einsum-3.3.0 protobuf-3.20.3 six-1.15.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.5.3 tensorflow-estimator-2.5.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires absl-py>=1.0.0, but you have absl-py 0.15.0 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires flatbuffers>=23.1.21, but you have flatbuffers 1.12 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires keras<2.14,>=2.13.1, but you have keras 2.2.4 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.19.5 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorboard<2.14,>=2.13, but you have tensorboard 2.11.2 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires tensorflow-estimator<2.14,>=2.13.0, but you have tensorflow-estimator 2.5.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.0.1\n",
      "  Downloading torch-2.0.1-cp38-cp38-win_amd64.whl.metadata (23 kB)\n",
      "Collecting filelock (from torch==2.0.1)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from torch==2.0.1) (3.7.4.3)\n",
      "Collecting sympy (from torch==2.0.1)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.0.1)\n",
      "  Downloading networkx-3.1-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting jinja2 (from torch==2.0.1)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.0.1)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.0.1-cp38-cp38-win_amd64.whl (172.4 MB)\n",
      "   ---------------------------------------- 0.0/172.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/172.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.6/172.4 MB 9.4 MB/s eta 0:00:18\n",
      "   --- ------------------------------------ 16.8/172.4 MB 36.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 20.2/172.4 MB 35.5 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 21.0/172.4 MB 23.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 22.3/172.4 MB 21.1 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 26.5/172.4 MB 20.0 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 30.7/172.4 MB 20.1 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 34.9/172.4 MB 20.5 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 35.4/172.4 MB 18.3 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 37.0/172.4 MB 16.9 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 39.3/172.4 MB 16.8 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 45.4/172.4 MB 17.6 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 56.1/172.4 MB 19.9 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 81.5/172.4 MB 26.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 86.0/172.4 MB 26.6 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 88.3/172.4 MB 25.5 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 91.8/172.4 MB 25.4 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 95.2/172.4 MB 24.6 MB/s eta 0:00:04\n",
      "   ------------------------- ------------- 111.7/172.4 MB 27.2 MB/s eta 0:00:03\n",
      "   ----------------------------- --------- 130.8/172.4 MB 30.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 151.5/172.4 MB 33.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 157.8/172.4 MB 33.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 160.7/172.4 MB 32.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 163.6/172.4 MB 31.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 164.9/172.4 MB 30.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  172.2/172.4 MB 31.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 172.4/172.4 MB 30.2 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 56.6 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 63.1 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB ? eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, jinja2, filelock, torch\n",
      "Successfully installed filelock-3.16.1 jinja2-3.1.6 mpmath-1.3.0 networkx-3.1 sympy-1.13.3 torch-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script isympy.exe is installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting agents==1.4.0\n",
      "  Using cached agents-1.4.0.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: tensorflow in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from agents==1.4.0) (2.5.3)\n",
      "Requirement already satisfied: gym in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from agents==1.4.0) (0.17.3)\n",
      "Collecting ruamel.yaml (from agents==1.4.0)\n",
      "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from gym->agents==1.4.0) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from gym->agents==1.4.0) (1.19.5)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from gym->agents==1.4.0) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from gym->agents==1.4.0) (1.6.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->agents==1.4.0)\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp38-cp38-win_amd64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.15.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.12)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.20.3)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.15.0)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (3.7.4.3)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.44.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.12.1)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (0.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.11.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.5.0)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorflow->agents==1.4.0) (1.34.1)\n",
      "Requirement already satisfied: future in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym->agents==1.4.0) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.40.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (3.7)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (75.3.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from tensorboard~=2.5->tensorflow->agents==1.4.0) (3.0.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (4.9.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->agents==1.4.0) (8.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->agents==1.4.0) (2025.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (2.1.5)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow->agents==1.4.0) (0.6.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->agents==1.4.0) (3.3.1)\n",
      "Downloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
      "Downloading ruamel.yaml.clib-0.2.8-cp38-cp38-win_amd64.whl (118 kB)\n",
      "Building wheels for collected packages: agents\n",
      "  Building wheel for agents (setup.py): started\n",
      "  Building wheel for agents (setup.py): finished with status 'done'\n",
      "  Created wheel for agents: filename=agents-1.4.0-py3-none-any.whl size=62721 sha256=cca74523bf3eb838fd90a70bbc34aabe25cca54872ccd523f899595ac50fc339\n",
      "  Stored in directory: c:\\users\\mateo\\appdata\\local\\pip\\cache\\wheels\\87\\bb\\49\\8e14e51995c7fa61cb6cd10934be577d01a51d4236c62c80f2\n",
      "Successfully built agents\n",
      "Installing collected packages: ruamel.yaml.clib, ruamel.yaml, agents\n",
      "Successfully installed agents-1.4.0 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ejecutar solo la primera vez..\n",
    "\n",
    "if IN_COLAB:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install tensorflow==2.12\n",
    "else:\n",
    "  %pip install gym==0.17.3\n",
    "  %pip install git+https://github.com/Kojoley/atari-py.git\n",
    "  %pip install pyglet==1.5.0\n",
    "  %pip install h5py==3.1.0\n",
    "  %pip install Pillow==9.5.0\n",
    "  %pip install keras-rl2==1.0.5\n",
    "  %pip install Keras==2.2.4\n",
    "  %pip install tensorflow==2.5.3\n",
    "  %pip install torch==2.0.1\n",
    "  %pip install agents==1.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hzP_5ZuGb2X"
   },
   "source": [
    "---\n",
    "## **PARTE 2**. Enunciado\n",
    "\n",
    "Consideraciones a tener en cuenta:\n",
    "\n",
    "- El entorno sobre el que trabajaremos será _SpaceInvaders-v0_ y el algoritmo que usaremos será _DQN_.\n",
    "\n",
    "- Para nuestro ejercicio, el requisito mínimo será alcanzado cuando el agente consiga una **media de recompensa por encima de 20 puntos en modo test**. Por ello, esta media de la recompensa se calculará a partir del código de test en la última celda del notebook.\n",
    "\n",
    "Este proyecto práctico consta de tres partes:\n",
    "\n",
    "1.   Implementar la red neuronal que se usará en la solución\n",
    "2.   Implementar las distintas piezas de la solución DQN\n",
    "3.   Justificar la respuesta en relación a los resultados obtenidos\n",
    "\n",
    "**Rúbrica**: Se valorará la originalidad en la solución aportada, así como la capacidad de discutir los resultados de forma detallada. El requisito mínimo servirá para aprobar la actividad, bajo premisa de que la discusión del resultado sera apropiada.\n",
    "\n",
    "IMPORTANTE:\n",
    "\n",
    "* Si no se consigue una puntuación óptima, responder sobre la mejor puntuación obtenida.\n",
    "* Para entrenamientos largos, recordad que podéis usar checkpoints de vuestros modelos para retomar los entrenamientos. En este caso, recordad cambiar los parámetros adecuadamente (sobre todo los relacionados con el proceso de exploración).\n",
    "* Se deberá entregar unicamente el notebook y los pesos del mejor modelo en un fichero .zip, de forma organizada.\n",
    "* Cada alumno deberá de subir la solución de forma individual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_b3mzw8IzJP"
   },
   "source": [
    "---\n",
    "## **PARTE 3**. Desarrollo y preguntas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duPmUNOVGb2a"
   },
   "source": [
    "#### Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "j3eRhgI-Gb2a"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor\n",
    "from rl.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4jgQjzoGb2a"
   },
   "source": [
    "#### Configuración base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwOE6I_KGb2a",
    "outputId": "941f9c3a-e542-42e6-bd55-a097f9b71828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de acciones disponibles: 6\n",
      "Formato de las observaciones: Box(0, 255, (210, 160, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (84, 84)\n",
    "WINDOW_LENGTH = 1\n",
    "\n",
    "env_name = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n\n",
    "\n",
    "print(\"Numero de acciones disponibles: \" + str(nb_actions))\n",
    "print(\"Formato de las observaciones: \" + str(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "9jGEZUcpGb2a"
   },
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yitXTADGb2b"
   },
   "source": [
    "1. Implementación de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen del modelo:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 84, 84, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 32)        2080      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,681,062\n",
      "Trainable params: 1,681,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Red Neuronal\n",
    "def build_model(nb_actions, input_shape, window_length):\n",
    "    model = Sequential()\n",
    "    model.add(Permute((2, 3, 1), input_shape=(window_length,) + input_shape))\n",
    "    model.add(Convolution2D(32, 8, strides=4, activation='relu'))\n",
    "    model.add(Convolution2D(64, 4, strides=2, activation='relu'))\n",
    "    model.add(Convolution2D(64, 3, strides=1, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(nb_actions, activation='linear'))\n",
    "    \n",
    "    print(\"Resumen del modelo:\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_model(nb_actions, INPUT_SHAPE, WINDOW_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB9-_5HPGb2b"
   },
   "source": [
    "2. Implementación de la solución DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementacion DQN\n",
    "\n",
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "\n",
    "# Política epsilon-greedy con decaimiento lineal\n",
    "policy = LinearAnnealedPolicy(\n",
    "    inner_policy=EpsGreedyQPolicy(),\n",
    "    attr='eps',\n",
    "    value_max=1.0,      # Epsilon inicial (100% exploración)\n",
    "    value_min=0.1,      # Epsilon final (10% exploración)\n",
    "    value_test=0.05,    # Epsilon para testing (5% exploración)\n",
    "    nb_steps=1000000    # Pasos para el decaimiento completo\n",
    ")\n",
    "\n",
    "processor = AtariProcessor()\n",
    "\n",
    "dqn = DQNAgent(\n",
    "    model=model,\n",
    "    nb_actions=nb_actions,\n",
    "    policy=policy,\n",
    "    memory=memory,\n",
    "    processor=processor,\n",
    "    nb_steps_warmup=50000,      # Pasos antes de comenzar el entrenamiento\n",
    "    gamma=0.99,                 # Factor de descuento\n",
    "    target_model_update=10000,  # Frecuencia de actualización del target model\n",
    "    train_interval=4,           # Entrenar cada 4 pasos\n",
    "    delta_clip=1.0             # Clipping del error para estabilidad\n",
    ")\n",
    "\n",
    "dqn.compile(Adam(learning_rate=0.00025), metrics=['mae'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelIntervalCheckpoint('dqn_space_invaders_weights_{step}.h5f', interval=250000),\n",
    "    FileLogger('dqn_space_invaders_log.json', interval=100)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n",
      "Training for 2000000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     420/2000000: episode: 1, duration: 0.763s, episode steps: 420, steps per second: 550, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    1131/2000000: episode: 2, duration: 1.196s, episode steps: 711, steps per second: 594, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    1941/2000000: episode: 3, duration: 1.263s, episode steps: 810, steps per second: 641, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    2827/2000000: episode: 4, duration: 1.487s, episode steps: 886, steps per second: 596, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    3345/2000000: episode: 5, duration: 0.955s, episode steps: 518, steps per second: 542, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    3990/2000000: episode: 6, duration: 1.023s, episode steps: 645, steps per second: 630, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    4451/2000000: episode: 7, duration: 0.725s, episode steps: 461, steps per second: 636, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.484 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    5248/2000000: episode: 8, duration: 1.308s, episode steps: 797, steps per second: 609, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    5733/2000000: episode: 9, duration: 0.761s, episode steps: 485, steps per second: 638, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    6372/2000000: episode: 10, duration: 1.042s, episode steps: 639, steps per second: 614, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    7096/2000000: episode: 11, duration: 1.185s, episode steps: 724, steps per second: 611, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    7701/2000000: episode: 12, duration: 1.174s, episode steps: 605, steps per second: 515, episode reward:  2.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    8483/2000000: episode: 13, duration: 1.355s, episode steps: 782, steps per second: 577, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "    9204/2000000: episode: 14, duration: 1.253s, episode steps: 721, steps per second: 575, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   10008/2000000: episode: 15, duration: 1.373s, episode steps: 804, steps per second: 586, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   10685/2000000: episode: 16, duration: 1.170s, episode steps: 677, steps per second: 578, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   11469/2000000: episode: 17, duration: 1.334s, episode steps: 784, steps per second: 588, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   12508/2000000: episode: 18, duration: 1.829s, episode steps: 1039, steps per second: 568, episode reward:  9.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   13076/2000000: episode: 19, duration: 0.924s, episode steps: 568, steps per second: 615, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   13650/2000000: episode: 20, duration: 0.904s, episode steps: 574, steps per second: 635, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   14030/2000000: episode: 21, duration: 0.643s, episode steps: 380, steps per second: 591, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   14640/2000000: episode: 22, duration: 1.039s, episode steps: 610, steps per second: 587, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   15662/2000000: episode: 23, duration: 1.628s, episode steps: 1022, steps per second: 628, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   16503/2000000: episode: 24, duration: 1.440s, episode steps: 841, steps per second: 584, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   17044/2000000: episode: 25, duration: 1.061s, episode steps: 541, steps per second: 510, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   17983/2000000: episode: 26, duration: 1.674s, episode steps: 939, steps per second: 561, episode reward:  9.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   18953/2000000: episode: 27, duration: 1.608s, episode steps: 970, steps per second: 603, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   19584/2000000: episode: 28, duration: 0.996s, episode steps: 631, steps per second: 633, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   19941/2000000: episode: 29, duration: 0.560s, episode steps: 357, steps per second: 637, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   20339/2000000: episode: 30, duration: 0.643s, episode steps: 398, steps per second: 619, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   21180/2000000: episode: 31, duration: 1.299s, episode steps: 841, steps per second: 648, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   21978/2000000: episode: 32, duration: 1.264s, episode steps: 798, steps per second: 631, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   22533/2000000: episode: 33, duration: 0.855s, episode steps: 555, steps per second: 649, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   23244/2000000: episode: 34, duration: 1.155s, episode steps: 711, steps per second: 616, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   23620/2000000: episode: 35, duration: 0.611s, episode steps: 376, steps per second: 615, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   24202/2000000: episode: 36, duration: 0.890s, episode steps: 582, steps per second: 654, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   24744/2000000: episode: 37, duration: 0.896s, episode steps: 542, steps per second: 605, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   25384/2000000: episode: 38, duration: 0.979s, episode steps: 640, steps per second: 654, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   26048/2000000: episode: 39, duration: 1.196s, episode steps: 664, steps per second: 555, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   26584/2000000: episode: 40, duration: 1.004s, episode steps: 536, steps per second: 534, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   27168/2000000: episode: 41, duration: 1.160s, episode steps: 584, steps per second: 503, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   27810/2000000: episode: 42, duration: 1.092s, episode steps: 642, steps per second: 588, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   28449/2000000: episode: 43, duration: 1.061s, episode steps: 639, steps per second: 602, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   29191/2000000: episode: 44, duration: 1.269s, episode steps: 742, steps per second: 585, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   29774/2000000: episode: 45, duration: 1.003s, episode steps: 583, steps per second: 581, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30177/2000000: episode: 46, duration: 0.698s, episode steps: 403, steps per second: 577, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   30727/2000000: episode: 47, duration: 0.903s, episode steps: 550, steps per second: 609, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   31253/2000000: episode: 48, duration: 0.860s, episode steps: 526, steps per second: 612, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   32029/2000000: episode: 49, duration: 1.197s, episode steps: 776, steps per second: 648, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   33233/2000000: episode: 50, duration: 1.876s, episode steps: 1204, steps per second: 642, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   33629/2000000: episode: 51, duration: 0.608s, episode steps: 396, steps per second: 652, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   33990/2000000: episode: 52, duration: 0.588s, episode steps: 361, steps per second: 614, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   34733/2000000: episode: 53, duration: 1.163s, episode steps: 743, steps per second: 639, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   35525/2000000: episode: 54, duration: 1.427s, episode steps: 792, steps per second: 555, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   35996/2000000: episode: 55, duration: 0.863s, episode steps: 471, steps per second: 546, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   36756/2000000: episode: 56, duration: 1.502s, episode steps: 760, steps per second: 506, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   37300/2000000: episode: 57, duration: 0.867s, episode steps: 544, steps per second: 627, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   37810/2000000: episode: 58, duration: 0.828s, episode steps: 510, steps per second: 616, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   38415/2000000: episode: 59, duration: 0.985s, episode steps: 605, steps per second: 614, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   39399/2000000: episode: 60, duration: 1.702s, episode steps: 984, steps per second: 578, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   39995/2000000: episode: 61, duration: 0.944s, episode steps: 596, steps per second: 632, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   40773/2000000: episode: 62, duration: 1.262s, episode steps: 778, steps per second: 617, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41398/2000000: episode: 63, duration: 1.078s, episode steps: 625, steps per second: 580, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   41784/2000000: episode: 64, duration: 0.675s, episode steps: 386, steps per second: 572, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   42490/2000000: episode: 65, duration: 1.152s, episode steps: 706, steps per second: 613, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   43140/2000000: episode: 66, duration: 1.055s, episode steps: 650, steps per second: 616, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   44136/2000000: episode: 67, duration: 1.693s, episode steps: 996, steps per second: 588, episode reward: 12.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   44708/2000000: episode: 68, duration: 0.980s, episode steps: 572, steps per second: 584, episode reward:  3.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   45241/2000000: episode: 69, duration: 1.054s, episode steps: 533, steps per second: 506, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   46323/2000000: episode: 70, duration: 1.934s, episode steps: 1082, steps per second: 560, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   46896/2000000: episode: 71, duration: 0.950s, episode steps: 573, steps per second: 603, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   47583/2000000: episode: 72, duration: 1.160s, episode steps: 687, steps per second: 592, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.323 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   48490/2000000: episode: 73, duration: 1.687s, episode steps: 907, steps per second: 538, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   49436/2000000: episode: 74, duration: 1.492s, episode steps: 946, steps per second: 634, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   49831/2000000: episode: 75, duration: 0.643s, episode steps: 395, steps per second: 614, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mateo\\anaconda3\\envs\\gym_atari\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2424: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50407/2000000: episode: 76, duration: 4.826s, episode steps: 576, steps per second: 119, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.006959, mae: 0.024824, mean_q: 0.044898, mean_eps: 0.954816\n",
      "   50917/2000000: episode: 77, duration: 5.751s, episode steps: 510, steps per second:  89, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.006280, mae: 0.025889, mean_q: 0.037724, mean_eps: 0.954404\n",
      "   51592/2000000: episode: 78, duration: 7.431s, episode steps: 675, steps per second:  91, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.006349, mae: 0.026170, mean_q: 0.039764, mean_eps: 0.953871\n",
      "   52259/2000000: episode: 79, duration: 7.521s, episode steps: 667, steps per second:  89, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.006381, mae: 0.026750, mean_q: 0.036926, mean_eps: 0.953268\n",
      "   52880/2000000: episode: 80, duration: 6.952s, episode steps: 621, steps per second:  89, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.008254, mae: 0.030416, mean_q: 0.039341, mean_eps: 0.952689\n",
      "   53796/2000000: episode: 81, duration: 10.367s, episode steps: 916, steps per second:  88, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.006537, mae: 0.028643, mean_q: 0.036633, mean_eps: 0.951998\n",
      "   54611/2000000: episode: 82, duration: 8.845s, episode steps: 815, steps per second:  92, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.006722, mae: 0.027183, mean_q: 0.037095, mean_eps: 0.951218\n",
      "   55567/2000000: episode: 83, duration: 11.685s, episode steps: 956, steps per second:  82, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.007128, mae: 0.029865, mean_q: 0.040075, mean_eps: 0.950421\n",
      "   56242/2000000: episode: 84, duration: 7.944s, episode steps: 675, steps per second:  85, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.006361, mae: 0.026353, mean_q: 0.036557, mean_eps: 0.949686\n",
      "   56684/2000000: episode: 85, duration: 5.112s, episode steps: 442, steps per second:  86, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.007677, mae: 0.029698, mean_q: 0.038695, mean_eps: 0.949184\n",
      "   57230/2000000: episode: 86, duration: 6.010s, episode steps: 546, steps per second:  91, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.007511, mae: 0.030449, mean_q: 0.037977, mean_eps: 0.948740\n",
      "   57949/2000000: episode: 87, duration: 7.999s, episode steps: 719, steps per second:  90, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.006555, mae: 0.028033, mean_q: 0.034582, mean_eps: 0.948169\n",
      "   58430/2000000: episode: 88, duration: 5.060s, episode steps: 481, steps per second:  95, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.006410, mae: 0.029033, mean_q: 0.035900, mean_eps: 0.947629\n",
      "   59071/2000000: episode: 89, duration: 6.903s, episode steps: 641, steps per second:  93, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.006924, mae: 0.027343, mean_q: 0.034542, mean_eps: 0.947125\n",
      "   59822/2000000: episode: 90, duration: 8.329s, episode steps: 751, steps per second:  90, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.006944, mae: 0.028470, mean_q: 0.035733, mean_eps: 0.946499\n",
      "   60314/2000000: episode: 91, duration: 5.225s, episode steps: 492, steps per second:  94, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.008184, mae: 0.049878, mean_q: 0.067000, mean_eps: 0.945939\n",
      "   61044/2000000: episode: 92, duration: 7.930s, episode steps: 730, steps per second:  92, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.006592, mae: 0.062939, mean_q: 0.082351, mean_eps: 0.945390\n",
      "   61540/2000000: episode: 93, duration: 5.136s, episode steps: 496, steps per second:  97, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.007189, mae: 0.065521, mean_q: 0.081143, mean_eps: 0.944839\n",
      "   62150/2000000: episode: 94, duration: 6.314s, episode steps: 610, steps per second:  97, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.005154, mae: 0.061280, mean_q: 0.079511, mean_eps: 0.944340\n",
      "   62821/2000000: episode: 95, duration: 7.274s, episode steps: 671, steps per second:  92, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.007257, mae: 0.065383, mean_q: 0.082003, mean_eps: 0.943763\n",
      "   63425/2000000: episode: 96, duration: 6.355s, episode steps: 604, steps per second:  95, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.004519, mae: 0.059384, mean_q: 0.074531, mean_eps: 0.943188\n",
      "   64007/2000000: episode: 97, duration: 6.397s, episode steps: 582, steps per second:  91, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.007125, mae: 0.064871, mean_q: 0.083280, mean_eps: 0.942656\n",
      "   64675/2000000: episode: 98, duration: 7.392s, episode steps: 668, steps per second:  90, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.007256, mae: 0.066033, mean_q: 0.082137, mean_eps: 0.942094\n",
      "   65058/2000000: episode: 99, duration: 4.197s, episode steps: 383, steps per second:  91, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.006714, mae: 0.066037, mean_q: 0.081483, mean_eps: 0.941621\n",
      "   66068/2000000: episode: 100, duration: 11.249s, episode steps: 1010, steps per second:  90, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.006728, mae: 0.064845, mean_q: 0.084175, mean_eps: 0.940994\n",
      "   66620/2000000: episode: 101, duration: 6.115s, episode steps: 552, steps per second:  90, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.006252, mae: 0.062670, mean_q: 0.077198, mean_eps: 0.940292\n",
      "   67303/2000000: episode: 102, duration: 7.376s, episode steps: 683, steps per second:  93, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.006109, mae: 0.063288, mean_q: 0.079032, mean_eps: 0.939736\n",
      "   68009/2000000: episode: 103, duration: 7.114s, episode steps: 706, steps per second:  99, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.006458, mae: 0.064549, mean_q: 0.081062, mean_eps: 0.939110\n",
      "   68798/2000000: episode: 104, duration: 8.341s, episode steps: 789, steps per second:  95, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.006263, mae: 0.063314, mean_q: 0.079295, mean_eps: 0.938436\n",
      "   69187/2000000: episode: 105, duration: 4.256s, episode steps: 389, steps per second:  91, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.007119, mae: 0.065641, mean_q: 0.081119, mean_eps: 0.937907\n",
      "   70176/2000000: episode: 106, duration: 11.082s, episode steps: 989, steps per second:  89, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.007119, mae: 0.068032, mean_q: 0.084092, mean_eps: 0.937288\n",
      "   70803/2000000: episode: 107, duration: 7.076s, episode steps: 627, steps per second:  89, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.007230, mae: 0.080112, mean_q: 0.099786, mean_eps: 0.936561\n",
      "   71205/2000000: episode: 108, duration: 4.556s, episode steps: 402, steps per second:  88, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.007199, mae: 0.081053, mean_q: 0.101721, mean_eps: 0.936096\n",
      "   72699/2000000: episode: 109, duration: 15.859s, episode steps: 1494, steps per second:  94, episode reward: 14.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.006571, mae: 0.080423, mean_q: 0.104292, mean_eps: 0.935243\n",
      "   73514/2000000: episode: 110, duration: 8.246s, episode steps: 815, steps per second:  99, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.006621, mae: 0.080648, mean_q: 0.100401, mean_eps: 0.934205\n",
      "   74228/2000000: episode: 111, duration: 7.426s, episode steps: 714, steps per second:  96, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.006822, mae: 0.080213, mean_q: 0.102016, mean_eps: 0.933517\n",
      "   74615/2000000: episode: 112, duration: 3.948s, episode steps: 387, steps per second:  98, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.006093, mae: 0.080199, mean_q: 0.100730, mean_eps: 0.933022\n",
      "   75426/2000000: episode: 113, duration: 8.222s, episode steps: 811, steps per second:  99, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.006113, mae: 0.079346, mean_q: 0.098397, mean_eps: 0.932482\n",
      "   76169/2000000: episode: 114, duration: 7.828s, episode steps: 743, steps per second:  95, episode reward:  6.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.008250, mae: 0.083304, mean_q: 0.105033, mean_eps: 0.931782\n",
      "   76940/2000000: episode: 115, duration: 8.180s, episode steps: 771, steps per second:  94, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.007202, mae: 0.082793, mean_q: 0.102907, mean_eps: 0.931101\n",
      "   77458/2000000: episode: 116, duration: 5.610s, episode steps: 518, steps per second:  92, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.005880, mae: 0.078432, mean_q: 0.099243, mean_eps: 0.930522\n",
      "   77869/2000000: episode: 117, duration: 4.270s, episode steps: 411, steps per second:  96, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.005398, mae: 0.078276, mean_q: 0.097090, mean_eps: 0.930102\n",
      "   78258/2000000: episode: 118, duration: 4.278s, episode steps: 389, steps per second:  91, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.006472, mae: 0.079066, mean_q: 0.099978, mean_eps: 0.929742\n",
      "   78651/2000000: episode: 119, duration: 4.529s, episode steps: 393, steps per second:  87, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.008407, mae: 0.085291, mean_q: 0.106144, mean_eps: 0.929391\n",
      "   79133/2000000: episode: 120, duration: 5.399s, episode steps: 482, steps per second:  89, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.006896, mae: 0.080822, mean_q: 0.102428, mean_eps: 0.928997\n",
      "   80139/2000000: episode: 121, duration: 11.214s, episode steps: 1006, steps per second:  90, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.005666, mae: 0.079044, mean_q: 0.100892, mean_eps: 0.928328\n",
      "   81132/2000000: episode: 122, duration: 10.745s, episode steps: 993, steps per second:  92, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.007729, mae: 0.102378, mean_q: 0.128514, mean_eps: 0.927429\n",
      "   81660/2000000: episode: 123, duration: 5.809s, episode steps: 528, steps per second:  91, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.005953, mae: 0.097984, mean_q: 0.127582, mean_eps: 0.926745\n",
      "   82189/2000000: episode: 124, duration: 6.091s, episode steps: 529, steps per second:  87, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.005860, mae: 0.096329, mean_q: 0.121893, mean_eps: 0.926268\n",
      "   83176/2000000: episode: 125, duration: 11.043s, episode steps: 987, steps per second:  89, episode reward: 10.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.006481, mae: 0.098507, mean_q: 0.124315, mean_eps: 0.925586\n",
      "   83984/2000000: episode: 126, duration: 8.529s, episode steps: 808, steps per second:  95, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.007429, mae: 0.101060, mean_q: 0.126608, mean_eps: 0.924780\n",
      "   84567/2000000: episode: 127, duration: 6.285s, episode steps: 583, steps per second:  93, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.004450, mae: 0.094070, mean_q: 0.117970, mean_eps: 0.924153\n",
      "   84992/2000000: episode: 128, duration: 4.605s, episode steps: 425, steps per second:  92, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.005376, mae: 0.095378, mean_q: 0.118765, mean_eps: 0.923700\n",
      "   85471/2000000: episode: 129, duration: 5.011s, episode steps: 479, steps per second:  96, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.006783, mae: 0.098550, mean_q: 0.121596, mean_eps: 0.923293\n",
      "   86095/2000000: episode: 130, duration: 6.709s, episode steps: 624, steps per second:  93, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.007460, mae: 0.100950, mean_q: 0.125553, mean_eps: 0.922796\n",
      "   86698/2000000: episode: 131, duration: 6.242s, episode steps: 603, steps per second:  97, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.401 [0.000, 5.000],  loss: 0.006405, mae: 0.098711, mean_q: 0.122282, mean_eps: 0.922244\n",
      "   87544/2000000: episode: 132, duration: 9.527s, episode steps: 846, steps per second:  89, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.006728, mae: 0.098833, mean_q: 0.124833, mean_eps: 0.921592\n",
      "   88229/2000000: episode: 133, duration: 7.668s, episode steps: 685, steps per second:  89, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.006575, mae: 0.097797, mean_q: 0.123304, mean_eps: 0.920903\n",
      "   89024/2000000: episode: 134, duration: 8.687s, episode steps: 795, steps per second:  92, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.006562, mae: 0.099857, mean_q: 0.123962, mean_eps: 0.920237\n",
      "   89775/2000000: episode: 135, duration: 7.664s, episode steps: 751, steps per second:  98, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.008069, mae: 0.101651, mean_q: 0.126349, mean_eps: 0.919542\n",
      "   90310/2000000: episode: 136, duration: 5.457s, episode steps: 535, steps per second:  98, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.004996, mae: 0.103858, mean_q: 0.130319, mean_eps: 0.918962\n",
      "   90944/2000000: episode: 137, duration: 6.686s, episode steps: 634, steps per second:  95, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.007626, mae: 0.116884, mean_q: 0.146063, mean_eps: 0.918437\n",
      "   91604/2000000: episode: 138, duration: 6.770s, episode steps: 660, steps per second:  97, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.007322, mae: 0.118107, mean_q: 0.149103, mean_eps: 0.917855\n",
      "   92161/2000000: episode: 139, duration: 5.884s, episode steps: 557, steps per second:  95, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.006849, mae: 0.116825, mean_q: 0.145924, mean_eps: 0.917306\n",
      "   92745/2000000: episode: 140, duration: 6.017s, episode steps: 584, steps per second:  97, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.007085, mae: 0.116127, mean_q: 0.147803, mean_eps: 0.916791\n",
      "   93273/2000000: episode: 141, duration: 5.341s, episode steps: 528, steps per second:  99, episode reward:  0.000, mean reward:  0.000 [ 0.000,  0.000], mean action: 2.547 [0.000, 5.000],  loss: 0.006350, mae: 0.117602, mean_q: 0.147784, mean_eps: 0.916291\n",
      "   93697/2000000: episode: 142, duration: 4.581s, episode steps: 424, steps per second:  93, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.005563, mae: 0.114203, mean_q: 0.143439, mean_eps: 0.915863\n",
      "   94332/2000000: episode: 143, duration: 6.487s, episode steps: 635, steps per second:  98, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.345 [0.000, 5.000],  loss: 0.005561, mae: 0.113359, mean_q: 0.141840, mean_eps: 0.915387\n",
      "   94702/2000000: episode: 144, duration: 3.823s, episode steps: 370, steps per second:  97, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.007109, mae: 0.117897, mean_q: 0.146430, mean_eps: 0.914936\n",
      "   95195/2000000: episode: 145, duration: 5.209s, episode steps: 493, steps per second:  95, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.007045, mae: 0.116912, mean_q: 0.143312, mean_eps: 0.914547\n",
      "   95593/2000000: episode: 146, duration: 4.151s, episode steps: 398, steps per second:  96, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.007090, mae: 0.116759, mean_q: 0.144223, mean_eps: 0.914145\n",
      "   96374/2000000: episode: 147, duration: 7.854s, episode steps: 781, steps per second:  99, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.005340, mae: 0.112814, mean_q: 0.139456, mean_eps: 0.913614\n",
      "   96932/2000000: episode: 148, duration: 5.908s, episode steps: 558, steps per second:  94, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.006303, mae: 0.116956, mean_q: 0.145944, mean_eps: 0.913013\n",
      "   97723/2000000: episode: 149, duration: 8.120s, episode steps: 791, steps per second:  97, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.006747, mae: 0.116681, mean_q: 0.146601, mean_eps: 0.912407\n",
      "   98486/2000000: episode: 150, duration: 7.971s, episode steps: 763, steps per second:  96, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.006683, mae: 0.116128, mean_q: 0.143242, mean_eps: 0.911706\n",
      "   98868/2000000: episode: 151, duration: 3.866s, episode steps: 382, steps per second:  99, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.007478, mae: 0.119737, mean_q: 0.148445, mean_eps: 0.911192\n",
      "   99866/2000000: episode: 152, duration: 10.372s, episode steps: 998, steps per second:  96, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.008079, mae: 0.120329, mean_q: 0.150613, mean_eps: 0.910571\n",
      "  100316/2000000: episode: 153, duration: 4.658s, episode steps: 450, steps per second:  97, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.006478, mae: 0.128908, mean_q: 0.162341, mean_eps: 0.909919\n",
      "  100970/2000000: episode: 154, duration: 6.666s, episode steps: 654, steps per second:  98, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.006509, mae: 0.136757, mean_q: 0.171818, mean_eps: 0.909422\n",
      "  101814/2000000: episode: 155, duration: 9.045s, episode steps: 844, steps per second:  93, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.006369, mae: 0.138350, mean_q: 0.173038, mean_eps: 0.908747\n",
      "  102611/2000000: episode: 156, duration: 8.261s, episode steps: 797, steps per second:  96, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.007004, mae: 0.140233, mean_q: 0.177764, mean_eps: 0.908009\n",
      "  103497/2000000: episode: 157, duration: 9.384s, episode steps: 886, steps per second:  94, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.006152, mae: 0.138810, mean_q: 0.173298, mean_eps: 0.907251\n",
      "  104302/2000000: episode: 158, duration: 8.375s, episode steps: 805, steps per second:  96, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.005804, mae: 0.138456, mean_q: 0.173389, mean_eps: 0.906490\n",
      "  104913/2000000: episode: 159, duration: 6.605s, episode steps: 611, steps per second:  93, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.006240, mae: 0.138623, mean_q: 0.173356, mean_eps: 0.905853\n",
      "  105470/2000000: episode: 160, duration: 6.067s, episode steps: 557, steps per second:  92, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.005729, mae: 0.137117, mean_q: 0.172174, mean_eps: 0.905327\n",
      "  106025/2000000: episode: 161, duration: 6.108s, episode steps: 555, steps per second:  91, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.006129, mae: 0.138011, mean_q: 0.172085, mean_eps: 0.904827\n",
      "  107148/2000000: episode: 162, duration: 11.893s, episode steps: 1123, steps per second:  94, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.006479, mae: 0.139334, mean_q: 0.173628, mean_eps: 0.904073\n",
      "  108150/2000000: episode: 163, duration: 11.354s, episode steps: 1002, steps per second:  88, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.005830, mae: 0.137902, mean_q: 0.173237, mean_eps: 0.903117\n",
      "  108787/2000000: episode: 164, duration: 7.228s, episode steps: 637, steps per second:  88, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.007050, mae: 0.139356, mean_q: 0.172888, mean_eps: 0.902379\n",
      "  109424/2000000: episode: 165, duration: 7.049s, episode steps: 637, steps per second:  90, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.007136, mae: 0.141177, mean_q: 0.175592, mean_eps: 0.901806\n",
      "  110201/2000000: episode: 166, duration: 8.655s, episode steps: 777, steps per second:  90, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.006784, mae: 0.145817, mean_q: 0.183225, mean_eps: 0.901169\n",
      "  110840/2000000: episode: 167, duration: 7.360s, episode steps: 639, steps per second:  87, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.007295, mae: 0.163885, mean_q: 0.203508, mean_eps: 0.900532\n",
      "  111282/2000000: episode: 168, duration: 4.782s, episode steps: 442, steps per second:  92, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.005956, mae: 0.161748, mean_q: 0.200255, mean_eps: 0.900046\n",
      "  112097/2000000: episode: 169, duration: 8.977s, episode steps: 815, steps per second:  91, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.006158, mae: 0.164661, mean_q: 0.205753, mean_eps: 0.899479\n",
      "  112875/2000000: episode: 170, duration: 7.864s, episode steps: 778, steps per second:  99, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.007060, mae: 0.164891, mean_q: 0.206388, mean_eps: 0.898763\n",
      "  113513/2000000: episode: 171, duration: 6.913s, episode steps: 638, steps per second:  92, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.006120, mae: 0.161873, mean_q: 0.202081, mean_eps: 0.898125\n",
      "  114153/2000000: episode: 172, duration: 6.760s, episode steps: 640, steps per second:  95, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.007417, mae: 0.162933, mean_q: 0.203114, mean_eps: 0.897549\n",
      "  114931/2000000: episode: 173, duration: 8.372s, episode steps: 778, steps per second:  93, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.007212, mae: 0.163567, mean_q: 0.203667, mean_eps: 0.896912\n",
      "  115497/2000000: episode: 174, duration: 5.834s, episode steps: 566, steps per second:  97, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.005542, mae: 0.160487, mean_q: 0.199857, mean_eps: 0.896307\n",
      "  116166/2000000: episode: 175, duration: 7.054s, episode steps: 669, steps per second:  95, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.006715, mae: 0.163272, mean_q: 0.203058, mean_eps: 0.895751\n",
      "  116722/2000000: episode: 176, duration: 5.741s, episode steps: 556, steps per second:  97, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.007017, mae: 0.163761, mean_q: 0.202073, mean_eps: 0.895200\n",
      "  117359/2000000: episode: 177, duration: 6.606s, episode steps: 637, steps per second:  96, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.007864, mae: 0.166324, mean_q: 0.205280, mean_eps: 0.894664\n",
      "  117870/2000000: episode: 178, duration: 5.600s, episode steps: 511, steps per second:  91, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.005928, mae: 0.162816, mean_q: 0.204384, mean_eps: 0.894147\n",
      "  118238/2000000: episode: 179, duration: 3.887s, episode steps: 368, steps per second:  95, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.006244, mae: 0.164763, mean_q: 0.205806, mean_eps: 0.893751\n",
      "  118878/2000000: episode: 180, duration: 6.804s, episode steps: 640, steps per second:  94, episode reward:  4.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.006629, mae: 0.164665, mean_q: 0.203493, mean_eps: 0.893298\n",
      "  119580/2000000: episode: 181, duration: 7.767s, episode steps: 702, steps per second:  90, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.006113, mae: 0.160672, mean_q: 0.198573, mean_eps: 0.892695\n",
      "  120381/2000000: episode: 182, duration: 8.383s, episode steps: 801, steps per second:  96, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.006597, mae: 0.178283, mean_q: 0.221923, mean_eps: 0.892018\n",
      "  121004/2000000: episode: 183, duration: 6.804s, episode steps: 623, steps per second:  92, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.007544, mae: 0.200051, mean_q: 0.247983, mean_eps: 0.891377\n",
      "  121972/2000000: episode: 184, duration: 10.581s, episode steps: 968, steps per second:  91, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.005478, mae: 0.194759, mean_q: 0.242690, mean_eps: 0.890663\n",
      "  122346/2000000: episode: 185, duration: 4.272s, episode steps: 374, steps per second:  88, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.006419, mae: 0.197668, mean_q: 0.248608, mean_eps: 0.890058\n",
      "  122986/2000000: episode: 186, duration: 7.002s, episode steps: 640, steps per second:  91, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.006501, mae: 0.198997, mean_q: 0.247682, mean_eps: 0.889601\n",
      "  123361/2000000: episode: 187, duration: 4.090s, episode steps: 375, steps per second:  92, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.006413, mae: 0.197200, mean_q: 0.248631, mean_eps: 0.889143\n",
      "  123855/2000000: episode: 188, duration: 5.575s, episode steps: 494, steps per second:  89, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.006147, mae: 0.194948, mean_q: 0.243054, mean_eps: 0.888753\n",
      "  124445/2000000: episode: 189, duration: 6.563s, episode steps: 590, steps per second:  90, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.005767, mae: 0.197489, mean_q: 0.247502, mean_eps: 0.888265\n",
      "  124786/2000000: episode: 190, duration: 3.789s, episode steps: 341, steps per second:  90, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.007848, mae: 0.201358, mean_q: 0.248535, mean_eps: 0.887846\n",
      "  125554/2000000: episode: 191, duration: 8.489s, episode steps: 768, steps per second:  90, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.006817, mae: 0.199519, mean_q: 0.247249, mean_eps: 0.887347\n",
      "  125926/2000000: episode: 192, duration: 3.867s, episode steps: 372, steps per second:  96, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.007375, mae: 0.200622, mean_q: 0.250726, mean_eps: 0.886834\n",
      "  127182/2000000: episode: 193, duration: 13.361s, episode steps: 1256, steps per second:  94, episode reward: 13.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.006040, mae: 0.197335, mean_q: 0.243741, mean_eps: 0.886101\n",
      "  128435/2000000: episode: 194, duration: 13.598s, episode steps: 1253, steps per second:  92, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.007181, mae: 0.201740, mean_q: 0.251608, mean_eps: 0.884973\n",
      "  129439/2000000: episode: 195, duration: 11.671s, episode steps: 1004, steps per second:  86, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.005980, mae: 0.196710, mean_q: 0.244477, mean_eps: 0.883958\n",
      "  130289/2000000: episode: 196, duration: 9.255s, episode steps: 850, steps per second:  92, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.005617, mae: 0.199253, mean_q: 0.247839, mean_eps: 0.883122\n",
      "  131072/2000000: episode: 197, duration: 8.666s, episode steps: 783, steps per second:  90, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.007440, mae: 0.214224, mean_q: 0.263696, mean_eps: 0.882388\n",
      "  131520/2000000: episode: 198, duration: 4.768s, episode steps: 448, steps per second:  94, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.007660, mae: 0.214383, mean_q: 0.263664, mean_eps: 0.881835\n",
      "  132235/2000000: episode: 199, duration: 7.741s, episode steps: 715, steps per second:  92, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.006364, mae: 0.212238, mean_q: 0.262696, mean_eps: 0.881312\n",
      "  132739/2000000: episode: 200, duration: 5.463s, episode steps: 504, steps per second:  92, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.006712, mae: 0.213410, mean_q: 0.264126, mean_eps: 0.880763\n",
      "  133296/2000000: episode: 201, duration: 5.952s, episode steps: 557, steps per second:  94, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.007070, mae: 0.215074, mean_q: 0.268657, mean_eps: 0.880286\n",
      "  134345/2000000: episode: 202, duration: 11.376s, episode steps: 1049, steps per second:  92, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.006638, mae: 0.212436, mean_q: 0.264527, mean_eps: 0.879562\n",
      "  134966/2000000: episode: 203, duration: 6.797s, episode steps: 621, steps per second:  91, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.006038, mae: 0.212584, mean_q: 0.264210, mean_eps: 0.878810\n",
      "  135465/2000000: episode: 204, duration: 5.901s, episode steps: 499, steps per second:  85, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.007776, mae: 0.218064, mean_q: 0.269689, mean_eps: 0.878306\n",
      "  136101/2000000: episode: 205, duration: 7.378s, episode steps: 636, steps per second:  86, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.006431, mae: 0.213783, mean_q: 0.264723, mean_eps: 0.877794\n",
      "  137044/2000000: episode: 206, duration: 10.821s, episode steps: 943, steps per second:  87, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.006807, mae: 0.212982, mean_q: 0.263562, mean_eps: 0.877085\n",
      "  137563/2000000: episode: 207, duration: 5.895s, episode steps: 519, steps per second:  88, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.008074, mae: 0.214717, mean_q: 0.264840, mean_eps: 0.876428\n",
      "  138096/2000000: episode: 208, duration: 6.358s, episode steps: 533, steps per second:  84, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.007165, mae: 0.215144, mean_q: 0.267050, mean_eps: 0.875955\n",
      "  138582/2000000: episode: 209, duration: 6.460s, episode steps: 486, steps per second:  75, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.007031, mae: 0.216225, mean_q: 0.266728, mean_eps: 0.875496\n",
      "  139228/2000000: episode: 210, duration: 7.650s, episode steps: 646, steps per second:  84, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.005967, mae: 0.212189, mean_q: 0.262861, mean_eps: 0.874986\n",
      "  139755/2000000: episode: 211, duration: 6.332s, episode steps: 527, steps per second:  83, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.008182, mae: 0.220204, mean_q: 0.272037, mean_eps: 0.874459\n",
      "  140273/2000000: episode: 212, duration: 5.917s, episode steps: 518, steps per second:  88, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.005593, mae: 0.221172, mean_q: 0.275974, mean_eps: 0.873987\n",
      "  141252/2000000: episode: 213, duration: 10.892s, episode steps: 979, steps per second:  90, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.005742, mae: 0.227674, mean_q: 0.283781, mean_eps: 0.873314\n",
      "  141957/2000000: episode: 214, duration: 7.462s, episode steps: 705, steps per second:  94, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.007853, mae: 0.236954, mean_q: 0.294035, mean_eps: 0.872556\n",
      "  142487/2000000: episode: 215, duration: 5.946s, episode steps: 530, steps per second:  89, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.006296, mae: 0.232386, mean_q: 0.288179, mean_eps: 0.872000\n",
      "  143116/2000000: episode: 216, duration: 6.840s, episode steps: 629, steps per second:  92, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.006269, mae: 0.232461, mean_q: 0.287699, mean_eps: 0.871480\n",
      "  143831/2000000: episode: 217, duration: 7.858s, episode steps: 715, steps per second:  91, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.007154, mae: 0.232381, mean_q: 0.287197, mean_eps: 0.870875\n",
      "  144526/2000000: episode: 218, duration: 7.332s, episode steps: 695, steps per second:  95, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.006212, mae: 0.230066, mean_q: 0.285117, mean_eps: 0.870240\n",
      "  145101/2000000: episode: 219, duration: 6.389s, episode steps: 575, steps per second:  90, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.005933, mae: 0.231278, mean_q: 0.287996, mean_eps: 0.869667\n",
      "  146146/2000000: episode: 220, duration: 11.157s, episode steps: 1045, steps per second:  94, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.006468, mae: 0.231702, mean_q: 0.285727, mean_eps: 0.868938\n",
      "  146547/2000000: episode: 221, duration: 4.180s, episode steps: 401, steps per second:  96, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.006640, mae: 0.228428, mean_q: 0.281283, mean_eps: 0.868289\n",
      "  147120/2000000: episode: 222, duration: 5.984s, episode steps: 573, steps per second:  96, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.008234, mae: 0.232958, mean_q: 0.287388, mean_eps: 0.867851\n",
      "  147659/2000000: episode: 223, duration: 5.865s, episode steps: 539, steps per second:  92, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.005531, mae: 0.229518, mean_q: 0.282521, mean_eps: 0.867351\n",
      "  148567/2000000: episode: 224, duration: 9.642s, episode steps: 908, steps per second:  94, episode reward: 12.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.385 [0.000, 5.000],  loss: 0.007278, mae: 0.231780, mean_q: 0.285740, mean_eps: 0.866699\n",
      "  149378/2000000: episode: 225, duration: 9.590s, episode steps: 811, steps per second:  85, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.006318, mae: 0.232111, mean_q: 0.286731, mean_eps: 0.865925\n",
      "  151024/2000000: episode: 226, duration: 17.712s, episode steps: 1646, steps per second:  93, episode reward: 26.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.006310, mae: 0.239086, mean_q: 0.295307, mean_eps: 0.864820\n",
      "  151677/2000000: episode: 227, duration: 7.021s, episode steps: 653, steps per second:  93, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.005028, mae: 0.242169, mean_q: 0.299314, mean_eps: 0.863785\n",
      "  152315/2000000: episode: 228, duration: 6.562s, episode steps: 638, steps per second:  97, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.006569, mae: 0.246749, mean_q: 0.304814, mean_eps: 0.863204\n",
      "  153422/2000000: episode: 229, duration: 11.594s, episode steps: 1107, steps per second:  95, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.007012, mae: 0.248748, mean_q: 0.306458, mean_eps: 0.862419\n",
      "  153928/2000000: episode: 230, duration: 5.432s, episode steps: 506, steps per second:  93, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.006038, mae: 0.244278, mean_q: 0.301190, mean_eps: 0.861693\n",
      "  154284/2000000: episode: 231, duration: 3.871s, episode steps: 356, steps per second:  92, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.005805, mae: 0.243513, mean_q: 0.300358, mean_eps: 0.861306\n",
      "  154810/2000000: episode: 232, duration: 5.489s, episode steps: 526, steps per second:  96, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.004830, mae: 0.242149, mean_q: 0.297815, mean_eps: 0.860909\n",
      "  155493/2000000: episode: 233, duration: 7.098s, episode steps: 683, steps per second:  96, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.006133, mae: 0.246905, mean_q: 0.303624, mean_eps: 0.860363\n",
      "  156191/2000000: episode: 234, duration: 7.382s, episode steps: 698, steps per second:  95, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.006390, mae: 0.245027, mean_q: 0.301691, mean_eps: 0.859742\n",
      "  156923/2000000: episode: 235, duration: 7.796s, episode steps: 732, steps per second:  94, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.006586, mae: 0.249777, mean_q: 0.309757, mean_eps: 0.859100\n",
      "  157545/2000000: episode: 236, duration: 6.669s, episode steps: 622, steps per second:  93, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.415 [0.000, 5.000],  loss: 0.006716, mae: 0.248225, mean_q: 0.306785, mean_eps: 0.858489\n",
      "  157940/2000000: episode: 237, duration: 4.591s, episode steps: 395, steps per second:  86, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.005821, mae: 0.246341, mean_q: 0.304237, mean_eps: 0.858032\n",
      "  158923/2000000: episode: 238, duration: 10.500s, episode steps: 983, steps per second:  94, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.007643, mae: 0.251447, mean_q: 0.310118, mean_eps: 0.857413\n",
      "  159344/2000000: episode: 239, duration: 4.687s, episode steps: 421, steps per second:  90, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.287 [0.000, 5.000],  loss: 0.007598, mae: 0.252264, mean_q: 0.311797, mean_eps: 0.856781\n",
      "  160162/2000000: episode: 240, duration: 8.772s, episode steps: 818, steps per second:  93, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.006100, mae: 0.249768, mean_q: 0.306915, mean_eps: 0.856223\n",
      "  160658/2000000: episode: 241, duration: 5.160s, episode steps: 496, steps per second:  96, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.005890, mae: 0.262444, mean_q: 0.323968, mean_eps: 0.855631\n",
      "  161053/2000000: episode: 242, duration: 4.066s, episode steps: 395, steps per second:  97, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.008043, mae: 0.270333, mean_q: 0.332542, mean_eps: 0.855230\n",
      "  161926/2000000: episode: 243, duration: 9.263s, episode steps: 873, steps per second:  94, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.006396, mae: 0.261279, mean_q: 0.321022, mean_eps: 0.854659\n",
      "  162575/2000000: episode: 244, duration: 7.315s, episode steps: 649, steps per second:  89, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.006548, mae: 0.264308, mean_q: 0.326893, mean_eps: 0.853975\n",
      "  163095/2000000: episode: 245, duration: 6.062s, episode steps: 520, steps per second:  86, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.006029, mae: 0.262217, mean_q: 0.321979, mean_eps: 0.853449\n",
      "  163608/2000000: episode: 246, duration: 6.116s, episode steps: 513, steps per second:  84, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.007774, mae: 0.268945, mean_q: 0.330497, mean_eps: 0.852985\n",
      "  164405/2000000: episode: 247, duration: 9.671s, episode steps: 797, steps per second:  82, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.007364, mae: 0.267159, mean_q: 0.329619, mean_eps: 0.852395\n",
      "  165491/2000000: episode: 248, duration: 12.956s, episode steps: 1086, steps per second:  84, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.007829, mae: 0.266411, mean_q: 0.327464, mean_eps: 0.851547\n",
      "  166330/2000000: episode: 249, duration: 10.015s, episode steps: 839, steps per second:  84, episode reward:  7.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.007136, mae: 0.269040, mean_q: 0.331083, mean_eps: 0.850681\n",
      "  166850/2000000: episode: 250, duration: 5.852s, episode steps: 520, steps per second:  89, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.006353, mae: 0.262137, mean_q: 0.322935, mean_eps: 0.850069\n",
      "  167528/2000000: episode: 251, duration: 7.574s, episode steps: 678, steps per second:  90, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.006839, mae: 0.255235, mean_q: 0.314800, mean_eps: 0.849531\n",
      "  168364/2000000: episode: 252, duration: 9.246s, episode steps: 836, steps per second:  90, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.007924, mae: 0.267895, mean_q: 0.329350, mean_eps: 0.848850\n",
      "  169106/2000000: episode: 253, duration: 8.287s, episode steps: 742, steps per second:  90, episode reward:  7.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.006611, mae: 0.267074, mean_q: 0.329749, mean_eps: 0.848139\n",
      "  170103/2000000: episode: 254, duration: 10.661s, episode steps: 997, steps per second:  94, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.006936, mae: 0.262926, mean_q: 0.323922, mean_eps: 0.847356\n",
      "  170592/2000000: episode: 255, duration: 5.270s, episode steps: 489, steps per second:  93, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.007511, mae: 0.291062, mean_q: 0.359638, mean_eps: 0.846689\n",
      "  171817/2000000: episode: 256, duration: 13.435s, episode steps: 1225, steps per second:  91, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.006866, mae: 0.288712, mean_q: 0.355714, mean_eps: 0.845916\n",
      "  172606/2000000: episode: 257, duration: 8.797s, episode steps: 789, steps per second:  90, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.007919, mae: 0.291650, mean_q: 0.358539, mean_eps: 0.845009\n",
      "  173204/2000000: episode: 258, duration: 6.533s, episode steps: 598, steps per second:  92, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.007612, mae: 0.286640, mean_q: 0.352005, mean_eps: 0.844386\n",
      "  173772/2000000: episode: 259, duration: 6.184s, episode steps: 568, steps per second:  92, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.006584, mae: 0.286571, mean_q: 0.354142, mean_eps: 0.843863\n",
      "  174251/2000000: episode: 260, duration: 5.300s, episode steps: 479, steps per second:  90, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.006764, mae: 0.287166, mean_q: 0.353484, mean_eps: 0.843391\n",
      "  174750/2000000: episode: 261, duration: 5.450s, episode steps: 499, steps per second:  92, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.007629, mae: 0.290447, mean_q: 0.355893, mean_eps: 0.842950\n",
      "  175121/2000000: episode: 262, duration: 4.172s, episode steps: 371, steps per second:  89, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.674 [0.000, 5.000],  loss: 0.005734, mae: 0.282249, mean_q: 0.346373, mean_eps: 0.842558\n",
      "  176428/2000000: episode: 263, duration: 14.485s, episode steps: 1307, steps per second:  90, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.007691, mae: 0.289507, mean_q: 0.356780, mean_eps: 0.841803\n",
      "  177148/2000000: episode: 264, duration: 7.854s, episode steps: 720, steps per second:  92, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.008085, mae: 0.288028, mean_q: 0.354822, mean_eps: 0.840893\n",
      "  178241/2000000: episode: 265, duration: 11.692s, episode steps: 1093, steps per second:  93, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.006741, mae: 0.286223, mean_q: 0.353288, mean_eps: 0.840075\n",
      "  179328/2000000: episode: 266, duration: 11.622s, episode steps: 1087, steps per second:  94, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.006540, mae: 0.288094, mean_q: 0.354181, mean_eps: 0.839094\n",
      "  180130/2000000: episode: 267, duration: 8.839s, episode steps: 802, steps per second:  91, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.008016, mae: 0.295196, mean_q: 0.363550, mean_eps: 0.838245\n",
      "  180944/2000000: episode: 268, duration: 8.565s, episode steps: 814, steps per second:  95, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.007908, mae: 0.316343, mean_q: 0.392716, mean_eps: 0.837518\n",
      "  181465/2000000: episode: 269, duration: 5.479s, episode steps: 521, steps per second:  95, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.007672, mae: 0.316633, mean_q: 0.392149, mean_eps: 0.836916\n",
      "  182530/2000000: episode: 270, duration: 11.523s, episode steps: 1065, steps per second:  92, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.006503, mae: 0.310768, mean_q: 0.383830, mean_eps: 0.836202\n",
      "  182994/2000000: episode: 271, duration: 4.896s, episode steps: 464, steps per second:  95, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.006583, mae: 0.314208, mean_q: 0.388098, mean_eps: 0.835514\n",
      "  183827/2000000: episode: 272, duration: 9.053s, episode steps: 833, steps per second:  92, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.007230, mae: 0.315803, mean_q: 0.390215, mean_eps: 0.834931\n",
      "  184238/2000000: episode: 273, duration: 4.328s, episode steps: 411, steps per second:  95, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.005285, mae: 0.307961, mean_q: 0.379891, mean_eps: 0.834371\n",
      "  185653/2000000: episode: 274, duration: 15.544s, episode steps: 1415, steps per second:  91, episode reward: 18.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.007627, mae: 0.313083, mean_q: 0.386647, mean_eps: 0.833549\n",
      "  186047/2000000: episode: 275, duration: 4.390s, episode steps: 394, steps per second:  90, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.004801, mae: 0.308176, mean_q: 0.379533, mean_eps: 0.832735\n",
      "  186977/2000000: episode: 276, duration: 10.102s, episode steps: 930, steps per second:  92, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.007919, mae: 0.314268, mean_q: 0.387873, mean_eps: 0.832139\n",
      "  187367/2000000: episode: 277, duration: 4.086s, episode steps: 390, steps per second:  95, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.008131, mae: 0.313695, mean_q: 0.386751, mean_eps: 0.831545\n",
      "  188633/2000000: episode: 278, duration: 13.514s, episode steps: 1266, steps per second:  94, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.007133, mae: 0.316226, mean_q: 0.391488, mean_eps: 0.830800\n",
      "  189222/2000000: episode: 279, duration: 6.291s, episode steps: 589, steps per second:  94, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.008234, mae: 0.316558, mean_q: 0.388969, mean_eps: 0.829965\n",
      "  189913/2000000: episode: 280, duration: 7.534s, episode steps: 691, steps per second:  92, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.008180, mae: 0.322320, mean_q: 0.397739, mean_eps: 0.829389\n",
      "  190633/2000000: episode: 281, duration: 8.033s, episode steps: 720, steps per second:  90, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.007054, mae: 0.337182, mean_q: 0.416044, mean_eps: 0.828753\n",
      "  191433/2000000: episode: 282, duration: 8.590s, episode steps: 800, steps per second:  93, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.007739, mae: 0.342036, mean_q: 0.421586, mean_eps: 0.828069\n",
      "  192121/2000000: episode: 283, duration: 7.572s, episode steps: 688, steps per second:  91, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.007171, mae: 0.339107, mean_q: 0.417533, mean_eps: 0.827400\n",
      "  192798/2000000: episode: 284, duration: 7.361s, episode steps: 677, steps per second:  92, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.008313, mae: 0.344477, mean_q: 0.423132, mean_eps: 0.826786\n",
      "  193274/2000000: episode: 285, duration: 5.698s, episode steps: 476, steps per second:  84, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.007022, mae: 0.341144, mean_q: 0.419550, mean_eps: 0.826268\n",
      "  193941/2000000: episode: 286, duration: 7.616s, episode steps: 667, steps per second:  88, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.007653, mae: 0.344064, mean_q: 0.422737, mean_eps: 0.825753\n",
      "  194990/2000000: episode: 287, duration: 11.859s, episode steps: 1049, steps per second:  88, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.007196, mae: 0.344566, mean_q: 0.426653, mean_eps: 0.824981\n",
      "  195386/2000000: episode: 288, duration: 4.512s, episode steps: 396, steps per second:  88, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.008527, mae: 0.352674, mean_q: 0.433688, mean_eps: 0.824331\n",
      "  196080/2000000: episode: 289, duration: 7.881s, episode steps: 694, steps per second:  88, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.007764, mae: 0.342385, mean_q: 0.421081, mean_eps: 0.823841\n",
      "  196603/2000000: episode: 290, duration: 6.051s, episode steps: 523, steps per second:  86, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.007587, mae: 0.349076, mean_q: 0.427962, mean_eps: 0.823294\n",
      "  197198/2000000: episode: 291, duration: 7.084s, episode steps: 595, steps per second:  84, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.006566, mae: 0.338080, mean_q: 0.417186, mean_eps: 0.822790\n",
      "  197874/2000000: episode: 292, duration: 8.396s, episode steps: 676, steps per second:  81, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.007182, mae: 0.338311, mean_q: 0.415895, mean_eps: 0.822218\n",
      "  198405/2000000: episode: 293, duration: 6.309s, episode steps: 531, steps per second:  84, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.007637, mae: 0.339243, mean_q: 0.417782, mean_eps: 0.821674\n",
      "  199163/2000000: episode: 294, duration: 11.213s, episode steps: 758, steps per second:  68, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.006603, mae: 0.343235, mean_q: 0.423330, mean_eps: 0.821094\n",
      "  199536/2000000: episode: 295, duration: 5.687s, episode steps: 373, steps per second:  66, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.008788, mae: 0.346010, mean_q: 0.425901, mean_eps: 0.820587\n",
      "  200222/2000000: episode: 296, duration: 9.755s, episode steps: 686, steps per second:  70, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.007501, mae: 0.352953, mean_q: 0.434826, mean_eps: 0.820110\n",
      "  200652/2000000: episode: 297, duration: 5.586s, episode steps: 430, steps per second:  77, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.007154, mae: 0.365592, mean_q: 0.451451, mean_eps: 0.819608\n",
      "  201122/2000000: episode: 298, duration: 5.865s, episode steps: 470, steps per second:  80, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.007662, mae: 0.364681, mean_q: 0.446723, mean_eps: 0.819203\n",
      "  202033/2000000: episode: 299, duration: 10.381s, episode steps: 911, steps per second:  88, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.007423, mae: 0.364453, mean_q: 0.449177, mean_eps: 0.818580\n",
      "  202582/2000000: episode: 300, duration: 6.453s, episode steps: 549, steps per second:  85, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.008441, mae: 0.370665, mean_q: 0.456623, mean_eps: 0.817923\n",
      "  203418/2000000: episode: 301, duration: 9.386s, episode steps: 836, steps per second:  89, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.008532, mae: 0.369542, mean_q: 0.454986, mean_eps: 0.817300\n",
      "  204077/2000000: episode: 302, duration: 7.535s, episode steps: 659, steps per second:  87, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.687 [0.000, 5.000],  loss: 0.008977, mae: 0.369614, mean_q: 0.453902, mean_eps: 0.816627\n",
      "  205081/2000000: episode: 303, duration: 11.559s, episode steps: 1004, steps per second:  87, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.007703, mae: 0.370631, mean_q: 0.455074, mean_eps: 0.815878\n",
      "  205628/2000000: episode: 304, duration: 6.434s, episode steps: 547, steps per second:  85, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.008508, mae: 0.360160, mean_q: 0.441122, mean_eps: 0.815181\n",
      "  205995/2000000: episode: 305, duration: 4.125s, episode steps: 367, steps per second:  89, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.009080, mae: 0.362708, mean_q: 0.445051, mean_eps: 0.814771\n",
      "  206651/2000000: episode: 306, duration: 7.382s, episode steps: 656, steps per second:  89, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.007484, mae: 0.364114, mean_q: 0.446621, mean_eps: 0.814310\n",
      "  207484/2000000: episode: 307, duration: 9.929s, episode steps: 833, steps per second:  84, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.009624, mae: 0.371057, mean_q: 0.456631, mean_eps: 0.813641\n",
      "  208536/2000000: episode: 308, duration: 12.267s, episode steps: 1052, steps per second:  86, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.007877, mae: 0.363616, mean_q: 0.447457, mean_eps: 0.812793\n",
      "  209014/2000000: episode: 309, duration: 5.481s, episode steps: 478, steps per second:  87, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.008549, mae: 0.371563, mean_q: 0.457861, mean_eps: 0.812103\n",
      "  210012/2000000: episode: 310, duration: 11.181s, episode steps: 998, steps per second:  89, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.008040, mae: 0.368673, mean_q: 0.452922, mean_eps: 0.811439\n",
      "  210616/2000000: episode: 311, duration: 6.783s, episode steps: 604, steps per second:  89, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.007597, mae: 0.393349, mean_q: 0.484638, mean_eps: 0.810719\n",
      "  211130/2000000: episode: 312, duration: 5.851s, episode steps: 514, steps per second:  88, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.007492, mae: 0.398651, mean_q: 0.491268, mean_eps: 0.810215\n",
      "  211899/2000000: episode: 313, duration: 8.612s, episode steps: 769, steps per second:  89, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.007794, mae: 0.396695, mean_q: 0.488831, mean_eps: 0.809637\n",
      "  212996/2000000: episode: 314, duration: 12.372s, episode steps: 1097, steps per second:  89, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.008816, mae: 0.399462, mean_q: 0.491070, mean_eps: 0.808799\n",
      "  213492/2000000: episode: 315, duration: 5.542s, episode steps: 496, steps per second:  89, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.007568, mae: 0.398686, mean_q: 0.490425, mean_eps: 0.808082\n",
      "  214309/2000000: episode: 316, duration: 9.237s, episode steps: 817, steps per second:  88, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.009070, mae: 0.401953, mean_q: 0.494756, mean_eps: 0.807490\n",
      "  214710/2000000: episode: 317, duration: 4.481s, episode steps: 401, steps per second:  89, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.008306, mae: 0.396917, mean_q: 0.487512, mean_eps: 0.806941\n",
      "  215378/2000000: episode: 318, duration: 7.632s, episode steps: 668, steps per second:  88, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.008850, mae: 0.399856, mean_q: 0.489865, mean_eps: 0.806460\n",
      "  216171/2000000: episode: 319, duration: 9.031s, episode steps: 793, steps per second:  88, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.008850, mae: 0.402721, mean_q: 0.495373, mean_eps: 0.805803\n",
      "  216701/2000000: episode: 320, duration: 6.153s, episode steps: 530, steps per second:  86, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.007978, mae: 0.400173, mean_q: 0.492194, mean_eps: 0.805208\n",
      "  217723/2000000: episode: 321, duration: 11.535s, episode steps: 1022, steps per second:  89, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.007971, mae: 0.397533, mean_q: 0.489502, mean_eps: 0.804509\n",
      "  218260/2000000: episode: 322, duration: 6.339s, episode steps: 537, steps per second:  85, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.008586, mae: 0.403597, mean_q: 0.495509, mean_eps: 0.803809\n",
      "  218593/2000000: episode: 323, duration: 3.724s, episode steps: 333, steps per second:  89, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.007991, mae: 0.399661, mean_q: 0.492304, mean_eps: 0.803417\n",
      "  219215/2000000: episode: 324, duration: 6.932s, episode steps: 622, steps per second:  90, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.007537, mae: 0.398559, mean_q: 0.490971, mean_eps: 0.802986\n",
      "  220037/2000000: episode: 325, duration: 9.558s, episode steps: 822, steps per second:  86, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.007343, mae: 0.403573, mean_q: 0.496412, mean_eps: 0.802337\n",
      "  220785/2000000: episode: 326, duration: 8.719s, episode steps: 748, steps per second:  86, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.007888, mae: 0.416425, mean_q: 0.512384, mean_eps: 0.801629\n",
      "  221916/2000000: episode: 327, duration: 12.877s, episode steps: 1131, steps per second:  88, episode reward: 10.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.008062, mae: 0.420956, mean_q: 0.516802, mean_eps: 0.800785\n",
      "  222542/2000000: episode: 328, duration: 7.043s, episode steps: 626, steps per second:  89, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.007626, mae: 0.425052, mean_q: 0.521797, mean_eps: 0.799995\n",
      "  223127/2000000: episode: 329, duration: 6.444s, episode steps: 585, steps per second:  91, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.008103, mae: 0.418680, mean_q: 0.513584, mean_eps: 0.799449\n",
      "  223516/2000000: episode: 330, duration: 4.421s, episode steps: 389, steps per second:  88, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.007641, mae: 0.424671, mean_q: 0.522852, mean_eps: 0.799012\n",
      "  223955/2000000: episode: 331, duration: 4.884s, episode steps: 439, steps per second:  90, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.008413, mae: 0.415987, mean_q: 0.512843, mean_eps: 0.798639\n",
      "  224834/2000000: episode: 332, duration: 9.569s, episode steps: 879, steps per second:  92, episode reward: 12.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.009201, mae: 0.421659, mean_q: 0.517434, mean_eps: 0.798045\n",
      "  225654/2000000: episode: 333, duration: 9.190s, episode steps: 820, steps per second:  89, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.008339, mae: 0.423281, mean_q: 0.519675, mean_eps: 0.797280\n",
      "  226329/2000000: episode: 334, duration: 7.491s, episode steps: 675, steps per second:  90, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.008252, mae: 0.420990, mean_q: 0.515653, mean_eps: 0.796607\n",
      "  227173/2000000: episode: 335, duration: 9.306s, episode steps: 844, steps per second:  91, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.007771, mae: 0.422602, mean_q: 0.519192, mean_eps: 0.795923\n",
      "  227655/2000000: episode: 336, duration: 5.204s, episode steps: 482, steps per second:  93, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.007408, mae: 0.417716, mean_q: 0.512897, mean_eps: 0.795327\n",
      "  228317/2000000: episode: 337, duration: 7.474s, episode steps: 662, steps per second:  89, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.007896, mae: 0.421389, mean_q: 0.516331, mean_eps: 0.794813\n",
      "  228716/2000000: episode: 338, duration: 4.347s, episode steps: 399, steps per second:  92, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.008740, mae: 0.414986, mean_q: 0.509422, mean_eps: 0.794336\n",
      "  229558/2000000: episode: 339, duration: 9.494s, episode steps: 842, steps per second:  89, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.009096, mae: 0.426599, mean_q: 0.524969, mean_eps: 0.793778\n",
      "  230199/2000000: episode: 340, duration: 7.164s, episode steps: 641, steps per second:  89, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.007354, mae: 0.427611, mean_q: 0.524793, mean_eps: 0.793110\n",
      "  230828/2000000: episode: 341, duration: 7.049s, episode steps: 629, steps per second:  89, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.008208, mae: 0.442001, mean_q: 0.543057, mean_eps: 0.792539\n",
      "  231417/2000000: episode: 342, duration: 6.551s, episode steps: 589, steps per second:  90, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.007011, mae: 0.441084, mean_q: 0.543368, mean_eps: 0.791990\n",
      "  231819/2000000: episode: 343, duration: 4.506s, episode steps: 402, steps per second:  89, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.007341, mae: 0.446546, mean_q: 0.548539, mean_eps: 0.791544\n",
      "  232951/2000000: episode: 344, duration: 12.598s, episode steps: 1132, steps per second:  90, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.007209, mae: 0.443484, mean_q: 0.545875, mean_eps: 0.790854\n",
      "  233467/2000000: episode: 345, duration: 5.599s, episode steps: 516, steps per second:  92, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.009089, mae: 0.451640, mean_q: 0.555137, mean_eps: 0.790113\n",
      "  233979/2000000: episode: 346, duration: 5.774s, episode steps: 512, steps per second:  89, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.006858, mae: 0.436001, mean_q: 0.538569, mean_eps: 0.789650\n",
      "  234369/2000000: episode: 347, duration: 4.380s, episode steps: 390, steps per second:  89, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.006229, mae: 0.433828, mean_q: 0.535689, mean_eps: 0.789243\n",
      "  235032/2000000: episode: 348, duration: 7.287s, episode steps: 663, steps per second:  91, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.006831, mae: 0.440272, mean_q: 0.540671, mean_eps: 0.788770\n",
      "  235401/2000000: episode: 349, duration: 4.127s, episode steps: 369, steps per second:  89, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.006265, mae: 0.438907, mean_q: 0.538278, mean_eps: 0.788306\n",
      "  235806/2000000: episode: 350, duration: 4.529s, episode steps: 405, steps per second:  89, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.008270, mae: 0.444964, mean_q: 0.546424, mean_eps: 0.787956\n",
      "  236198/2000000: episode: 351, duration: 4.251s, episode steps: 392, steps per second:  92, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.008252, mae: 0.438907, mean_q: 0.539125, mean_eps: 0.787598\n",
      "  237112/2000000: episode: 352, duration: 10.206s, episode steps: 914, steps per second:  90, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.008119, mae: 0.444237, mean_q: 0.545245, mean_eps: 0.787011\n",
      "  237524/2000000: episode: 353, duration: 5.037s, episode steps: 412, steps per second:  82, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.008117, mae: 0.455841, mean_q: 0.561860, mean_eps: 0.786416\n",
      "  238104/2000000: episode: 354, duration: 6.794s, episode steps: 580, steps per second:  85, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.007714, mae: 0.441204, mean_q: 0.540554, mean_eps: 0.785969\n",
      "  238908/2000000: episode: 355, duration: 8.758s, episode steps: 804, steps per second:  92, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.008479, mae: 0.442533, mean_q: 0.542980, mean_eps: 0.785346\n",
      "  239300/2000000: episode: 356, duration: 4.627s, episode steps: 392, steps per second:  85, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.059 [0.000, 5.000],  loss: 0.007138, mae: 0.439019, mean_q: 0.538991, mean_eps: 0.784808\n",
      "  240176/2000000: episode: 357, duration: 9.883s, episode steps: 876, steps per second:  89, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.007163, mae: 0.445466, mean_q: 0.547440, mean_eps: 0.784238\n",
      "  240861/2000000: episode: 358, duration: 7.724s, episode steps: 685, steps per second:  89, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.006905, mae: 0.463580, mean_q: 0.569656, mean_eps: 0.783534\n",
      "  241764/2000000: episode: 359, duration: 9.993s, episode steps: 903, steps per second:  90, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.008101, mae: 0.467576, mean_q: 0.574151, mean_eps: 0.782819\n",
      "  242680/2000000: episode: 360, duration: 10.343s, episode steps: 916, steps per second:  89, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.008653, mae: 0.476412, mean_q: 0.585593, mean_eps: 0.782002\n",
      "  243381/2000000: episode: 361, duration: 7.811s, episode steps: 701, steps per second:  90, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.007665, mae: 0.466874, mean_q: 0.574945, mean_eps: 0.781273\n",
      "  243922/2000000: episode: 362, duration: 6.172s, episode steps: 541, steps per second:  88, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.008837, mae: 0.472257, mean_q: 0.579309, mean_eps: 0.780713\n",
      "  244426/2000000: episode: 363, duration: 5.439s, episode steps: 504, steps per second:  93, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.008436, mae: 0.475149, mean_q: 0.583726, mean_eps: 0.780243\n",
      "  245055/2000000: episode: 364, duration: 6.956s, episode steps: 629, steps per second:  90, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.008085, mae: 0.468671, mean_q: 0.577670, mean_eps: 0.779734\n",
      "  245786/2000000: episode: 365, duration: 7.912s, episode steps: 731, steps per second:  92, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.008139, mae: 0.469970, mean_q: 0.578410, mean_eps: 0.779122\n",
      "  246461/2000000: episode: 366, duration: 7.313s, episode steps: 675, steps per second:  92, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.008540, mae: 0.464704, mean_q: 0.568708, mean_eps: 0.778488\n",
      "  247093/2000000: episode: 367, duration: 6.932s, episode steps: 632, steps per second:  91, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.009353, mae: 0.474563, mean_q: 0.582631, mean_eps: 0.777900\n",
      "  247472/2000000: episode: 368, duration: 4.173s, episode steps: 379, steps per second:  91, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.007962, mae: 0.466908, mean_q: 0.573628, mean_eps: 0.777446\n",
      "  248162/2000000: episode: 369, duration: 7.713s, episode steps: 690, steps per second:  89, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.008550, mae: 0.476251, mean_q: 0.585719, mean_eps: 0.776966\n",
      "  249050/2000000: episode: 370, duration: 9.783s, episode steps: 888, steps per second:  91, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.008248, mae: 0.474135, mean_q: 0.582625, mean_eps: 0.776255\n",
      "  250007/2000000: episode: 371, duration: 11.028s, episode steps: 957, steps per second:  87, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.008163, mae: 0.481108, mean_q: 0.592395, mean_eps: 0.775425\n",
      "  250709/2000000: episode: 372, duration: 8.085s, episode steps: 702, steps per second:  87, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.008036, mae: 0.507248, mean_q: 0.624140, mean_eps: 0.774678\n",
      "  251300/2000000: episode: 373, duration: 6.618s, episode steps: 591, steps per second:  89, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.563 [0.000, 5.000],  loss: 0.009923, mae: 0.504266, mean_q: 0.621030, mean_eps: 0.774096\n",
      "  251700/2000000: episode: 374, duration: 4.350s, episode steps: 400, steps per second:  92, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.009176, mae: 0.519150, mean_q: 0.639045, mean_eps: 0.773652\n",
      "  252506/2000000: episode: 375, duration: 8.806s, episode steps: 806, steps per second:  92, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.008450, mae: 0.510212, mean_q: 0.628361, mean_eps: 0.773108\n",
      "  253598/2000000: episode: 376, duration: 12.213s, episode steps: 1092, steps per second:  89, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.009186, mae: 0.507772, mean_q: 0.624077, mean_eps: 0.772253\n",
      "  254070/2000000: episode: 377, duration: 5.380s, episode steps: 472, steps per second:  88, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.009819, mae: 0.506038, mean_q: 0.619991, mean_eps: 0.771549\n",
      "  255317/2000000: episode: 378, duration: 14.140s, episode steps: 1247, steps per second:  88, episode reward: 14.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.008112, mae: 0.511076, mean_q: 0.629169, mean_eps: 0.770775\n",
      "  256272/2000000: episode: 379, duration: 10.550s, episode steps: 955, steps per second:  91, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.007815, mae: 0.506365, mean_q: 0.620695, mean_eps: 0.769785\n",
      "  256838/2000000: episode: 380, duration: 6.348s, episode steps: 566, steps per second:  89, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.007409, mae: 0.507355, mean_q: 0.621844, mean_eps: 0.769101\n",
      "  257388/2000000: episode: 381, duration: 5.908s, episode steps: 550, steps per second:  93, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.007792, mae: 0.505260, mean_q: 0.618688, mean_eps: 0.768599\n",
      "  257893/2000000: episode: 382, duration: 5.499s, episode steps: 505, steps per second:  92, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.008868, mae: 0.515308, mean_q: 0.634385, mean_eps: 0.768124\n",
      "  258531/2000000: episode: 383, duration: 7.050s, episode steps: 638, steps per second:  91, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.007829, mae: 0.509085, mean_q: 0.625376, mean_eps: 0.767609\n",
      "  259607/2000000: episode: 384, duration: 11.901s, episode steps: 1076, steps per second:  90, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.008503, mae: 0.506389, mean_q: 0.621462, mean_eps: 0.766839\n",
      "  259993/2000000: episode: 385, duration: 4.220s, episode steps: 386, steps per second:  91, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.009989, mae: 0.508407, mean_q: 0.624691, mean_eps: 0.766180\n",
      "  260398/2000000: episode: 386, duration: 4.457s, episode steps: 405, steps per second:  91, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.008485, mae: 0.537125, mean_q: 0.662384, mean_eps: 0.765824\n",
      "  261188/2000000: episode: 387, duration: 9.085s, episode steps: 790, steps per second:  87, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.009669, mae: 0.545497, mean_q: 0.671950, mean_eps: 0.765287\n",
      "  261629/2000000: episode: 388, duration: 5.044s, episode steps: 441, steps per second:  87, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.009546, mae: 0.558123, mean_q: 0.686341, mean_eps: 0.764733\n",
      "  262217/2000000: episode: 389, duration: 6.678s, episode steps: 588, steps per second:  88, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.010440, mae: 0.555702, mean_q: 0.682382, mean_eps: 0.764268\n",
      "  262997/2000000: episode: 390, duration: 8.658s, episode steps: 780, steps per second:  90, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.007793, mae: 0.544060, mean_q: 0.669677, mean_eps: 0.763653\n",
      "  263997/2000000: episode: 391, duration: 11.262s, episode steps: 1000, steps per second:  89, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.007509, mae: 0.551475, mean_q: 0.676482, mean_eps: 0.762852\n",
      "  264390/2000000: episode: 392, duration: 4.283s, episode steps: 393, steps per second:  92, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.007425, mae: 0.553721, mean_q: 0.680159, mean_eps: 0.762225\n",
      "  265391/2000000: episode: 393, duration: 11.089s, episode steps: 1001, steps per second:  90, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.008829, mae: 0.548955, mean_q: 0.673296, mean_eps: 0.761599\n",
      "  266111/2000000: episode: 394, duration: 8.051s, episode steps: 720, steps per second:  89, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.008681, mae: 0.547155, mean_q: 0.670526, mean_eps: 0.760825\n",
      "  266758/2000000: episode: 395, duration: 7.499s, episode steps: 647, steps per second:  86, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.009237, mae: 0.554990, mean_q: 0.685221, mean_eps: 0.760209\n",
      "  267900/2000000: episode: 396, duration: 12.595s, episode steps: 1142, steps per second:  91, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.008157, mae: 0.553171, mean_q: 0.678224, mean_eps: 0.759405\n",
      "  268531/2000000: episode: 397, duration: 6.962s, episode steps: 631, steps per second:  91, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.009698, mae: 0.548408, mean_q: 0.673436, mean_eps: 0.758607\n",
      "  269399/2000000: episode: 398, duration: 9.699s, episode steps: 868, steps per second:  89, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.427 [0.000, 5.000],  loss: 0.008734, mae: 0.550459, mean_q: 0.675707, mean_eps: 0.757932\n",
      "  270032/2000000: episode: 399, duration: 7.159s, episode steps: 633, steps per second:  88, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.009368, mae: 0.550342, mean_q: 0.673987, mean_eps: 0.757257\n",
      "  270433/2000000: episode: 400, duration: 4.544s, episode steps: 401, steps per second:  88, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.392 [0.000, 5.000],  loss: 0.008447, mae: 0.575229, mean_q: 0.706322, mean_eps: 0.756791\n",
      "  270941/2000000: episode: 401, duration: 5.911s, episode steps: 508, steps per second:  86, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.008354, mae: 0.579209, mean_q: 0.715109, mean_eps: 0.756381\n",
      "  271674/2000000: episode: 402, duration: 8.376s, episode steps: 733, steps per second:  88, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.009466, mae: 0.572969, mean_q: 0.702997, mean_eps: 0.755823\n",
      "  272365/2000000: episode: 403, duration: 8.080s, episode steps: 691, steps per second:  86, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.008752, mae: 0.580269, mean_q: 0.711839, mean_eps: 0.755182\n",
      "  273006/2000000: episode: 404, duration: 7.448s, episode steps: 641, steps per second:  86, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.008886, mae: 0.573763, mean_q: 0.705974, mean_eps: 0.754583\n",
      "  273900/2000000: episode: 405, duration: 10.310s, episode steps: 894, steps per second:  87, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.008244, mae: 0.574834, mean_q: 0.707546, mean_eps: 0.753893\n",
      "  274754/2000000: episode: 406, duration: 9.566s, episode steps: 854, steps per second:  89, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.008740, mae: 0.568797, mean_q: 0.697582, mean_eps: 0.753107\n",
      "  275639/2000000: episode: 407, duration: 9.805s, episode steps: 885, steps per second:  90, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.009954, mae: 0.582532, mean_q: 0.714244, mean_eps: 0.752324\n",
      "  276284/2000000: episode: 408, duration: 7.039s, episode steps: 645, steps per second:  92, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.008760, mae: 0.576584, mean_q: 0.706444, mean_eps: 0.751636\n",
      "  276772/2000000: episode: 409, duration: 5.545s, episode steps: 488, steps per second:  88, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.383 [0.000, 5.000],  loss: 0.009888, mae: 0.586257, mean_q: 0.718306, mean_eps: 0.751127\n",
      "  277505/2000000: episode: 410, duration: 8.325s, episode steps: 733, steps per second:  88, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.008860, mae: 0.577240, mean_q: 0.709356, mean_eps: 0.750576\n",
      "  278053/2000000: episode: 411, duration: 6.156s, episode steps: 548, steps per second:  89, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.009125, mae: 0.571054, mean_q: 0.700979, mean_eps: 0.749998\n",
      "  278704/2000000: episode: 412, duration: 7.124s, episode steps: 651, steps per second:  91, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.009096, mae: 0.586338, mean_q: 0.719931, mean_eps: 0.749460\n",
      "  279204/2000000: episode: 413, duration: 5.684s, episode steps: 500, steps per second:  88, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.008831, mae: 0.577857, mean_q: 0.712794, mean_eps: 0.748943\n",
      "  280022/2000000: episode: 414, duration: 9.460s, episode steps: 818, steps per second:  86, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.348 [0.000, 5.000],  loss: 0.008376, mae: 0.578059, mean_q: 0.710621, mean_eps: 0.748349\n",
      "  280427/2000000: episode: 415, duration: 4.703s, episode steps: 405, steps per second:  86, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.009475, mae: 0.613891, mean_q: 0.752908, mean_eps: 0.747798\n",
      "  280943/2000000: episode: 416, duration: 6.105s, episode steps: 516, steps per second:  85, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.008368, mae: 0.607863, mean_q: 0.745828, mean_eps: 0.747384\n",
      "  281321/2000000: episode: 417, duration: 4.333s, episode steps: 378, steps per second:  87, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.009326, mae: 0.607521, mean_q: 0.746724, mean_eps: 0.746981\n",
      "  282087/2000000: episode: 418, duration: 9.015s, episode steps: 766, steps per second:  85, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.008934, mae: 0.607472, mean_q: 0.744162, mean_eps: 0.746466\n",
      "  282582/2000000: episode: 419, duration: 5.904s, episode steps: 495, steps per second:  84, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.008895, mae: 0.604323, mean_q: 0.741594, mean_eps: 0.745899\n",
      "  283247/2000000: episode: 420, duration: 7.666s, episode steps: 665, steps per second:  87, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.008505, mae: 0.604308, mean_q: 0.742524, mean_eps: 0.745377\n",
      "  284182/2000000: episode: 421, duration: 10.881s, episode steps: 935, steps per second:  86, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.008668, mae: 0.608081, mean_q: 0.747133, mean_eps: 0.744657\n",
      "  285235/2000000: episode: 422, duration: 12.332s, episode steps: 1053, steps per second:  85, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.008437, mae: 0.603329, mean_q: 0.741478, mean_eps: 0.743763\n",
      "  285791/2000000: episode: 423, duration: 6.366s, episode steps: 556, steps per second:  87, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.009466, mae: 0.616902, mean_q: 0.755045, mean_eps: 0.743039\n",
      "  286688/2000000: episode: 424, duration: 10.533s, episode steps: 897, steps per second:  85, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.010468, mae: 0.605530, mean_q: 0.741368, mean_eps: 0.742386\n",
      "  287080/2000000: episode: 425, duration: 4.514s, episode steps: 392, steps per second:  87, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.008329, mae: 0.608250, mean_q: 0.748274, mean_eps: 0.741806\n",
      "  287877/2000000: episode: 426, duration: 9.497s, episode steps: 797, steps per second:  84, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.008947, mae: 0.601838, mean_q: 0.737490, mean_eps: 0.741270\n",
      "  288702/2000000: episode: 427, duration: 9.166s, episode steps: 825, steps per second:  90, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.009580, mae: 0.605165, mean_q: 0.739820, mean_eps: 0.740539\n",
      "  289179/2000000: episode: 428, duration: 5.503s, episode steps: 477, steps per second:  87, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.008513, mae: 0.611950, mean_q: 0.749985, mean_eps: 0.739954\n",
      "  289832/2000000: episode: 429, duration: 7.379s, episode steps: 653, steps per second:  88, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.010031, mae: 0.610653, mean_q: 0.748985, mean_eps: 0.739446\n",
      "  290260/2000000: episode: 430, duration: 5.183s, episode steps: 428, steps per second:  83, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.007796, mae: 0.620724, mean_q: 0.763844, mean_eps: 0.738960\n",
      "  290769/2000000: episode: 431, duration: 6.062s, episode steps: 509, steps per second:  84, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.007378, mae: 0.633894, mean_q: 0.777554, mean_eps: 0.738537\n",
      "  291269/2000000: episode: 432, duration: 5.808s, episode steps: 500, steps per second:  86, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.009563, mae: 0.634944, mean_q: 0.779292, mean_eps: 0.738082\n",
      "  292307/2000000: episode: 433, duration: 12.138s, episode steps: 1038, steps per second:  86, episode reward: 32.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.008982, mae: 0.637928, mean_q: 0.782937, mean_eps: 0.737391\n",
      "  292692/2000000: episode: 434, duration: 4.689s, episode steps: 385, steps per second:  82, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.009861, mae: 0.638904, mean_q: 0.784347, mean_eps: 0.736752\n",
      "  293642/2000000: episode: 435, duration: 11.319s, episode steps: 950, steps per second:  84, episode reward:  9.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.008322, mae: 0.638905, mean_q: 0.784493, mean_eps: 0.736151\n",
      "  294165/2000000: episode: 436, duration: 6.048s, episode steps: 523, steps per second:  86, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.009693, mae: 0.649370, mean_q: 0.798237, mean_eps: 0.735486\n",
      "  294672/2000000: episode: 437, duration: 5.998s, episode steps: 507, steps per second:  85, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.008933, mae: 0.634878, mean_q: 0.779188, mean_eps: 0.735024\n",
      "  295260/2000000: episode: 438, duration: 6.857s, episode steps: 588, steps per second:  86, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.010353, mae: 0.633235, mean_q: 0.776487, mean_eps: 0.734532\n",
      "  295850/2000000: episode: 439, duration: 7.143s, episode steps: 590, steps per second:  83, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.631 [0.000, 5.000],  loss: 0.009199, mae: 0.637269, mean_q: 0.781993, mean_eps: 0.734001\n",
      "  296627/2000000: episode: 440, duration: 8.759s, episode steps: 777, steps per second:  89, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.009732, mae: 0.638919, mean_q: 0.784991, mean_eps: 0.733386\n",
      "  297366/2000000: episode: 441, duration: 8.373s, episode steps: 739, steps per second:  88, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.008848, mae: 0.633865, mean_q: 0.778509, mean_eps: 0.732704\n",
      "  298316/2000000: episode: 442, duration: 10.719s, episode steps: 950, steps per second:  89, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.452 [0.000, 5.000],  loss: 0.009898, mae: 0.641283, mean_q: 0.789704, mean_eps: 0.731944\n",
      "  298971/2000000: episode: 443, duration: 7.476s, episode steps: 655, steps per second:  88, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.008641, mae: 0.639648, mean_q: 0.784574, mean_eps: 0.731222\n",
      "  299345/2000000: episode: 444, duration: 4.258s, episode steps: 374, steps per second:  88, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.008790, mae: 0.641991, mean_q: 0.786753, mean_eps: 0.730758\n",
      "  299736/2000000: episode: 445, duration: 4.323s, episode steps: 391, steps per second:  90, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.009958, mae: 0.644069, mean_q: 0.789442, mean_eps: 0.730414\n",
      "  300211/2000000: episode: 446, duration: 5.494s, episode steps: 475, steps per second:  86, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.009668, mae: 0.663101, mean_q: 0.811766, mean_eps: 0.730025\n",
      "  300966/2000000: episode: 447, duration: 8.649s, episode steps: 755, steps per second:  87, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.009510, mae: 0.684496, mean_q: 0.840315, mean_eps: 0.729471\n",
      "  302110/2000000: episode: 448, duration: 13.104s, episode steps: 1144, steps per second:  87, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.008394, mae: 0.684751, mean_q: 0.838812, mean_eps: 0.728616\n",
      "  302831/2000000: episode: 449, duration: 8.233s, episode steps: 721, steps per second:  88, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.009291, mae: 0.685078, mean_q: 0.840005, mean_eps: 0.727777\n",
      "  303479/2000000: episode: 450, duration: 7.630s, episode steps: 648, steps per second:  85, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.009342, mae: 0.682730, mean_q: 0.836449, mean_eps: 0.727161\n",
      "  303978/2000000: episode: 451, duration: 5.740s, episode steps: 499, steps per second:  87, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.009818, mae: 0.690826, mean_q: 0.846397, mean_eps: 0.726645\n",
      "  304517/2000000: episode: 452, duration: 6.401s, episode steps: 539, steps per second:  84, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.009186, mae: 0.691174, mean_q: 0.846555, mean_eps: 0.726177\n",
      "  305022/2000000: episode: 453, duration: 5.798s, episode steps: 505, steps per second:  87, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.009801, mae: 0.687553, mean_q: 0.841422, mean_eps: 0.725707\n",
      "  305791/2000000: episode: 454, duration: 9.127s, episode steps: 769, steps per second:  84, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.008990, mae: 0.685352, mean_q: 0.839724, mean_eps: 0.725135\n",
      "  306826/2000000: episode: 455, duration: 11.661s, episode steps: 1035, steps per second:  89, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.008796, mae: 0.684261, mean_q: 0.839393, mean_eps: 0.724323\n",
      "  307358/2000000: episode: 456, duration: 6.088s, episode steps: 532, steps per second:  87, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.009779, mae: 0.685807, mean_q: 0.838028, mean_eps: 0.723617\n",
      "  307742/2000000: episode: 457, duration: 4.254s, episode steps: 384, steps per second:  90, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.009558, mae: 0.682583, mean_q: 0.832224, mean_eps: 0.723205\n",
      "  308384/2000000: episode: 458, duration: 7.631s, episode steps: 642, steps per second:  84, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.009725, mae: 0.697570, mean_q: 0.853726, mean_eps: 0.722744\n",
      "  309003/2000000: episode: 459, duration: 7.130s, episode steps: 619, steps per second:  87, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.009316, mae: 0.685047, mean_q: 0.838938, mean_eps: 0.722177\n",
      "  309624/2000000: episode: 460, duration: 7.061s, episode steps: 621, steps per second:  88, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.008985, mae: 0.688042, mean_q: 0.841920, mean_eps: 0.721619\n",
      "  310237/2000000: episode: 461, duration: 6.978s, episode steps: 613, steps per second:  88, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.010295, mae: 0.691686, mean_q: 0.845893, mean_eps: 0.721063\n",
      "  310709/2000000: episode: 462, duration: 5.367s, episode steps: 472, steps per second:  88, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.008459, mae: 0.685971, mean_q: 0.841081, mean_eps: 0.720573\n",
      "  311100/2000000: episode: 463, duration: 4.808s, episode steps: 391, steps per second:  81, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.009425, mae: 0.692756, mean_q: 0.850730, mean_eps: 0.720186\n",
      "  311876/2000000: episode: 464, duration: 9.135s, episode steps: 776, steps per second:  85, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.008729, mae: 0.695889, mean_q: 0.852079, mean_eps: 0.719663\n",
      "  312250/2000000: episode: 465, duration: 4.314s, episode steps: 374, steps per second:  87, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.008826, mae: 0.685531, mean_q: 0.837942, mean_eps: 0.719144\n",
      "  312640/2000000: episode: 466, duration: 4.768s, episode steps: 390, steps per second:  82, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.009236, mae: 0.676146, mean_q: 0.827788, mean_eps: 0.718800\n",
      "  313588/2000000: episode: 467, duration: 11.307s, episode steps: 948, steps per second:  84, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.008098, mae: 0.687222, mean_q: 0.843130, mean_eps: 0.718199\n",
      "  314406/2000000: episode: 468, duration: 10.015s, episode steps: 818, steps per second:  82, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.009517, mae: 0.691200, mean_q: 0.847583, mean_eps: 0.717404\n",
      "  315068/2000000: episode: 469, duration: 7.844s, episode steps: 662, steps per second:  84, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.010176, mae: 0.689623, mean_q: 0.843085, mean_eps: 0.716738\n",
      "  315736/2000000: episode: 470, duration: 7.967s, episode steps: 668, steps per second:  84, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.008826, mae: 0.687882, mean_q: 0.842257, mean_eps: 0.716140\n",
      "  316691/2000000: episode: 471, duration: 11.550s, episode steps: 955, steps per second:  83, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.836 [0.000, 5.000],  loss: 0.009745, mae: 0.684913, mean_q: 0.839307, mean_eps: 0.715409\n",
      "  317507/2000000: episode: 472, duration: 9.660s, episode steps: 816, steps per second:  84, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.009712, mae: 0.683672, mean_q: 0.838382, mean_eps: 0.714612\n",
      "  318490/2000000: episode: 473, duration: 12.036s, episode steps: 983, steps per second:  82, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.009617, mae: 0.688282, mean_q: 0.843952, mean_eps: 0.713802\n",
      "  319328/2000000: episode: 474, duration: 10.214s, episode steps: 838, steps per second:  82, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.009457, mae: 0.690193, mean_q: 0.846575, mean_eps: 0.712983\n",
      "  320824/2000000: episode: 475, duration: 17.160s, episode steps: 1496, steps per second:  87, episode reward: 29.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.009279, mae: 0.694434, mean_q: 0.851809, mean_eps: 0.711933\n",
      "  321209/2000000: episode: 476, duration: 4.608s, episode steps: 385, steps per second:  84, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.006365, mae: 0.703045, mean_q: 0.860644, mean_eps: 0.711086\n",
      "  321899/2000000: episode: 477, duration: 8.384s, episode steps: 690, steps per second:  82, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.010322, mae: 0.706668, mean_q: 0.864395, mean_eps: 0.710601\n",
      "  322462/2000000: episode: 478, duration: 6.702s, episode steps: 563, steps per second:  84, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.010077, mae: 0.706636, mean_q: 0.866480, mean_eps: 0.710038\n",
      "  323273/2000000: episode: 479, duration: 10.024s, episode steps: 811, steps per second:  81, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.009799, mae: 0.708715, mean_q: 0.869672, mean_eps: 0.709419\n",
      "  324234/2000000: episode: 480, duration: 11.698s, episode steps: 961, steps per second:  82, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.009450, mae: 0.704126, mean_q: 0.864285, mean_eps: 0.708621\n",
      "  325013/2000000: episode: 481, duration: 9.384s, episode steps: 779, steps per second:  83, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.009944, mae: 0.714510, mean_q: 0.874659, mean_eps: 0.707838\n",
      "  325522/2000000: episode: 482, duration: 5.882s, episode steps: 509, steps per second:  87, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: 0.008086, mae: 0.701919, mean_q: 0.861526, mean_eps: 0.707259\n",
      "  326171/2000000: episode: 483, duration: 7.895s, episode steps: 649, steps per second:  82, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.009926, mae: 0.711028, mean_q: 0.870135, mean_eps: 0.706739\n",
      "  326984/2000000: episode: 484, duration: 9.683s, episode steps: 813, steps per second:  84, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.010505, mae: 0.713847, mean_q: 0.871904, mean_eps: 0.706082\n",
      "  327464/2000000: episode: 485, duration: 5.612s, episode steps: 480, steps per second:  86, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.581 [0.000, 5.000],  loss: 0.010460, mae: 0.703250, mean_q: 0.859602, mean_eps: 0.705500\n",
      "  327845/2000000: episode: 486, duration: 4.350s, episode steps: 381, steps per second:  88, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.007564, mae: 0.693721, mean_q: 0.849269, mean_eps: 0.705111\n",
      "  328703/2000000: episode: 487, duration: 10.039s, episode steps: 858, steps per second:  85, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.008514, mae: 0.692873, mean_q: 0.847402, mean_eps: 0.704553\n",
      "  329427/2000000: episode: 488, duration: 8.447s, episode steps: 724, steps per second:  86, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.010287, mae: 0.705410, mean_q: 0.864084, mean_eps: 0.703842\n",
      "  329958/2000000: episode: 489, duration: 6.344s, episode steps: 531, steps per second:  84, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.009467, mae: 0.708446, mean_q: 0.869052, mean_eps: 0.703277\n",
      "  330510/2000000: episode: 490, duration: 6.335s, episode steps: 552, steps per second:  87, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.010419, mae: 0.745975, mean_q: 0.914408, mean_eps: 0.702789\n",
      "  331041/2000000: episode: 491, duration: 6.285s, episode steps: 531, steps per second:  84, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.373 [0.000, 5.000],  loss: 0.010997, mae: 0.749925, mean_q: 0.920541, mean_eps: 0.702302\n",
      "  332382/2000000: episode: 492, duration: 16.024s, episode steps: 1341, steps per second:  84, episode reward: 12.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.009413, mae: 0.741047, mean_q: 0.908598, mean_eps: 0.701459\n",
      "  333007/2000000: episode: 493, duration: 7.488s, episode steps: 625, steps per second:  83, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.009231, mae: 0.742933, mean_q: 0.909938, mean_eps: 0.700575\n",
      "  333389/2000000: episode: 494, duration: 4.568s, episode steps: 382, steps per second:  84, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.010601, mae: 0.751265, mean_q: 0.918056, mean_eps: 0.700122\n",
      "  334117/2000000: episode: 495, duration: 8.926s, episode steps: 728, steps per second:  82, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.008274, mae: 0.745358, mean_q: 0.912963, mean_eps: 0.699621\n",
      "  334850/2000000: episode: 496, duration: 8.803s, episode steps: 733, steps per second:  83, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.010337, mae: 0.747244, mean_q: 0.915657, mean_eps: 0.698964\n",
      "  335434/2000000: episode: 497, duration: 6.903s, episode steps: 584, steps per second:  85, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.009031, mae: 0.743845, mean_q: 0.911507, mean_eps: 0.698372\n",
      "  335972/2000000: episode: 498, duration: 6.296s, episode steps: 538, steps per second:  85, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.010264, mae: 0.750243, mean_q: 0.917346, mean_eps: 0.697868\n",
      "  336355/2000000: episode: 499, duration: 4.594s, episode steps: 383, steps per second:  83, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.009100, mae: 0.748298, mean_q: 0.914166, mean_eps: 0.697454\n",
      "  336754/2000000: episode: 500, duration: 4.933s, episode steps: 399, steps per second:  81, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.011010, mae: 0.755624, mean_q: 0.921900, mean_eps: 0.697101\n",
      "  337390/2000000: episode: 501, duration: 7.641s, episode steps: 636, steps per second:  83, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.011415, mae: 0.747555, mean_q: 0.914911, mean_eps: 0.696635\n",
      "  337960/2000000: episode: 502, duration: 7.117s, episode steps: 570, steps per second:  80, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.009467, mae: 0.745837, mean_q: 0.916343, mean_eps: 0.696093\n",
      "  338861/2000000: episode: 503, duration: 10.596s, episode steps: 901, steps per second:  85, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.010232, mae: 0.747714, mean_q: 0.914925, mean_eps: 0.695431\n",
      "  339503/2000000: episode: 504, duration: 7.617s, episode steps: 642, steps per second:  84, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.009471, mae: 0.739938, mean_q: 0.907697, mean_eps: 0.694736\n",
      "  340361/2000000: episode: 505, duration: 9.921s, episode steps: 858, steps per second:  86, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.516 [0.000, 5.000],  loss: 0.009926, mae: 0.771766, mean_q: 0.947209, mean_eps: 0.694061\n",
      "  340861/2000000: episode: 506, duration: 6.165s, episode steps: 500, steps per second:  81, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.009850, mae: 0.785592, mean_q: 0.962248, mean_eps: 0.693449\n",
      "  341476/2000000: episode: 507, duration: 7.379s, episode steps: 615, steps per second:  83, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.010870, mae: 0.782739, mean_q: 0.957435, mean_eps: 0.692949\n",
      "  342127/2000000: episode: 508, duration: 7.867s, episode steps: 651, steps per second:  83, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.010526, mae: 0.779183, mean_q: 0.956003, mean_eps: 0.692380\n",
      "  342740/2000000: episode: 509, duration: 7.165s, episode steps: 613, steps per second:  86, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.009959, mae: 0.792934, mean_q: 0.970805, mean_eps: 0.691811\n",
      "  343421/2000000: episode: 510, duration: 8.308s, episode steps: 681, steps per second:  82, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.009547, mae: 0.784032, mean_q: 0.959452, mean_eps: 0.691228\n",
      "  343869/2000000: episode: 511, duration: 5.528s, episode steps: 448, steps per second:  81, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.009519, mae: 0.776699, mean_q: 0.951047, mean_eps: 0.690719\n",
      "  344447/2000000: episode: 512, duration: 7.046s, episode steps: 578, steps per second:  82, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.009431, mae: 0.771062, mean_q: 0.946421, mean_eps: 0.690258\n",
      "  345234/2000000: episode: 513, duration: 9.561s, episode steps: 787, steps per second:  82, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.009690, mae: 0.795025, mean_q: 0.975565, mean_eps: 0.689644\n",
      "  345999/2000000: episode: 514, duration: 9.523s, episode steps: 765, steps per second:  80, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.010287, mae: 0.790034, mean_q: 0.967597, mean_eps: 0.688946\n",
      "  346797/2000000: episode: 515, duration: 9.540s, episode steps: 798, steps per second:  84, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.010598, mae: 0.782385, mean_q: 0.956950, mean_eps: 0.688242\n",
      "  347521/2000000: episode: 516, duration: 8.631s, episode steps: 724, steps per second:  84, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.009916, mae: 0.784653, mean_q: 0.960691, mean_eps: 0.687556\n",
      "  348208/2000000: episode: 517, duration: 7.954s, episode steps: 687, steps per second:  86, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.009361, mae: 0.785922, mean_q: 0.960574, mean_eps: 0.686922\n",
      "  348886/2000000: episode: 518, duration: 8.269s, episode steps: 678, steps per second:  82, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.009592, mae: 0.785398, mean_q: 0.963410, mean_eps: 0.686309\n",
      "  349530/2000000: episode: 519, duration: 7.400s, episode steps: 644, steps per second:  87, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.010934, mae: 0.782241, mean_q: 0.955241, mean_eps: 0.685713\n",
      "  350356/2000000: episode: 520, duration: 9.733s, episode steps: 826, steps per second:  85, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.009666, mae: 0.799087, mean_q: 0.979082, mean_eps: 0.685052\n",
      "  350710/2000000: episode: 521, duration: 4.169s, episode steps: 354, steps per second:  85, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.010862, mae: 0.824967, mean_q: 1.011072, mean_eps: 0.684521\n",
      "  351696/2000000: episode: 522, duration: 11.798s, episode steps: 986, steps per second:  84, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.009803, mae: 0.810808, mean_q: 0.991234, mean_eps: 0.683918\n",
      "  352257/2000000: episode: 523, duration: 6.469s, episode steps: 561, steps per second:  87, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.010102, mae: 0.821049, mean_q: 1.002854, mean_eps: 0.683222\n",
      "  352725/2000000: episode: 524, duration: 5.664s, episode steps: 468, steps per second:  83, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.010082, mae: 0.814195, mean_q: 0.995369, mean_eps: 0.682757\n",
      "  353629/2000000: episode: 525, duration: 10.757s, episode steps: 904, steps per second:  84, episode reward: 11.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.009934, mae: 0.827381, mean_q: 1.010862, mean_eps: 0.682140\n",
      "  354874/2000000: episode: 526, duration: 15.212s, episode steps: 1245, steps per second:  82, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.010273, mae: 0.819539, mean_q: 1.001425, mean_eps: 0.681173\n",
      "  355746/2000000: episode: 527, duration: 10.758s, episode steps: 872, steps per second:  81, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.008888, mae: 0.814975, mean_q: 0.996483, mean_eps: 0.680221\n",
      "  356371/2000000: episode: 528, duration: 7.620s, episode steps: 625, steps per second:  82, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.010644, mae: 0.827322, mean_q: 1.010303, mean_eps: 0.679548\n",
      "  357277/2000000: episode: 529, duration: 11.094s, episode steps: 906, steps per second:  82, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.009894, mae: 0.820449, mean_q: 1.002911, mean_eps: 0.678858\n",
      "  357886/2000000: episode: 530, duration: 7.449s, episode steps: 609, steps per second:  82, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.009749, mae: 0.820348, mean_q: 1.003999, mean_eps: 0.678176\n",
      "  358759/2000000: episode: 531, duration: 10.613s, episode steps: 873, steps per second:  82, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.010186, mae: 0.819835, mean_q: 1.001945, mean_eps: 0.677510\n",
      "  359786/2000000: episode: 532, duration: 12.671s, episode steps: 1027, steps per second:  81, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.009768, mae: 0.822284, mean_q: 1.004878, mean_eps: 0.676655\n",
      "  360428/2000000: episode: 533, duration: 7.925s, episode steps: 642, steps per second:  81, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.009993, mae: 0.832727, mean_q: 1.018747, mean_eps: 0.675905\n",
      "  361133/2000000: episode: 534, duration: 8.704s, episode steps: 705, steps per second:  81, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.009495, mae: 0.852499, mean_q: 1.043309, mean_eps: 0.675298\n",
      "  361626/2000000: episode: 535, duration: 6.078s, episode steps: 493, steps per second:  81, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.677 [0.000, 5.000],  loss: 0.010332, mae: 0.864331, mean_q: 1.055284, mean_eps: 0.674758\n",
      "  362761/2000000: episode: 536, duration: 13.984s, episode steps: 1135, steps per second:  81, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.011183, mae: 0.850104, mean_q: 1.039761, mean_eps: 0.674025\n",
      "  363561/2000000: episode: 537, duration: 9.980s, episode steps: 800, steps per second:  80, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.009350, mae: 0.845531, mean_q: 1.032894, mean_eps: 0.673154\n",
      "  364020/2000000: episode: 538, duration: 5.716s, episode steps: 459, steps per second:  80, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.010786, mae: 0.859049, mean_q: 1.049905, mean_eps: 0.672589\n",
      "  364801/2000000: episode: 539, duration: 9.740s, episode steps: 781, steps per second:  80, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.009406, mae: 0.849315, mean_q: 1.037412, mean_eps: 0.672031\n",
      "  365690/2000000: episode: 540, duration: 10.845s, episode steps: 889, steps per second:  82, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.009533, mae: 0.852977, mean_q: 1.042719, mean_eps: 0.671279\n",
      "  366635/2000000: episode: 541, duration: 11.416s, episode steps: 945, steps per second:  83, episode reward: 10.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.009353, mae: 0.847365, mean_q: 1.036407, mean_eps: 0.670454\n",
      "  367119/2000000: episode: 542, duration: 5.849s, episode steps: 484, steps per second:  83, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.010371, mae: 0.853384, mean_q: 1.040734, mean_eps: 0.669812\n",
      "  367762/2000000: episode: 543, duration: 7.648s, episode steps: 643, steps per second:  84, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.011582, mae: 0.859900, mean_q: 1.050736, mean_eps: 0.669304\n",
      "  368748/2000000: episode: 544, duration: 11.773s, episode steps: 986, steps per second:  84, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.010599, mae: 0.853622, mean_q: 1.042033, mean_eps: 0.668571\n",
      "  369364/2000000: episode: 545, duration: 7.306s, episode steps: 616, steps per second:  84, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.010973, mae: 0.858720, mean_q: 1.049332, mean_eps: 0.667851\n",
      "  370170/2000000: episode: 546, duration: 9.923s, episode steps: 806, steps per second:  81, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.008807, mae: 0.860905, mean_q: 1.052888, mean_eps: 0.667211\n",
      "  370720/2000000: episode: 547, duration: 6.784s, episode steps: 550, steps per second:  81, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.010224, mae: 0.881476, mean_q: 1.076259, mean_eps: 0.666600\n",
      "  371084/2000000: episode: 548, duration: 4.838s, episode steps: 364, steps per second:  75, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.810 [0.000, 5.000],  loss: 0.008325, mae: 0.863907, mean_q: 1.057094, mean_eps: 0.666190\n",
      "  372049/2000000: episode: 549, duration: 11.954s, episode steps: 965, steps per second:  81, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.010011, mae: 0.871978, mean_q: 1.064101, mean_eps: 0.665591\n",
      "  372617/2000000: episode: 550, duration: 7.189s, episode steps: 568, steps per second:  79, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.009816, mae: 0.871069, mean_q: 1.065496, mean_eps: 0.664899\n",
      "  373263/2000000: episode: 551, duration: 8.137s, episode steps: 646, steps per second:  79, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.012054, mae: 0.878340, mean_q: 1.072864, mean_eps: 0.664354\n",
      "  374043/2000000: episode: 552, duration: 9.937s, episode steps: 780, steps per second:  78, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.010387, mae: 0.869786, mean_q: 1.062476, mean_eps: 0.663713\n",
      "  374771/2000000: episode: 553, duration: 8.794s, episode steps: 728, steps per second:  83, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.010762, mae: 0.869757, mean_q: 1.062527, mean_eps: 0.663035\n",
      "  375423/2000000: episode: 554, duration: 8.114s, episode steps: 652, steps per second:  80, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.008947, mae: 0.859110, mean_q: 1.050000, mean_eps: 0.662414\n",
      "  376277/2000000: episode: 555, duration: 10.453s, episode steps: 854, steps per second:  82, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.010229, mae: 0.879567, mean_q: 1.075045, mean_eps: 0.661735\n",
      "  376834/2000000: episode: 556, duration: 6.639s, episode steps: 557, steps per second:  84, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.011718, mae: 0.886468, mean_q: 1.083000, mean_eps: 0.661100\n",
      "  377564/2000000: episode: 557, duration: 9.062s, episode steps: 730, steps per second:  81, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.009433, mae: 0.880510, mean_q: 1.073810, mean_eps: 0.660522\n",
      "  378073/2000000: episode: 558, duration: 6.120s, episode steps: 509, steps per second:  83, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.009140, mae: 0.869120, mean_q: 1.060105, mean_eps: 0.659964\n",
      "  378922/2000000: episode: 559, duration: 10.330s, episode steps: 849, steps per second:  82, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.009602, mae: 0.869055, mean_q: 1.061050, mean_eps: 0.659352\n",
      "  379876/2000000: episode: 560, duration: 11.489s, episode steps: 954, steps per second:  83, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.635 [0.000, 5.000],  loss: 0.010164, mae: 0.879104, mean_q: 1.073945, mean_eps: 0.658542\n",
      "  380257/2000000: episode: 561, duration: 4.957s, episode steps: 381, steps per second:  77, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.009991, mae: 0.882985, mean_q: 1.079224, mean_eps: 0.657941\n",
      "  380662/2000000: episode: 562, duration: 5.062s, episode steps: 405, steps per second:  80, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.010447, mae: 0.891752, mean_q: 1.089864, mean_eps: 0.657586\n",
      "  381701/2000000: episode: 563, duration: 13.115s, episode steps: 1039, steps per second:  79, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.009745, mae: 0.887211, mean_q: 1.085991, mean_eps: 0.656936\n",
      "  382071/2000000: episode: 564, duration: 4.510s, episode steps: 370, steps per second:  82, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.630 [0.000, 5.000],  loss: 0.011524, mae: 0.896649, mean_q: 1.096210, mean_eps: 0.656303\n",
      "  383401/2000000: episode: 565, duration: 17.065s, episode steps: 1330, steps per second:  78, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.214 [0.000, 5.000],  loss: 0.011076, mae: 0.894652, mean_q: 1.092544, mean_eps: 0.655538\n",
      "  384099/2000000: episode: 566, duration: 9.009s, episode steps: 698, steps per second:  77, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.009412, mae: 0.888014, mean_q: 1.088177, mean_eps: 0.654625\n",
      "  385059/2000000: episode: 567, duration: 12.161s, episode steps: 960, steps per second:  79, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.009459, mae: 0.894655, mean_q: 1.095906, mean_eps: 0.653880\n",
      "  385722/2000000: episode: 568, duration: 8.457s, episode steps: 663, steps per second:  78, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.013526, mae: 0.903186, mean_q: 1.102334, mean_eps: 0.653149\n",
      "  386459/2000000: episode: 569, duration: 9.008s, episode steps: 737, steps per second:  82, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.009860, mae: 0.894630, mean_q: 1.094257, mean_eps: 0.652519\n",
      "  387106/2000000: episode: 570, duration: 7.658s, episode steps: 647, steps per second:  84, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.009994, mae: 0.887556, mean_q: 1.083461, mean_eps: 0.651896\n",
      "  387943/2000000: episode: 571, duration: 10.518s, episode steps: 837, steps per second:  80, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.010362, mae: 0.900735, mean_q: 1.099983, mean_eps: 0.651228\n",
      "  388320/2000000: episode: 572, duration: 4.809s, episode steps: 377, steps per second:  78, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.010689, mae: 0.885309, mean_q: 1.081245, mean_eps: 0.650683\n",
      "  388977/2000000: episode: 573, duration: 8.452s, episode steps: 657, steps per second:  78, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.009854, mae: 0.895697, mean_q: 1.095351, mean_eps: 0.650217\n",
      "  389616/2000000: episode: 574, duration: 7.958s, episode steps: 639, steps per second:  80, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.010540, mae: 0.886399, mean_q: 1.081758, mean_eps: 0.649634\n",
      "  390170/2000000: episode: 575, duration: 7.234s, episode steps: 554, steps per second:  77, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.009978, mae: 0.900112, mean_q: 1.098709, mean_eps: 0.649097\n",
      "  390526/2000000: episode: 576, duration: 4.444s, episode steps: 356, steps per second:  80, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.010744, mae: 0.918826, mean_q: 1.124086, mean_eps: 0.648687\n",
      "  391359/2000000: episode: 577, duration: 10.487s, episode steps: 833, steps per second:  79, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.009860, mae: 0.918100, mean_q: 1.122275, mean_eps: 0.648152\n",
      "  391948/2000000: episode: 578, duration: 7.442s, episode steps: 589, steps per second:  79, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.010349, mae: 0.922661, mean_q: 1.127057, mean_eps: 0.647513\n",
      "  392731/2000000: episode: 579, duration: 10.098s, episode steps: 783, steps per second:  78, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.010341, mae: 0.916493, mean_q: 1.118453, mean_eps: 0.646896\n",
      "  393671/2000000: episode: 580, duration: 11.798s, episode steps: 940, steps per second:  80, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.009800, mae: 0.920077, mean_q: 1.122897, mean_eps: 0.646120\n",
      "  394417/2000000: episode: 581, duration: 9.644s, episode steps: 746, steps per second:  77, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.010170, mae: 0.918410, mean_q: 1.122168, mean_eps: 0.645360\n",
      "  394831/2000000: episode: 582, duration: 5.323s, episode steps: 414, steps per second:  78, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.009477, mae: 0.916612, mean_q: 1.119243, mean_eps: 0.644838\n",
      "  395477/2000000: episode: 583, duration: 8.367s, episode steps: 646, steps per second:  77, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.010585, mae: 0.917473, mean_q: 1.119288, mean_eps: 0.644361\n",
      "  396507/2000000: episode: 584, duration: 12.460s, episode steps: 1030, steps per second:  83, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.010479, mae: 0.924910, mean_q: 1.127251, mean_eps: 0.643607\n",
      "  397035/2000000: episode: 585, duration: 6.389s, episode steps: 528, steps per second:  83, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.447 [0.000, 5.000],  loss: 0.011493, mae: 0.922467, mean_q: 1.124838, mean_eps: 0.642907\n",
      "  397427/2000000: episode: 586, duration: 4.778s, episode steps: 392, steps per second:  82, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.010738, mae: 0.908182, mean_q: 1.110516, mean_eps: 0.642493\n",
      "  398072/2000000: episode: 587, duration: 7.982s, episode steps: 645, steps per second:  81, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.009474, mae: 0.931379, mean_q: 1.137707, mean_eps: 0.642027\n",
      "  398969/2000000: episode: 588, duration: 10.881s, episode steps: 897, steps per second:  82, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.010596, mae: 0.931224, mean_q: 1.136937, mean_eps: 0.641332\n",
      "  399602/2000000: episode: 589, duration: 7.978s, episode steps: 633, steps per second:  79, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.011967, mae: 0.932544, mean_q: 1.136852, mean_eps: 0.640643\n",
      "  400136/2000000: episode: 590, duration: 6.714s, episode steps: 534, steps per second:  80, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.624 [0.000, 5.000],  loss: 0.010592, mae: 0.931400, mean_q: 1.136199, mean_eps: 0.640119\n",
      "  400677/2000000: episode: 591, duration: 7.052s, episode steps: 541, steps per second:  77, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.009618, mae: 0.951676, mean_q: 1.162919, mean_eps: 0.639635\n",
      "  401287/2000000: episode: 592, duration: 7.568s, episode steps: 610, steps per second:  81, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.008994, mae: 0.937401, mean_q: 1.145884, mean_eps: 0.639116\n",
      "  402102/2000000: episode: 593, duration: 10.526s, episode steps: 815, steps per second:  77, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.010397, mae: 0.953527, mean_q: 1.163210, mean_eps: 0.638475\n",
      "  402946/2000000: episode: 594, duration: 10.392s, episode steps: 844, steps per second:  81, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.010883, mae: 0.958575, mean_q: 1.171134, mean_eps: 0.637728\n",
      "  404268/2000000: episode: 595, duration: 16.293s, episode steps: 1322, steps per second:  81, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.011464, mae: 0.958817, mean_q: 1.168979, mean_eps: 0.636755\n",
      "  404903/2000000: episode: 596, duration: 8.124s, episode steps: 635, steps per second:  78, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.011015, mae: 0.965097, mean_q: 1.176664, mean_eps: 0.635874\n",
      "  405293/2000000: episode: 597, duration: 4.843s, episode steps: 390, steps per second:  81, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.011536, mae: 0.959901, mean_q: 1.170362, mean_eps: 0.635412\n",
      "  405789/2000000: episode: 598, duration: 6.466s, episode steps: 496, steps per second:  77, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.011401, mae: 0.954278, mean_q: 1.164217, mean_eps: 0.635012\n",
      "  406310/2000000: episode: 599, duration: 6.208s, episode steps: 521, steps per second:  84, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.010786, mae: 0.962825, mean_q: 1.175184, mean_eps: 0.634555\n",
      "  406706/2000000: episode: 600, duration: 4.987s, episode steps: 396, steps per second:  79, episode reward:  9.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.010136, mae: 0.964490, mean_q: 1.178162, mean_eps: 0.634143\n",
      "  407573/2000000: episode: 601, duration: 10.634s, episode steps: 867, steps per second:  82, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.009760, mae: 0.948122, mean_q: 1.156470, mean_eps: 0.633574\n",
      "  408079/2000000: episode: 602, duration: 6.215s, episode steps: 506, steps per second:  81, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.672 [0.000, 5.000],  loss: 0.011715, mae: 0.967880, mean_q: 1.182834, mean_eps: 0.632957\n",
      "  408932/2000000: episode: 603, duration: 10.304s, episode steps: 853, steps per second:  83, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.009719, mae: 0.955824, mean_q: 1.166621, mean_eps: 0.632346\n",
      "  409386/2000000: episode: 604, duration: 5.776s, episode steps: 454, steps per second:  79, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.012114, mae: 0.947338, mean_q: 1.155691, mean_eps: 0.631758\n",
      "  410100/2000000: episode: 605, duration: 8.922s, episode steps: 714, steps per second:  80, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.009991, mae: 0.963530, mean_q: 1.175303, mean_eps: 0.631232\n",
      "  410749/2000000: episode: 606, duration: 8.264s, episode steps: 649, steps per second:  79, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.010907, mae: 0.986884, mean_q: 1.206131, mean_eps: 0.630618\n",
      "  411271/2000000: episode: 607, duration: 6.389s, episode steps: 522, steps per second:  82, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.205 [0.000, 5.000],  loss: 0.010393, mae: 0.982302, mean_q: 1.199842, mean_eps: 0.630091\n",
      "  411645/2000000: episode: 608, duration: 4.603s, episode steps: 374, steps per second:  81, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.011464, mae: 0.998635, mean_q: 1.219295, mean_eps: 0.629688\n",
      "  412151/2000000: episode: 609, duration: 6.224s, episode steps: 506, steps per second:  81, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.747 [0.000, 5.000],  loss: 0.013068, mae: 0.994106, mean_q: 1.211817, mean_eps: 0.629292\n",
      "  412745/2000000: episode: 610, duration: 7.413s, episode steps: 594, steps per second:  80, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.009247, mae: 0.985189, mean_q: 1.202509, mean_eps: 0.628797\n",
      "  413296/2000000: episode: 611, duration: 6.992s, episode steps: 551, steps per second:  79, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.730 [0.000, 5.000],  loss: 0.010451, mae: 0.997672, mean_q: 1.217448, mean_eps: 0.628282\n",
      "  413673/2000000: episode: 612, duration: 4.785s, episode steps: 377, steps per second:  79, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.010032, mae: 0.982082, mean_q: 1.198680, mean_eps: 0.627864\n",
      "  414314/2000000: episode: 613, duration: 8.248s, episode steps: 641, steps per second:  78, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.011007, mae: 0.980969, mean_q: 1.196616, mean_eps: 0.627405\n",
      "  414984/2000000: episode: 614, duration: 8.760s, episode steps: 670, steps per second:  76, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.011442, mae: 0.989206, mean_q: 1.205882, mean_eps: 0.626817\n",
      "  415432/2000000: episode: 615, duration: 5.529s, episode steps: 448, steps per second:  81, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.009806, mae: 0.973802, mean_q: 1.188630, mean_eps: 0.626315\n",
      "  416010/2000000: episode: 616, duration: 7.219s, episode steps: 578, steps per second:  80, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.011167, mae: 0.994503, mean_q: 1.214532, mean_eps: 0.625852\n",
      "  417114/2000000: episode: 617, duration: 13.760s, episode steps: 1104, steps per second:  80, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.009694, mae: 0.977730, mean_q: 1.192133, mean_eps: 0.625094\n",
      "  417680/2000000: episode: 618, duration: 6.879s, episode steps: 566, steps per second:  82, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.012114, mae: 0.998950, mean_q: 1.216291, mean_eps: 0.624344\n",
      "  418501/2000000: episode: 619, duration: 10.168s, episode steps: 821, steps per second:  81, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.009950, mae: 0.990486, mean_q: 1.209927, mean_eps: 0.623719\n",
      "  418925/2000000: episode: 620, duration: 5.300s, episode steps: 424, steps per second:  80, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.844 [0.000, 5.000],  loss: 0.009727, mae: 0.983467, mean_q: 1.204811, mean_eps: 0.623157\n",
      "  419443/2000000: episode: 621, duration: 6.354s, episode steps: 518, steps per second:  82, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.010522, mae: 0.982241, mean_q: 1.201472, mean_eps: 0.622734\n",
      "  420068/2000000: episode: 622, duration: 7.841s, episode steps: 625, steps per second:  80, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.518 [0.000, 5.000],  loss: 0.009204, mae: 0.986802, mean_q: 1.205117, mean_eps: 0.622221\n",
      "  420879/2000000: episode: 623, duration: 10.160s, episode steps: 811, steps per second:  80, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.010472, mae: 1.012409, mean_q: 1.236227, mean_eps: 0.621575\n",
      "  421672/2000000: episode: 624, duration: 9.902s, episode steps: 793, steps per second:  80, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.010835, mae: 1.015033, mean_q: 1.237969, mean_eps: 0.620853\n",
      "  422277/2000000: episode: 625, duration: 7.888s, episode steps: 605, steps per second:  77, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.010923, mae: 1.023484, mean_q: 1.246544, mean_eps: 0.620223\n",
      "  422939/2000000: episode: 626, duration: 8.347s, episode steps: 662, steps per second:  79, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.009967, mae: 1.018841, mean_q: 1.242817, mean_eps: 0.619653\n",
      "  423333/2000000: episode: 627, duration: 5.143s, episode steps: 394, steps per second:  77, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.009630, mae: 1.015022, mean_q: 1.238231, mean_eps: 0.619178\n",
      "  423835/2000000: episode: 628, duration: 6.625s, episode steps: 502, steps per second:  76, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.013587, mae: 1.028822, mean_q: 1.256130, mean_eps: 0.618774\n",
      "  424609/2000000: episode: 629, duration: 9.724s, episode steps: 774, steps per second:  80, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.011241, mae: 1.008399, mean_q: 1.229749, mean_eps: 0.618200\n",
      "  425192/2000000: episode: 630, duration: 7.142s, episode steps: 583, steps per second:  82, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.009136, mae: 1.029070, mean_q: 1.254534, mean_eps: 0.617590\n",
      "  425862/2000000: episode: 631, duration: 8.535s, episode steps: 670, steps per second:  79, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.009703, mae: 1.006170, mean_q: 1.225992, mean_eps: 0.617027\n",
      "  426294/2000000: episode: 632, duration: 5.392s, episode steps: 432, steps per second:  80, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.896 [0.000, 5.000],  loss: 0.010343, mae: 1.022984, mean_q: 1.246874, mean_eps: 0.616530\n",
      "  426965/2000000: episode: 633, duration: 8.189s, episode steps: 671, steps per second:  82, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.420 [0.000, 5.000],  loss: 0.011918, mae: 1.021555, mean_q: 1.246177, mean_eps: 0.616033\n",
      "  427564/2000000: episode: 634, duration: 7.468s, episode steps: 599, steps per second:  80, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.093 [0.000, 5.000],  loss: 0.009912, mae: 1.011249, mean_q: 1.234283, mean_eps: 0.615462\n",
      "  428207/2000000: episode: 635, duration: 8.044s, episode steps: 643, steps per second:  80, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.009631, mae: 1.004757, mean_q: 1.225285, mean_eps: 0.614904\n",
      "  428969/2000000: episode: 636, duration: 10.040s, episode steps: 762, steps per second:  76, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.011542, mae: 1.015796, mean_q: 1.238370, mean_eps: 0.614271\n",
      "  430161/2000000: episode: 637, duration: 16.348s, episode steps: 1192, steps per second:  73, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.010665, mae: 1.026348, mean_q: 1.253161, mean_eps: 0.613391\n",
      "  430843/2000000: episode: 638, duration: 8.717s, episode steps: 682, steps per second:  78, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.011003, mae: 1.063241, mean_q: 1.297566, mean_eps: 0.612548\n",
      "  431201/2000000: episode: 639, duration: 4.468s, episode steps: 358, steps per second:  80, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.010603, mae: 1.067712, mean_q: 1.302962, mean_eps: 0.612080\n",
      "  431709/2000000: episode: 640, duration: 6.389s, episode steps: 508, steps per second:  80, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.965 [0.000, 5.000],  loss: 0.010019, mae: 1.056523, mean_q: 1.289650, mean_eps: 0.611690\n",
      "  432409/2000000: episode: 641, duration: 9.082s, episode steps: 700, steps per second:  77, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.012326, mae: 1.068215, mean_q: 1.304336, mean_eps: 0.611146\n",
      "  433479/2000000: episode: 642, duration: 14.016s, episode steps: 1070, steps per second:  76, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.010958, mae: 1.059138, mean_q: 1.291937, mean_eps: 0.610350\n",
      "  434473/2000000: episode: 643, duration: 12.636s, episode steps: 994, steps per second:  79, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.306 [0.000, 5.000],  loss: 0.012352, mae: 1.075903, mean_q: 1.311611, mean_eps: 0.609422\n",
      "  434859/2000000: episode: 644, duration: 4.851s, episode steps: 386, steps per second:  80, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.010048, mae: 1.047815, mean_q: 1.280105, mean_eps: 0.608801\n",
      "  435834/2000000: episode: 645, duration: 12.368s, episode steps: 975, steps per second:  79, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.011339, mae: 1.062102, mean_q: 1.296125, mean_eps: 0.608189\n",
      "  436216/2000000: episode: 646, duration: 4.837s, episode steps: 382, steps per second:  79, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.010831, mae: 1.073300, mean_q: 1.310136, mean_eps: 0.607578\n",
      "  436924/2000000: episode: 647, duration: 8.702s, episode steps: 708, steps per second:  81, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.011137, mae: 1.055859, mean_q: 1.287052, mean_eps: 0.607089\n",
      "  437662/2000000: episode: 648, duration: 9.410s, episode steps: 738, steps per second:  78, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.011652, mae: 1.058179, mean_q: 1.289884, mean_eps: 0.606437\n",
      "  438469/2000000: episode: 649, duration: 10.407s, episode steps: 807, steps per second:  78, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.010972, mae: 1.062857, mean_q: 1.295809, mean_eps: 0.605741\n",
      "  439079/2000000: episode: 650, duration: 7.637s, episode steps: 610, steps per second:  80, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.010096, mae: 1.064502, mean_q: 1.297127, mean_eps: 0.605103\n",
      "  439534/2000000: episode: 651, duration: 5.681s, episode steps: 455, steps per second:  80, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.011575, mae: 1.059577, mean_q: 1.293092, mean_eps: 0.604625\n",
      "  440551/2000000: episode: 652, duration: 12.795s, episode steps: 1017, steps per second:  79, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.010722, mae: 1.078931, mean_q: 1.315386, mean_eps: 0.603962\n",
      "  441086/2000000: episode: 653, duration: 7.052s, episode steps: 535, steps per second:  76, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.009089, mae: 1.103368, mean_q: 1.345121, mean_eps: 0.603264\n",
      "  441588/2000000: episode: 654, duration: 6.411s, episode steps: 502, steps per second:  78, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.012010, mae: 1.096736, mean_q: 1.335812, mean_eps: 0.602798\n",
      "  442228/2000000: episode: 655, duration: 8.335s, episode steps: 640, steps per second:  77, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.012178, mae: 1.111352, mean_q: 1.356118, mean_eps: 0.602285\n",
      "  442606/2000000: episode: 656, duration: 4.885s, episode steps: 378, steps per second:  77, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.009993, mae: 1.095905, mean_q: 1.336504, mean_eps: 0.601826\n",
      "  443231/2000000: episode: 657, duration: 8.034s, episode steps: 625, steps per second:  78, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.011452, mae: 1.096122, mean_q: 1.336234, mean_eps: 0.601374\n",
      "  443897/2000000: episode: 658, duration: 8.578s, episode steps: 666, steps per second:  78, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.012036, mae: 1.102729, mean_q: 1.344963, mean_eps: 0.600792\n",
      "  444532/2000000: episode: 659, duration: 7.915s, episode steps: 635, steps per second:  80, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.591 [0.000, 5.000],  loss: 0.011899, mae: 1.090956, mean_q: 1.329695, mean_eps: 0.600207\n",
      "  445296/2000000: episode: 660, duration: 9.732s, episode steps: 764, steps per second:  79, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.012453, mae: 1.102375, mean_q: 1.344985, mean_eps: 0.599579\n",
      "  445835/2000000: episode: 661, duration: 6.774s, episode steps: 539, steps per second:  80, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.010282, mae: 1.097309, mean_q: 1.337764, mean_eps: 0.598992\n",
      "  446791/2000000: episode: 662, duration: 12.118s, episode steps: 956, steps per second:  79, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.567 [0.000, 5.000],  loss: 0.011075, mae: 1.097156, mean_q: 1.336737, mean_eps: 0.598319\n",
      "  447441/2000000: episode: 663, duration: 8.541s, episode steps: 650, steps per second:  76, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.011093, mae: 1.092662, mean_q: 1.331735, mean_eps: 0.597596\n",
      "  447791/2000000: episode: 664, duration: 4.608s, episode steps: 350, steps per second:  76, episode reward:  3.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.011575, mae: 1.105970, mean_q: 1.347168, mean_eps: 0.597146\n",
      "  448276/2000000: episode: 665, duration: 6.043s, episode steps: 485, steps per second:  80, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.011447, mae: 1.103608, mean_q: 1.342170, mean_eps: 0.596771\n",
      "  449221/2000000: episode: 666, duration: 11.786s, episode steps: 945, steps per second:  80, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.010493, mae: 1.105379, mean_q: 1.345689, mean_eps: 0.596127\n",
      "  450441/2000000: episode: 667, duration: 15.858s, episode steps: 1220, steps per second:  77, episode reward: 14.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.011239, mae: 1.101475, mean_q: 1.342674, mean_eps: 0.595151\n",
      "  451000/2000000: episode: 668, duration: 7.303s, episode steps: 559, steps per second:  77, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.009671, mae: 1.086549, mean_q: 1.322528, mean_eps: 0.594352\n",
      "  452030/2000000: episode: 669, duration: 13.535s, episode steps: 1030, steps per second:  76, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.010134, mae: 1.096759, mean_q: 1.337493, mean_eps: 0.593637\n",
      "  453073/2000000: episode: 670, duration: 13.356s, episode steps: 1043, steps per second:  78, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.011517, mae: 1.084199, mean_q: 1.319860, mean_eps: 0.592703\n",
      "  453934/2000000: episode: 671, duration: 10.851s, episode steps: 861, steps per second:  79, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.829 [0.000, 5.000],  loss: 0.010444, mae: 1.091767, mean_q: 1.331066, mean_eps: 0.591846\n",
      "  454668/2000000: episode: 672, duration: 9.366s, episode steps: 734, steps per second:  78, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.010994, mae: 1.088371, mean_q: 1.324091, mean_eps: 0.591130\n",
      "  455217/2000000: episode: 673, duration: 6.940s, episode steps: 549, steps per second:  79, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.856 [0.000, 5.000],  loss: 0.011457, mae: 1.090998, mean_q: 1.325924, mean_eps: 0.590552\n",
      "  455579/2000000: episode: 674, duration: 4.436s, episode steps: 362, steps per second:  82, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.010926, mae: 1.082345, mean_q: 1.316475, mean_eps: 0.590142\n",
      "  456113/2000000: episode: 675, duration: 6.838s, episode steps: 534, steps per second:  78, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.011264, mae: 1.091711, mean_q: 1.329488, mean_eps: 0.589739\n",
      "  456987/2000000: episode: 676, duration: 11.325s, episode steps: 874, steps per second:  77, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.010335, mae: 1.086917, mean_q: 1.323014, mean_eps: 0.589105\n",
      "  457518/2000000: episode: 677, duration: 6.941s, episode steps: 531, steps per second:  77, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.009658, mae: 1.087645, mean_q: 1.322558, mean_eps: 0.588473\n",
      "  458232/2000000: episode: 678, duration: 8.916s, episode steps: 714, steps per second:  80, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.011067, mae: 1.093062, mean_q: 1.330111, mean_eps: 0.587913\n",
      "  459425/2000000: episode: 679, duration: 15.476s, episode steps: 1193, steps per second:  77, episode reward: 14.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.010121, mae: 1.093729, mean_q: 1.331329, mean_eps: 0.587055\n",
      "  460080/2000000: episode: 680, duration: 8.512s, episode steps: 655, steps per second:  77, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.834 [0.000, 5.000],  loss: 0.011275, mae: 1.091712, mean_q: 1.332033, mean_eps: 0.586223\n",
      "  460583/2000000: episode: 681, duration: 6.388s, episode steps: 503, steps per second:  79, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.072 [0.000, 5.000],  loss: 0.011734, mae: 1.129405, mean_q: 1.376866, mean_eps: 0.585703\n",
      "  461063/2000000: episode: 682, duration: 6.474s, episode steps: 480, steps per second:  74, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.010842, mae: 1.102220, mean_q: 1.342161, mean_eps: 0.585260\n",
      "  461823/2000000: episode: 683, duration: 9.946s, episode steps: 760, steps per second:  76, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.010211, mae: 1.119694, mean_q: 1.362042, mean_eps: 0.584702\n",
      "  462461/2000000: episode: 684, duration: 8.402s, episode steps: 638, steps per second:  76, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.009561, mae: 1.122772, mean_q: 1.365708, mean_eps: 0.584072\n",
      "  463005/2000000: episode: 685, duration: 6.691s, episode steps: 544, steps per second:  81, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.009598, mae: 1.105981, mean_q: 1.344461, mean_eps: 0.583539\n",
      "  463868/2000000: episode: 686, duration: 11.020s, episode steps: 863, steps per second:  78, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.010402, mae: 1.115705, mean_q: 1.356381, mean_eps: 0.582908\n",
      "  464716/2000000: episode: 687, duration: 10.854s, episode steps: 848, steps per second:  78, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.011459, mae: 1.132818, mean_q: 1.378584, mean_eps: 0.582139\n",
      "  465287/2000000: episode: 688, duration: 7.151s, episode steps: 571, steps per second:  80, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.011928, mae: 1.117149, mean_q: 1.358899, mean_eps: 0.581500\n",
      "  466121/2000000: episode: 689, duration: 10.838s, episode steps: 834, steps per second:  77, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.010134, mae: 1.108765, mean_q: 1.350242, mean_eps: 0.580866\n",
      "  466596/2000000: episode: 690, duration: 6.202s, episode steps: 475, steps per second:  77, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.682 [0.000, 5.000],  loss: 0.009731, mae: 1.114442, mean_q: 1.355462, mean_eps: 0.580278\n",
      "  467505/2000000: episode: 691, duration: 11.777s, episode steps: 909, steps per second:  77, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.010860, mae: 1.107452, mean_q: 1.346498, mean_eps: 0.579655\n",
      "  467955/2000000: episode: 692, duration: 5.622s, episode steps: 450, steps per second:  80, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.011744, mae: 1.119028, mean_q: 1.360722, mean_eps: 0.579043\n",
      "  468607/2000000: episode: 693, duration: 8.401s, episode steps: 652, steps per second:  78, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.009554, mae: 1.116167, mean_q: 1.359019, mean_eps: 0.578548\n",
      "  469395/2000000: episode: 694, duration: 10.224s, episode steps: 788, steps per second:  77, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.010037, mae: 1.108086, mean_q: 1.348968, mean_eps: 0.577900\n",
      "  470019/2000000: episode: 695, duration: 8.351s, episode steps: 624, steps per second:  75, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.009336, mae: 1.112425, mean_q: 1.353462, mean_eps: 0.577265\n",
      "  470731/2000000: episode: 696, duration: 9.538s, episode steps: 712, steps per second:  75, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.009392, mae: 1.142394, mean_q: 1.393084, mean_eps: 0.576663\n",
      "  471575/2000000: episode: 697, duration: 11.130s, episode steps: 844, steps per second:  76, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.009680, mae: 1.124179, mean_q: 1.368807, mean_eps: 0.575963\n",
      "  472858/2000000: episode: 698, duration: 16.595s, episode steps: 1283, steps per second:  77, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.011365, mae: 1.134124, mean_q: 1.378217, mean_eps: 0.575006\n",
      "  473845/2000000: episode: 699, duration: 12.588s, episode steps: 987, steps per second:  78, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.011855, mae: 1.133656, mean_q: 1.379391, mean_eps: 0.573983\n",
      "  474372/2000000: episode: 700, duration: 6.738s, episode steps: 527, steps per second:  78, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.009832, mae: 1.132640, mean_q: 1.377746, mean_eps: 0.573303\n",
      "  475471/2000000: episode: 701, duration: 14.269s, episode steps: 1099, steps per second:  77, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.011062, mae: 1.130844, mean_q: 1.376374, mean_eps: 0.572572\n",
      "  476074/2000000: episode: 702, duration: 8.014s, episode steps: 603, steps per second:  75, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.222 [0.000, 5.000],  loss: 0.010561, mae: 1.125855, mean_q: 1.369723, mean_eps: 0.571805\n",
      "  476891/2000000: episode: 703, duration: 10.508s, episode steps: 817, steps per second:  78, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.010167, mae: 1.136233, mean_q: 1.381273, mean_eps: 0.571166\n",
      "  477918/2000000: episode: 704, duration: 13.521s, episode steps: 1027, steps per second:  76, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.011354, mae: 1.136120, mean_q: 1.383621, mean_eps: 0.570336\n",
      "  478358/2000000: episode: 705, duration: 5.881s, episode steps: 440, steps per second:  75, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.010105, mae: 1.134305, mean_q: 1.383437, mean_eps: 0.569676\n",
      "  479416/2000000: episode: 706, duration: 14.197s, episode steps: 1058, steps per second:  75, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.011624, mae: 1.141087, mean_q: 1.388690, mean_eps: 0.569003\n",
      "  480441/2000000: episode: 707, duration: 13.853s, episode steps: 1025, steps per second:  74, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.010915, mae: 1.146599, mean_q: 1.398284, mean_eps: 0.568065\n",
      "  480817/2000000: episode: 708, duration: 5.191s, episode steps: 376, steps per second:  72, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.011406, mae: 1.152694, mean_q: 1.404128, mean_eps: 0.567433\n",
      "  481529/2000000: episode: 709, duration: 9.745s, episode steps: 712, steps per second:  73, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.010146, mae: 1.143522, mean_q: 1.391349, mean_eps: 0.566943\n",
      "  482523/2000000: episode: 710, duration: 14.023s, episode steps: 994, steps per second:  71, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.623 [0.000, 5.000],  loss: 0.011350, mae: 1.181375, mean_q: 1.437726, mean_eps: 0.566177\n",
      "  483329/2000000: episode: 711, duration: 11.259s, episode steps: 806, steps per second:  72, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.010506, mae: 1.164680, mean_q: 1.416739, mean_eps: 0.565367\n",
      "  484008/2000000: episode: 712, duration: 9.565s, episode steps: 679, steps per second:  71, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.010263, mae: 1.169227, mean_q: 1.424155, mean_eps: 0.564699\n",
      "  484630/2000000: episode: 713, duration: 8.528s, episode steps: 622, steps per second:  73, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.010034, mae: 1.162282, mean_q: 1.414401, mean_eps: 0.564114\n",
      "  485201/2000000: episode: 714, duration: 8.087s, episode steps: 571, steps per second:  71, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.011515, mae: 1.173355, mean_q: 1.428390, mean_eps: 0.563576\n",
      "  486302/2000000: episode: 715, duration: 16.816s, episode steps: 1101, steps per second:  65, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.010306, mae: 1.162122, mean_q: 1.414246, mean_eps: 0.562823\n",
      "  486691/2000000: episode: 716, duration: 6.542s, episode steps: 389, steps per second:  59, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.009662, mae: 1.151832, mean_q: 1.401247, mean_eps: 0.562154\n",
      "  487802/2000000: episode: 717, duration: 16.979s, episode steps: 1111, steps per second:  65, episode reward: 13.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.011858, mae: 1.161372, mean_q: 1.411508, mean_eps: 0.561479\n",
      "  488426/2000000: episode: 718, duration: 9.938s, episode steps: 624, steps per second:  63, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.009658, mae: 1.158365, mean_q: 1.410446, mean_eps: 0.560697\n",
      "  489118/2000000: episode: 719, duration: 10.389s, episode steps: 692, steps per second:  67, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.010839, mae: 1.161345, mean_q: 1.413418, mean_eps: 0.560105\n",
      "  490199/2000000: episode: 720, duration: 15.585s, episode steps: 1081, steps per second:  69, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.010273, mae: 1.169082, mean_q: 1.421928, mean_eps: 0.559308\n",
      "  490574/2000000: episode: 721, duration: 5.576s, episode steps: 375, steps per second:  67, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.009740, mae: 1.179385, mean_q: 1.437387, mean_eps: 0.558653\n",
      "  491242/2000000: episode: 722, duration: 9.558s, episode steps: 668, steps per second:  70, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.010160, mae: 1.178755, mean_q: 1.436230, mean_eps: 0.558183\n",
      "  491719/2000000: episode: 723, duration: 7.014s, episode steps: 477, steps per second:  68, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.011870, mae: 1.168374, mean_q: 1.420380, mean_eps: 0.557668\n",
      "  492563/2000000: episode: 724, duration: 12.128s, episode steps: 844, steps per second:  70, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.011553, mae: 1.180959, mean_q: 1.436474, mean_eps: 0.557074\n",
      "  493533/2000000: episode: 725, duration: 13.741s, episode steps: 970, steps per second:  71, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.011910, mae: 1.190468, mean_q: 1.448221, mean_eps: 0.556257\n",
      "  494480/2000000: episode: 726, duration: 13.618s, episode steps: 947, steps per second:  70, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.011271, mae: 1.174300, mean_q: 1.427846, mean_eps: 0.555395\n",
      "  495159/2000000: episode: 727, duration: 9.859s, episode steps: 679, steps per second:  69, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.647 [0.000, 5.000],  loss: 0.010829, mae: 1.185707, mean_q: 1.442557, mean_eps: 0.554664\n",
      "  495810/2000000: episode: 728, duration: 9.485s, episode steps: 651, steps per second:  69, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.011546, mae: 1.187163, mean_q: 1.444449, mean_eps: 0.554064\n",
      "  496416/2000000: episode: 729, duration: 9.053s, episode steps: 606, steps per second:  67, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.011350, mae: 1.186625, mean_q: 1.442799, mean_eps: 0.553499\n",
      "  497153/2000000: episode: 730, duration: 10.558s, episode steps: 737, steps per second:  70, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.011406, mae: 1.180744, mean_q: 1.435477, mean_eps: 0.552894\n",
      "  497668/2000000: episode: 731, duration: 7.339s, episode steps: 515, steps per second:  70, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.010553, mae: 1.178076, mean_q: 1.433453, mean_eps: 0.552331\n",
      "  498374/2000000: episode: 732, duration: 10.390s, episode steps: 706, steps per second:  68, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.010827, mae: 1.181243, mean_q: 1.437983, mean_eps: 0.551782\n",
      "  498920/2000000: episode: 733, duration: 7.821s, episode steps: 546, steps per second:  70, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.010783, mae: 1.172501, mean_q: 1.424696, mean_eps: 0.551219\n",
      "  499581/2000000: episode: 734, duration: 9.642s, episode steps: 661, steps per second:  69, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.761 [0.000, 5.000],  loss: 0.010144, mae: 1.166430, mean_q: 1.417366, mean_eps: 0.550675\n",
      "  500629/2000000: episode: 735, duration: 15.301s, episode steps: 1048, steps per second:  68, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.610 [0.000, 5.000],  loss: 0.011257, mae: 1.199102, mean_q: 1.458948, mean_eps: 0.549905\n",
      "  501029/2000000: episode: 736, duration: 5.704s, episode steps: 400, steps per second:  70, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.014528, mae: 1.223702, mean_q: 1.488619, mean_eps: 0.549253\n",
      "  501601/2000000: episode: 737, duration: 8.922s, episode steps: 572, steps per second:  64, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.011420, mae: 1.218419, mean_q: 1.481589, mean_eps: 0.548816\n",
      "  502099/2000000: episode: 738, duration: 7.006s, episode steps: 498, steps per second:  71, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.011803, mae: 1.219952, mean_q: 1.483576, mean_eps: 0.548335\n",
      "  502939/2000000: episode: 739, duration: 11.958s, episode steps: 840, steps per second:  70, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.010095, mae: 1.207756, mean_q: 1.467909, mean_eps: 0.547734\n",
      "  503765/2000000: episode: 740, duration: 11.779s, episode steps: 826, steps per second:  70, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.011388, mae: 1.207826, mean_q: 1.468781, mean_eps: 0.546983\n",
      "  504161/2000000: episode: 741, duration: 5.610s, episode steps: 396, steps per second:  71, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.010638, mae: 1.190112, mean_q: 1.447145, mean_eps: 0.546432\n",
      "  504593/2000000: episode: 742, duration: 6.208s, episode steps: 432, steps per second:  70, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.012216, mae: 1.234277, mean_q: 1.504807, mean_eps: 0.546060\n",
      "  505555/2000000: episode: 743, duration: 13.750s, episode steps: 962, steps per second:  70, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.011986, mae: 1.221175, mean_q: 1.484818, mean_eps: 0.545433\n",
      "  506475/2000000: episode: 744, duration: 13.216s, episode steps: 920, steps per second:  70, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.011829, mae: 1.199221, mean_q: 1.458110, mean_eps: 0.544587\n",
      "  506920/2000000: episode: 745, duration: 6.435s, episode steps: 445, steps per second:  69, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.011639, mae: 1.208270, mean_q: 1.470233, mean_eps: 0.543974\n",
      "  507885/2000000: episode: 746, duration: 13.447s, episode steps: 965, steps per second:  72, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.012633, mae: 1.219332, mean_q: 1.482948, mean_eps: 0.543338\n",
      "  509322/2000000: episode: 747, duration: 20.729s, episode steps: 1437, steps per second:  69, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.569 [0.000, 5.000],  loss: 0.012323, mae: 1.208500, mean_q: 1.469491, mean_eps: 0.542256\n",
      "  510247/2000000: episode: 748, duration: 13.045s, episode steps: 925, steps per second:  71, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.012034, mae: 1.222855, mean_q: 1.487320, mean_eps: 0.541194\n",
      "  511210/2000000: episode: 749, duration: 13.939s, episode steps: 963, steps per second:  69, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.011154, mae: 1.239334, mean_q: 1.505794, mean_eps: 0.540345\n",
      "  512105/2000000: episode: 750, duration: 12.895s, episode steps: 895, steps per second:  69, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.012090, mae: 1.231421, mean_q: 1.495557, mean_eps: 0.539508\n",
      "  513039/2000000: episode: 751, duration: 13.491s, episode steps: 934, steps per second:  69, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.012223, mae: 1.232960, mean_q: 1.498285, mean_eps: 0.538685\n",
      "  513752/2000000: episode: 752, duration: 10.524s, episode steps: 713, steps per second:  68, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.010891, mae: 1.231464, mean_q: 1.497257, mean_eps: 0.537945\n",
      "  514191/2000000: episode: 753, duration: 6.329s, episode steps: 439, steps per second:  69, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.012566, mae: 1.239384, mean_q: 1.506169, mean_eps: 0.537427\n",
      "  514594/2000000: episode: 754, duration: 5.736s, episode steps: 403, steps per second:  70, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.335 [0.000, 5.000],  loss: 0.010129, mae: 1.206127, mean_q: 1.467266, mean_eps: 0.537047\n",
      "  515305/2000000: episode: 755, duration: 10.305s, episode steps: 711, steps per second:  69, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.010680, mae: 1.223527, mean_q: 1.487313, mean_eps: 0.536545\n",
      "  515865/2000000: episode: 756, duration: 8.157s, episode steps: 560, steps per second:  69, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.010115, mae: 1.229173, mean_q: 1.491293, mean_eps: 0.535973\n",
      "  516317/2000000: episode: 757, duration: 6.397s, episode steps: 452, steps per second:  71, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.011656, mae: 1.232057, mean_q: 1.496024, mean_eps: 0.535517\n",
      "  516812/2000000: episode: 758, duration: 7.032s, episode steps: 495, steps per second:  70, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.011673, mae: 1.235511, mean_q: 1.500951, mean_eps: 0.535092\n",
      "  517276/2000000: episode: 759, duration: 6.979s, episode steps: 464, steps per second:  66, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.011604, mae: 1.240205, mean_q: 1.506760, mean_eps: 0.534662\n",
      "  518322/2000000: episode: 760, duration: 15.399s, episode steps: 1046, steps per second:  68, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.010488, mae: 1.234144, mean_q: 1.501129, mean_eps: 0.533982\n",
      "  519130/2000000: episode: 761, duration: 11.568s, episode steps: 808, steps per second:  70, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.010745, mae: 1.230172, mean_q: 1.497399, mean_eps: 0.533147\n",
      "  520314/2000000: episode: 762, duration: 17.365s, episode steps: 1184, steps per second:  68, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.011962, mae: 1.235264, mean_q: 1.502641, mean_eps: 0.532250\n",
      "  521092/2000000: episode: 763, duration: 11.507s, episode steps: 778, steps per second:  68, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.011387, mae: 1.283886, mean_q: 1.561417, mean_eps: 0.531368\n",
      "  521908/2000000: episode: 764, duration: 12.133s, episode steps: 816, steps per second:  67, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.011888, mae: 1.271810, mean_q: 1.545440, mean_eps: 0.530652\n",
      "  522637/2000000: episode: 765, duration: 10.759s, episode steps: 729, steps per second:  68, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.011020, mae: 1.273469, mean_q: 1.545649, mean_eps: 0.529955\n",
      "  523761/2000000: episode: 766, duration: 16.585s, episode steps: 1124, steps per second:  68, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.010941, mae: 1.262823, mean_q: 1.534060, mean_eps: 0.529120\n",
      "  524457/2000000: episode: 767, duration: 10.597s, episode steps: 696, steps per second:  66, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.013188, mae: 1.273535, mean_q: 1.548094, mean_eps: 0.528301\n",
      "  524943/2000000: episode: 768, duration: 8.142s, episode steps: 486, steps per second:  60, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012108, mae: 1.268954, mean_q: 1.543324, mean_eps: 0.527770\n",
      "  525274/2000000: episode: 769, duration: 5.059s, episode steps: 331, steps per second:  65, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.009 [0.000, 5.000],  loss: 0.012358, mae: 1.276529, mean_q: 1.551072, mean_eps: 0.527403\n",
      "  526307/2000000: episode: 770, duration: 16.278s, episode steps: 1033, steps per second:  63, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.011964, mae: 1.273434, mean_q: 1.550282, mean_eps: 0.526789\n",
      "  527642/2000000: episode: 771, duration: 21.263s, episode steps: 1335, steps per second:  63, episode reward: 19.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.011304, mae: 1.260984, mean_q: 1.530474, mean_eps: 0.525723\n",
      "  528118/2000000: episode: 772, duration: 7.296s, episode steps: 476, steps per second:  65, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.012302, mae: 1.277083, mean_q: 1.550611, mean_eps: 0.524908\n",
      "  529024/2000000: episode: 773, duration: 14.394s, episode steps: 906, steps per second:  63, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.011897, mae: 1.269438, mean_q: 1.541582, mean_eps: 0.524287\n",
      "  529681/2000000: episode: 774, duration: 10.370s, episode steps: 657, steps per second:  63, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.012562, mae: 1.263751, mean_q: 1.533833, mean_eps: 0.523583\n",
      "  530382/2000000: episode: 775, duration: 11.243s, episode steps: 701, steps per second:  62, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.011036, mae: 1.269137, mean_q: 1.541058, mean_eps: 0.522971\n",
      "  531049/2000000: episode: 776, duration: 10.377s, episode steps: 667, steps per second:  64, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.011104, mae: 1.258956, mean_q: 1.529643, mean_eps: 0.522356\n",
      "  531600/2000000: episode: 777, duration: 8.707s, episode steps: 551, steps per second:  63, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: 0.011696, mae: 1.270063, mean_q: 1.543838, mean_eps: 0.521808\n",
      "  532203/2000000: episode: 778, duration: 9.246s, episode steps: 603, steps per second:  65, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.013112, mae: 1.276421, mean_q: 1.551015, mean_eps: 0.521290\n",
      "  533135/2000000: episode: 779, duration: 15.248s, episode steps: 932, steps per second:  61, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.010759, mae: 1.268805, mean_q: 1.542309, mean_eps: 0.520599\n",
      "  533802/2000000: episode: 780, duration: 10.603s, episode steps: 667, steps per second:  63, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.012789, mae: 1.275885, mean_q: 1.549930, mean_eps: 0.519879\n",
      "  534576/2000000: episode: 781, duration: 12.148s, episode steps: 774, steps per second:  64, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.011475, mae: 1.277503, mean_q: 1.548611, mean_eps: 0.519231\n",
      "  535342/2000000: episode: 782, duration: 12.127s, episode steps: 766, steps per second:  63, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.012128, mae: 1.270235, mean_q: 1.541140, mean_eps: 0.518538\n",
      "  536306/2000000: episode: 783, duration: 15.711s, episode steps: 964, steps per second:  61, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.010901, mae: 1.264489, mean_q: 1.536289, mean_eps: 0.517758\n",
      "  536986/2000000: episode: 784, duration: 10.994s, episode steps: 680, steps per second:  62, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.840 [0.000, 5.000],  loss: 0.013696, mae: 1.270761, mean_q: 1.544905, mean_eps: 0.517019\n",
      "  537713/2000000: episode: 785, duration: 11.903s, episode steps: 727, steps per second:  61, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.011532, mae: 1.282386, mean_q: 1.558062, mean_eps: 0.516385\n",
      "  538084/2000000: episode: 786, duration: 6.071s, episode steps: 371, steps per second:  61, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.010479, mae: 1.274890, mean_q: 1.548582, mean_eps: 0.515892\n",
      "  538739/2000000: episode: 787, duration: 10.389s, episode steps: 655, steps per second:  63, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.508 [0.000, 5.000],  loss: 0.011160, mae: 1.261528, mean_q: 1.531776, mean_eps: 0.515431\n",
      "  539249/2000000: episode: 788, duration: 7.746s, episode steps: 510, steps per second:  66, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.804 [0.000, 5.000],  loss: 0.010378, mae: 1.271363, mean_q: 1.542701, mean_eps: 0.514905\n",
      "  540422/2000000: episode: 789, duration: 18.952s, episode steps: 1173, steps per second:  62, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.256 [0.000, 5.000],  loss: 0.011578, mae: 1.276161, mean_q: 1.550092, mean_eps: 0.514148\n",
      "  541138/2000000: episode: 790, duration: 11.321s, episode steps: 716, steps per second:  63, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.173 [0.000, 5.000],  loss: 0.012868, mae: 1.287056, mean_q: 1.563928, mean_eps: 0.513298\n",
      "  541757/2000000: episode: 791, duration: 9.775s, episode steps: 619, steps per second:  63, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: 0.011265, mae: 1.286457, mean_q: 1.563391, mean_eps: 0.512697\n",
      "  542336/2000000: episode: 792, duration: 9.284s, episode steps: 579, steps per second:  62, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.012423, mae: 1.277554, mean_q: 1.552551, mean_eps: 0.512159\n",
      "  542958/2000000: episode: 793, duration: 9.588s, episode steps: 622, steps per second:  65, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.011362, mae: 1.281982, mean_q: 1.557880, mean_eps: 0.511619\n",
      "  543417/2000000: episode: 794, duration: 7.157s, episode steps: 459, steps per second:  64, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.874 [0.000, 5.000],  loss: 0.010857, mae: 1.274078, mean_q: 1.545907, mean_eps: 0.511131\n",
      "  544315/2000000: episode: 795, duration: 14.071s, episode steps: 898, steps per second:  64, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.011581, mae: 1.280692, mean_q: 1.555747, mean_eps: 0.510521\n",
      "  544916/2000000: episode: 796, duration: 9.413s, episode steps: 601, steps per second:  64, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.012235, mae: 1.277598, mean_q: 1.551379, mean_eps: 0.509847\n",
      "  545468/2000000: episode: 797, duration: 8.177s, episode steps: 552, steps per second:  68, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.010238, mae: 1.279187, mean_q: 1.554089, mean_eps: 0.509329\n",
      "  546013/2000000: episode: 798, duration: 8.549s, episode steps: 545, steps per second:  64, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.013029, mae: 1.297927, mean_q: 1.576748, mean_eps: 0.508834\n",
      "  547040/2000000: episode: 799, duration: 15.633s, episode steps: 1027, steps per second:  66, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.011555, mae: 1.296426, mean_q: 1.574889, mean_eps: 0.508127\n",
      "  547448/2000000: episode: 800, duration: 6.139s, episode steps: 408, steps per second:  66, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.013041, mae: 1.287220, mean_q: 1.564001, mean_eps: 0.507482\n",
      "  547834/2000000: episode: 801, duration: 6.249s, episode steps: 386, steps per second:  62, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.756 [0.000, 5.000],  loss: 0.010740, mae: 1.283934, mean_q: 1.562745, mean_eps: 0.507124\n",
      "  548219/2000000: episode: 802, duration: 5.901s, episode steps: 385, steps per second:  65, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.012315, mae: 1.302522, mean_q: 1.584381, mean_eps: 0.506777\n",
      "  548829/2000000: episode: 803, duration: 9.358s, episode steps: 610, steps per second:  65, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.011513, mae: 1.297721, mean_q: 1.575528, mean_eps: 0.506328\n",
      "  549673/2000000: episode: 804, duration: 13.172s, episode steps: 844, steps per second:  64, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.012026, mae: 1.290392, mean_q: 1.567259, mean_eps: 0.505673\n",
      "  550502/2000000: episode: 805, duration: 12.942s, episode steps: 829, steps per second:  64, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.011804, mae: 1.300662, mean_q: 1.580599, mean_eps: 0.504921\n",
      "  551155/2000000: episode: 806, duration: 10.314s, episode steps: 653, steps per second:  63, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.793 [0.000, 5.000],  loss: 0.012472, mae: 1.315924, mean_q: 1.596432, mean_eps: 0.504255\n",
      "  551866/2000000: episode: 807, duration: 10.962s, episode steps: 711, steps per second:  65, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.012125, mae: 1.327066, mean_q: 1.611600, mean_eps: 0.503641\n",
      "  552571/2000000: episode: 808, duration: 10.870s, episode steps: 705, steps per second:  65, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.011494, mae: 1.328182, mean_q: 1.611269, mean_eps: 0.503004\n",
      "  553645/2000000: episode: 809, duration: 16.533s, episode steps: 1074, steps per second:  65, episode reward: 15.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.382 [0.000, 5.000],  loss: 0.012531, mae: 1.320941, mean_q: 1.603597, mean_eps: 0.502203\n",
      "  553993/2000000: episode: 810, duration: 5.863s, episode steps: 348, steps per second:  59, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.012248, mae: 1.330818, mean_q: 1.615482, mean_eps: 0.501562\n",
      "  554708/2000000: episode: 811, duration: 10.883s, episode steps: 715, steps per second:  66, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.012458, mae: 1.321377, mean_q: 1.604001, mean_eps: 0.501085\n",
      "  555163/2000000: episode: 812, duration: 6.916s, episode steps: 455, steps per second:  66, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.011374, mae: 1.319843, mean_q: 1.600979, mean_eps: 0.500559\n",
      "  555658/2000000: episode: 813, duration: 7.516s, episode steps: 495, steps per second:  66, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.011955, mae: 1.313792, mean_q: 1.595546, mean_eps: 0.500131\n",
      "  556043/2000000: episode: 814, duration: 5.909s, episode steps: 385, steps per second:  65, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.011357, mae: 1.314114, mean_q: 1.596423, mean_eps: 0.499735\n",
      "  556719/2000000: episode: 815, duration: 10.324s, episode steps: 676, steps per second:  65, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.011108, mae: 1.329113, mean_q: 1.613502, mean_eps: 0.499258\n",
      "  557447/2000000: episode: 816, duration: 10.975s, episode steps: 728, steps per second:  66, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.010730, mae: 1.315742, mean_q: 1.596862, mean_eps: 0.498626\n",
      "  558001/2000000: episode: 817, duration: 8.360s, episode steps: 554, steps per second:  66, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.011694, mae: 1.313530, mean_q: 1.593827, mean_eps: 0.498048\n",
      "  558440/2000000: episode: 818, duration: 6.724s, episode steps: 439, steps per second:  65, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.941 [0.000, 5.000],  loss: 0.012891, mae: 1.320945, mean_q: 1.605168, mean_eps: 0.497602\n",
      "  559074/2000000: episode: 819, duration: 9.274s, episode steps: 634, steps per second:  68, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.010797, mae: 1.323354, mean_q: 1.607877, mean_eps: 0.497120\n",
      "  559575/2000000: episode: 820, duration: 7.755s, episode steps: 501, steps per second:  65, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.896 [0.000, 5.000],  loss: 0.013927, mae: 1.339141, mean_q: 1.629094, mean_eps: 0.496608\n",
      "  560325/2000000: episode: 821, duration: 11.445s, episode steps: 750, steps per second:  66, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.012188, mae: 1.341182, mean_q: 1.628465, mean_eps: 0.496045\n",
      "  560807/2000000: episode: 822, duration: 7.184s, episode steps: 482, steps per second:  67, episode reward: 11.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.015192, mae: 1.373103, mean_q: 1.669117, mean_eps: 0.495491\n",
      "  561691/2000000: episode: 823, duration: 13.469s, episode steps: 884, steps per second:  66, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.013498, mae: 1.367263, mean_q: 1.662237, mean_eps: 0.494877\n",
      "  562659/2000000: episode: 824, duration: 14.657s, episode steps: 968, steps per second:  66, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.011410, mae: 1.365783, mean_q: 1.661278, mean_eps: 0.494043\n",
      "  563031/2000000: episode: 825, duration: 5.450s, episode steps: 372, steps per second:  68, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.790 [0.000, 5.000],  loss: 0.014125, mae: 1.353361, mean_q: 1.643982, mean_eps: 0.493440\n",
      "  564063/2000000: episode: 826, duration: 15.607s, episode steps: 1032, steps per second:  66, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.012370, mae: 1.360445, mean_q: 1.650782, mean_eps: 0.492809\n",
      "  565082/2000000: episode: 827, duration: 15.694s, episode steps: 1019, steps per second:  65, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.012404, mae: 1.356195, mean_q: 1.646474, mean_eps: 0.491885\n",
      "  565755/2000000: episode: 828, duration: 10.578s, episode steps: 673, steps per second:  64, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.013091, mae: 1.346017, mean_q: 1.634224, mean_eps: 0.491124\n",
      "  566559/2000000: episode: 829, duration: 12.450s, episode steps: 804, steps per second:  65, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.012160, mae: 1.370673, mean_q: 1.663570, mean_eps: 0.490460\n",
      "  567567/2000000: episode: 830, duration: 15.851s, episode steps: 1008, steps per second:  64, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.011907, mae: 1.366439, mean_q: 1.659890, mean_eps: 0.489644\n",
      "  567947/2000000: episode: 831, duration: 5.925s, episode steps: 380, steps per second:  64, episode reward: 11.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.012953, mae: 1.365983, mean_q: 1.659180, mean_eps: 0.489020\n",
      "  568295/2000000: episode: 832, duration: 5.220s, episode steps: 348, steps per second:  67, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.011650, mae: 1.361542, mean_q: 1.652106, mean_eps: 0.488692\n",
      "  568782/2000000: episode: 833, duration: 7.607s, episode steps: 487, steps per second:  64, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.013840, mae: 1.374492, mean_q: 1.670784, mean_eps: 0.488316\n",
      "  569495/2000000: episode: 834, duration: 11.039s, episode steps: 713, steps per second:  65, episode reward: 22.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.012539, mae: 1.366448, mean_q: 1.658938, mean_eps: 0.487776\n",
      "  570459/2000000: episode: 835, duration: 14.518s, episode steps: 964, steps per second:  66, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.011509, mae: 1.357090, mean_q: 1.647777, mean_eps: 0.487022\n",
      "  571167/2000000: episode: 836, duration: 11.024s, episode steps: 708, steps per second:  64, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.013087, mae: 1.387203, mean_q: 1.685486, mean_eps: 0.486269\n",
      "  572146/2000000: episode: 837, duration: 14.988s, episode steps: 979, steps per second:  65, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.627 [0.000, 5.000],  loss: 0.012982, mae: 1.373813, mean_q: 1.670952, mean_eps: 0.485510\n",
      "  572954/2000000: episode: 838, duration: 12.184s, episode steps: 808, steps per second:  66, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.013260, mae: 1.383718, mean_q: 1.679969, mean_eps: 0.484705\n",
      "  573453/2000000: episode: 839, duration: 7.866s, episode steps: 499, steps per second:  63, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.012538, mae: 1.363196, mean_q: 1.654114, mean_eps: 0.484116\n",
      "  574284/2000000: episode: 840, duration: 12.801s, episode steps: 831, steps per second:  65, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.012857, mae: 1.381027, mean_q: 1.675182, mean_eps: 0.483519\n",
      "  574627/2000000: episode: 841, duration: 5.127s, episode steps: 343, steps per second:  67, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.487 [0.000, 5.000],  loss: 0.013590, mae: 1.394770, mean_q: 1.693597, mean_eps: 0.482991\n",
      "  575412/2000000: episode: 842, duration: 12.548s, episode steps: 785, steps per second:  63, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.012058, mae: 1.360528, mean_q: 1.650442, mean_eps: 0.482484\n",
      "  576087/2000000: episode: 843, duration: 11.257s, episode steps: 675, steps per second:  60, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.014131, mae: 1.382511, mean_q: 1.678730, mean_eps: 0.481827\n",
      "  576739/2000000: episode: 844, duration: 10.045s, episode steps: 652, steps per second:  65, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.011857, mae: 1.364530, mean_q: 1.655831, mean_eps: 0.481229\n",
      "  577254/2000000: episode: 845, duration: 8.155s, episode steps: 515, steps per second:  63, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.011741, mae: 1.376588, mean_q: 1.671653, mean_eps: 0.480704\n",
      "  578160/2000000: episode: 846, duration: 14.001s, episode steps: 906, steps per second:  65, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.012623, mae: 1.377368, mean_q: 1.672815, mean_eps: 0.480065\n",
      "  578844/2000000: episode: 847, duration: 10.569s, episode steps: 684, steps per second:  65, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.858 [0.000, 5.000],  loss: 0.012209, mae: 1.387255, mean_q: 1.682246, mean_eps: 0.479350\n",
      "  579302/2000000: episode: 848, duration: 7.197s, episode steps: 458, steps per second:  64, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.011055, mae: 1.370937, mean_q: 1.663926, mean_eps: 0.478835\n",
      "  580439/2000000: episode: 849, duration: 17.185s, episode steps: 1137, steps per second:  66, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.012055, mae: 1.389050, mean_q: 1.687858, mean_eps: 0.478117\n",
      "  580895/2000000: episode: 850, duration: 6.987s, episode steps: 456, steps per second:  65, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.014347, mae: 1.412630, mean_q: 1.716895, mean_eps: 0.477401\n",
      "  581569/2000000: episode: 851, duration: 10.607s, episode steps: 674, steps per second:  64, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.012439, mae: 1.416116, mean_q: 1.720565, mean_eps: 0.476891\n",
      "  582464/2000000: episode: 852, duration: 13.997s, episode steps: 895, steps per second:  64, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.012930, mae: 1.404793, mean_q: 1.705139, mean_eps: 0.476186\n",
      "  583255/2000000: episode: 853, duration: 12.545s, episode steps: 791, steps per second:  63, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.012559, mae: 1.402630, mean_q: 1.703122, mean_eps: 0.475428\n",
      "  584068/2000000: episode: 854, duration: 12.690s, episode steps: 813, steps per second:  64, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.012631, mae: 1.416709, mean_q: 1.721367, mean_eps: 0.474706\n",
      "  584608/2000000: episode: 855, duration: 8.625s, episode steps: 540, steps per second:  63, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.413 [0.000, 5.000],  loss: 0.012731, mae: 1.405862, mean_q: 1.706026, mean_eps: 0.474098\n",
      "  585221/2000000: episode: 856, duration: 9.577s, episode steps: 613, steps per second:  64, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.013647, mae: 1.411675, mean_q: 1.710870, mean_eps: 0.473577\n",
      "  586122/2000000: episode: 857, duration: 13.576s, episode steps: 901, steps per second:  66, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.013388, mae: 1.409969, mean_q: 1.709488, mean_eps: 0.472895\n",
      "  586748/2000000: episode: 858, duration: 9.846s, episode steps: 626, steps per second:  64, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.011442, mae: 1.400415, mean_q: 1.700646, mean_eps: 0.472209\n",
      "  587517/2000000: episode: 859, duration: 12.009s, episode steps: 769, steps per second:  64, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.011928, mae: 1.410110, mean_q: 1.712533, mean_eps: 0.471581\n",
      "  587886/2000000: episode: 860, duration: 5.505s, episode steps: 369, steps per second:  67, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.911 [0.000, 5.000],  loss: 0.013725, mae: 1.402648, mean_q: 1.705740, mean_eps: 0.471068\n",
      "  588505/2000000: episode: 861, duration: 9.794s, episode steps: 619, steps per second:  63, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.012126, mae: 1.399468, mean_q: 1.700926, mean_eps: 0.470624\n",
      "  589531/2000000: episode: 862, duration: 15.986s, episode steps: 1026, steps per second:  64, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.013290, mae: 1.423430, mean_q: 1.727675, mean_eps: 0.469884\n",
      "  589912/2000000: episode: 863, duration: 5.867s, episode steps: 381, steps per second:  65, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.012030, mae: 1.389218, mean_q: 1.684908, mean_eps: 0.469252\n",
      "  590699/2000000: episode: 864, duration: 12.755s, episode steps: 787, steps per second:  62, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.011580, mae: 1.429354, mean_q: 1.735789, mean_eps: 0.468726\n",
      "  591202/2000000: episode: 865, duration: 8.237s, episode steps: 503, steps per second:  61, episode reward: 12.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.011462, mae: 1.440762, mean_q: 1.748713, mean_eps: 0.468145\n",
      "  592463/2000000: episode: 866, duration: 20.292s, episode steps: 1261, steps per second:  62, episode reward: 16.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.109 [0.000, 5.000],  loss: 0.013179, mae: 1.430034, mean_q: 1.737580, mean_eps: 0.467351\n",
      "  593076/2000000: episode: 867, duration: 9.683s, episode steps: 613, steps per second:  63, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.719 [0.000, 5.000],  loss: 0.012455, mae: 1.435200, mean_q: 1.743698, mean_eps: 0.466509\n",
      "  593780/2000000: episode: 868, duration: 11.039s, episode steps: 704, steps per second:  64, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.012508, mae: 1.434717, mean_q: 1.746444, mean_eps: 0.465917\n",
      "  594225/2000000: episode: 869, duration: 7.018s, episode steps: 445, steps per second:  63, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.014188, mae: 1.441468, mean_q: 1.748347, mean_eps: 0.465398\n",
      "  594551/2000000: episode: 870, duration: 5.167s, episode steps: 326, steps per second:  63, episode reward:  4.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.012824, mae: 1.433240, mean_q: 1.742647, mean_eps: 0.465051\n",
      "  595527/2000000: episode: 871, duration: 15.296s, episode steps: 976, steps per second:  64, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.013411, mae: 1.431149, mean_q: 1.735059, mean_eps: 0.464466\n",
      "  596678/2000000: episode: 872, duration: 18.334s, episode steps: 1151, steps per second:  63, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.014087, mae: 1.452749, mean_q: 1.764621, mean_eps: 0.463508\n",
      "  597225/2000000: episode: 873, duration: 8.531s, episode steps: 547, steps per second:  64, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.013619, mae: 1.424827, mean_q: 1.727902, mean_eps: 0.462743\n",
      "  598621/2000000: episode: 874, duration: 22.714s, episode steps: 1396, steps per second:  61, episode reward: 36.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.013250, mae: 1.445986, mean_q: 1.754621, mean_eps: 0.461868\n",
      "  599230/2000000: episode: 875, duration: 9.790s, episode steps: 609, steps per second:  62, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.012688, mae: 1.451367, mean_q: 1.762683, mean_eps: 0.460967\n",
      "  599727/2000000: episode: 876, duration: 8.050s, episode steps: 497, steps per second:  62, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.143 [0.000, 5.000],  loss: 0.014374, mae: 1.448249, mean_q: 1.760532, mean_eps: 0.460470\n",
      "  600362/2000000: episode: 877, duration: 10.284s, episode steps: 635, steps per second:  62, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.012257, mae: 1.440291, mean_q: 1.747968, mean_eps: 0.459960\n",
      "  600908/2000000: episode: 878, duration: 8.509s, episode steps: 546, steps per second:  64, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.013376, mae: 1.448946, mean_q: 1.758594, mean_eps: 0.459429\n",
      "  601606/2000000: episode: 879, duration: 11.000s, episode steps: 698, steps per second:  63, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.013256, mae: 1.455463, mean_q: 1.766751, mean_eps: 0.458870\n",
      "  602012/2000000: episode: 880, duration: 6.436s, episode steps: 406, steps per second:  63, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.013676, mae: 1.451705, mean_q: 1.760257, mean_eps: 0.458373\n",
      "  602901/2000000: episode: 881, duration: 14.120s, episode steps: 889, steps per second:  63, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.501 [0.000, 5.000],  loss: 0.013102, mae: 1.456235, mean_q: 1.767191, mean_eps: 0.457790\n",
      "  603793/2000000: episode: 882, duration: 14.230s, episode steps: 892, steps per second:  63, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.833 [0.000, 5.000],  loss: 0.012799, mae: 1.453280, mean_q: 1.761147, mean_eps: 0.456987\n",
      "  604726/2000000: episode: 883, duration: 15.054s, episode steps: 933, steps per second:  62, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.010786, mae: 1.441561, mean_q: 1.748622, mean_eps: 0.456166\n",
      "  605550/2000000: episode: 884, duration: 13.509s, episode steps: 824, steps per second:  61, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.013576, mae: 1.437138, mean_q: 1.745005, mean_eps: 0.455376\n",
      "  606236/2000000: episode: 885, duration: 11.222s, episode steps: 686, steps per second:  61, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.013511, mae: 1.456142, mean_q: 1.765218, mean_eps: 0.454697\n",
      "  606635/2000000: episode: 886, duration: 6.425s, episode steps: 399, steps per second:  62, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.519 [0.000, 5.000],  loss: 0.012342, mae: 1.449128, mean_q: 1.757961, mean_eps: 0.454209\n",
      "  607307/2000000: episode: 887, duration: 11.250s, episode steps: 672, steps per second:  60, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.012412, mae: 1.442214, mean_q: 1.750464, mean_eps: 0.453727\n",
      "  608899/2000000: episode: 888, duration: 25.371s, episode steps: 1592, steps per second:  63, episode reward: 32.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.012321, mae: 1.447159, mean_q: 1.755992, mean_eps: 0.452708\n",
      "  609588/2000000: episode: 889, duration: 11.176s, episode steps: 689, steps per second:  62, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.014191, mae: 1.455574, mean_q: 1.766781, mean_eps: 0.451682\n",
      "  610294/2000000: episode: 890, duration: 11.240s, episode steps: 706, steps per second:  63, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.013929, mae: 1.466582, mean_q: 1.781671, mean_eps: 0.451054\n",
      "  610947/2000000: episode: 891, duration: 10.249s, episode steps: 653, steps per second:  64, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.011843, mae: 1.473316, mean_q: 1.788177, mean_eps: 0.450442\n",
      "  611627/2000000: episode: 892, duration: 10.990s, episode steps: 680, steps per second:  62, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.869 [0.000, 5.000],  loss: 0.011013, mae: 1.491554, mean_q: 1.809229, mean_eps: 0.449843\n",
      "  612218/2000000: episode: 893, duration: 9.373s, episode steps: 591, steps per second:  63, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.012341, mae: 1.486599, mean_q: 1.800378, mean_eps: 0.449270\n",
      "  613338/2000000: episode: 894, duration: 18.426s, episode steps: 1120, steps per second:  61, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.013480, mae: 1.487055, mean_q: 1.802196, mean_eps: 0.448500\n",
      "  614177/2000000: episode: 895, duration: 13.640s, episode steps: 839, steps per second:  62, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.160 [0.000, 5.000],  loss: 0.012804, mae: 1.476357, mean_q: 1.789873, mean_eps: 0.447618\n",
      "  614550/2000000: episode: 896, duration: 6.045s, episode steps: 373, steps per second:  62, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.012472, mae: 1.485469, mean_q: 1.801261, mean_eps: 0.447072\n",
      "  615026/2000000: episode: 897, duration: 7.880s, episode steps: 476, steps per second:  60, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.183 [0.000, 5.000],  loss: 0.013309, mae: 1.480454, mean_q: 1.794266, mean_eps: 0.446691\n",
      "  615872/2000000: episode: 898, duration: 13.592s, episode steps: 846, steps per second:  62, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.143 [0.000, 5.000],  loss: 0.013152, mae: 1.487744, mean_q: 1.803758, mean_eps: 0.446097\n",
      "  616682/2000000: episode: 899, duration: 13.175s, episode steps: 810, steps per second:  61, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.011820, mae: 1.470560, mean_q: 1.781115, mean_eps: 0.445352\n",
      "  617064/2000000: episode: 900, duration: 6.148s, episode steps: 382, steps per second:  62, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.011675, mae: 1.489530, mean_q: 1.805271, mean_eps: 0.444815\n",
      "  618251/2000000: episode: 901, duration: 19.058s, episode steps: 1187, steps per second:  62, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.012463, mae: 1.490655, mean_q: 1.807002, mean_eps: 0.444110\n",
      "  618896/2000000: episode: 902, duration: 10.408s, episode steps: 645, steps per second:  62, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.012474, mae: 1.494796, mean_q: 1.814036, mean_eps: 0.443285\n",
      "  619664/2000000: episode: 903, duration: 12.486s, episode steps: 768, steps per second:  62, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.079 [0.000, 5.000],  loss: 0.011864, mae: 1.479502, mean_q: 1.793235, mean_eps: 0.442650\n",
      "  620133/2000000: episode: 904, duration: 7.687s, episode steps: 469, steps per second:  61, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.012165, mae: 1.493159, mean_q: 1.811407, mean_eps: 0.442092\n",
      "  621135/2000000: episode: 905, duration: 16.646s, episode steps: 1002, steps per second:  60, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.012181, mae: 1.501853, mean_q: 1.821639, mean_eps: 0.441429\n",
      "  621824/2000000: episode: 906, duration: 11.149s, episode steps: 689, steps per second:  62, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.462 [0.000, 5.000],  loss: 0.012173, mae: 1.506948, mean_q: 1.826662, mean_eps: 0.440670\n",
      "  622413/2000000: episode: 907, duration: 9.938s, episode steps: 589, steps per second:  59, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.012958, mae: 1.516848, mean_q: 1.840287, mean_eps: 0.440094\n",
      "  623750/2000000: episode: 908, duration: 21.754s, episode steps: 1337, steps per second:  61, episode reward: 23.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.358 [0.000, 5.000],  loss: 0.013023, mae: 1.504703, mean_q: 1.823873, mean_eps: 0.439226\n",
      "  624451/2000000: episode: 909, duration: 11.617s, episode steps: 701, steps per second:  60, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.813 [0.000, 5.000],  loss: 0.011396, mae: 1.504269, mean_q: 1.824211, mean_eps: 0.438310\n",
      "  624980/2000000: episode: 910, duration: 8.643s, episode steps: 529, steps per second:  61, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.011796, mae: 1.489647, mean_q: 1.805878, mean_eps: 0.437757\n",
      "  625541/2000000: episode: 911, duration: 8.976s, episode steps: 561, steps per second:  62, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.011637, mae: 1.501705, mean_q: 1.820060, mean_eps: 0.437266\n",
      "  626546/2000000: episode: 912, duration: 16.428s, episode steps: 1005, steps per second:  61, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.011816, mae: 1.513298, mean_q: 1.833712, mean_eps: 0.436560\n",
      "  627319/2000000: episode: 913, duration: 12.815s, episode steps: 773, steps per second:  60, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.012830, mae: 1.520668, mean_q: 1.843200, mean_eps: 0.435761\n",
      "  627869/2000000: episode: 914, duration: 9.180s, episode steps: 550, steps per second:  60, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.584 [0.000, 5.000],  loss: 0.012820, mae: 1.512505, mean_q: 1.832943, mean_eps: 0.435165\n",
      "  628410/2000000: episode: 915, duration: 8.877s, episode steps: 541, steps per second:  61, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.012718, mae: 1.504920, mean_q: 1.823972, mean_eps: 0.434674\n",
      "  629069/2000000: episode: 916, duration: 10.875s, episode steps: 659, steps per second:  61, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.013540, mae: 1.502172, mean_q: 1.820866, mean_eps: 0.434134\n",
      "  630023/2000000: episode: 917, duration: 15.748s, episode steps: 954, steps per second:  61, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.012652, mae: 1.507522, mean_q: 1.827015, mean_eps: 0.433409\n",
      "  630842/2000000: episode: 918, duration: 13.267s, episode steps: 819, steps per second:  62, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.014615, mae: 1.541377, mean_q: 1.867453, mean_eps: 0.432611\n",
      "  631467/2000000: episode: 919, duration: 10.353s, episode steps: 625, steps per second:  60, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.720 [0.000, 5.000],  loss: 0.011353, mae: 1.532463, mean_q: 1.859940, mean_eps: 0.431961\n",
      "  632445/2000000: episode: 920, duration: 15.994s, episode steps: 978, steps per second:  61, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.014484, mae: 1.541951, mean_q: 1.869327, mean_eps: 0.431240\n",
      "  633246/2000000: episode: 921, duration: 13.267s, episode steps: 801, steps per second:  60, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.013991, mae: 1.543819, mean_q: 1.870999, mean_eps: 0.430439\n",
      "  633642/2000000: episode: 922, duration: 6.515s, episode steps: 396, steps per second:  61, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.578 [0.000, 5.000],  loss: 0.013068, mae: 1.532242, mean_q: 1.857061, mean_eps: 0.429900\n",
      "  634293/2000000: episode: 923, duration: 10.726s, episode steps: 651, steps per second:  61, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.013286, mae: 1.544145, mean_q: 1.870903, mean_eps: 0.429429\n",
      "  634806/2000000: episode: 924, duration: 8.607s, episode steps: 513, steps per second:  60, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.012267, mae: 1.537158, mean_q: 1.864631, mean_eps: 0.428905\n",
      "  635424/2000000: episode: 925, duration: 10.604s, episode steps: 618, steps per second:  58, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.885 [0.000, 5.000],  loss: 0.015896, mae: 1.520160, mean_q: 1.841267, mean_eps: 0.428397\n",
      "  636358/2000000: episode: 926, duration: 15.571s, episode steps: 934, steps per second:  60, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.012818, mae: 1.535835, mean_q: 1.859916, mean_eps: 0.427699\n",
      "  636990/2000000: episode: 927, duration: 10.751s, episode steps: 632, steps per second:  59, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.013826, mae: 1.535776, mean_q: 1.861337, mean_eps: 0.426993\n",
      "  637514/2000000: episode: 928, duration: 8.507s, episode steps: 524, steps per second:  62, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.013722, mae: 1.545754, mean_q: 1.873800, mean_eps: 0.426473\n",
      "  638129/2000000: episode: 929, duration: 10.018s, episode steps: 615, steps per second:  61, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.011309, mae: 1.544406, mean_q: 1.872487, mean_eps: 0.425960\n",
      "  638899/2000000: episode: 930, duration: 12.726s, episode steps: 770, steps per second:  61, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.015843, mae: 1.553560, mean_q: 1.883819, mean_eps: 0.425337\n",
      "  639806/2000000: episode: 931, duration: 15.094s, episode steps: 907, steps per second:  60, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.012236, mae: 1.549818, mean_q: 1.877849, mean_eps: 0.424583\n",
      "  640181/2000000: episode: 932, duration: 6.310s, episode steps: 375, steps per second:  59, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.592 [0.000, 5.000],  loss: 0.013205, mae: 1.550599, mean_q: 1.881464, mean_eps: 0.424005\n",
      "  640902/2000000: episode: 933, duration: 12.246s, episode steps: 721, steps per second:  59, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.012229, mae: 1.559551, mean_q: 1.891006, mean_eps: 0.423512\n",
      "  641466/2000000: episode: 934, duration: 9.232s, episode steps: 564, steps per second:  61, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.757 [0.000, 5.000],  loss: 0.014297, mae: 1.574776, mean_q: 1.908748, mean_eps: 0.422934\n",
      "  642116/2000000: episode: 935, duration: 10.835s, episode steps: 650, steps per second:  60, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.012925, mae: 1.548099, mean_q: 1.876906, mean_eps: 0.422389\n",
      "  642868/2000000: episode: 936, duration: 12.487s, episode steps: 752, steps per second:  60, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.485 [0.000, 5.000],  loss: 0.012727, mae: 1.555044, mean_q: 1.886510, mean_eps: 0.421759\n",
      "  643937/2000000: episode: 937, duration: 18.003s, episode steps: 1069, steps per second:  59, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.928 [0.000, 5.000],  loss: 0.014029, mae: 1.566699, mean_q: 1.899854, mean_eps: 0.420938\n",
      "  644744/2000000: episode: 938, duration: 13.549s, episode steps: 807, steps per second:  60, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.206 [0.000, 5.000],  loss: 0.013370, mae: 1.567077, mean_q: 1.901038, mean_eps: 0.420094\n",
      "  645659/2000000: episode: 939, duration: 15.170s, episode steps: 915, steps per second:  60, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.014006, mae: 1.572454, mean_q: 1.905817, mean_eps: 0.419320\n",
      "  646094/2000000: episode: 940, duration: 7.219s, episode steps: 435, steps per second:  60, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.876 [0.000, 5.000],  loss: 0.010728, mae: 1.552067, mean_q: 1.881376, mean_eps: 0.418712\n",
      "  646903/2000000: episode: 941, duration: 13.430s, episode steps: 809, steps per second:  60, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.014347, mae: 1.550142, mean_q: 1.877831, mean_eps: 0.418152\n",
      "  647410/2000000: episode: 942, duration: 8.675s, episode steps: 507, steps per second:  58, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.012184, mae: 1.552739, mean_q: 1.881379, mean_eps: 0.417560\n",
      "  648311/2000000: episode: 943, duration: 15.012s, episode steps: 901, steps per second:  60, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.012614, mae: 1.565501, mean_q: 1.898813, mean_eps: 0.416926\n",
      "  648933/2000000: episode: 944, duration: 10.215s, episode steps: 622, steps per second:  61, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.934 [0.000, 5.000],  loss: 0.013418, mae: 1.552950, mean_q: 1.882620, mean_eps: 0.416240\n",
      "  649484/2000000: episode: 945, duration: 9.605s, episode steps: 551, steps per second:  57, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.014837, mae: 1.568300, mean_q: 1.898861, mean_eps: 0.415713\n",
      "  650058/2000000: episode: 946, duration: 9.873s, episode steps: 574, steps per second:  58, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.013734, mae: 1.568912, mean_q: 1.898681, mean_eps: 0.415207\n",
      "  650865/2000000: episode: 947, duration: 13.502s, episode steps: 807, steps per second:  60, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.908 [0.000, 5.000],  loss: 0.012989, mae: 1.568045, mean_q: 1.899818, mean_eps: 0.414584\n",
      "  651302/2000000: episode: 948, duration: 7.762s, episode steps: 437, steps per second:  56, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.013609, mae: 1.576061, mean_q: 1.909542, mean_eps: 0.414024\n",
      "  652264/2000000: episode: 949, duration: 16.126s, episode steps: 962, steps per second:  60, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.249 [0.000, 5.000],  loss: 0.013408, mae: 1.558773, mean_q: 1.887095, mean_eps: 0.413396\n",
      "  653400/2000000: episode: 950, duration: 19.142s, episode steps: 1136, steps per second:  59, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.013728, mae: 1.563111, mean_q: 1.894643, mean_eps: 0.412453\n",
      "  654069/2000000: episode: 951, duration: 11.179s, episode steps: 669, steps per second:  60, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.013397, mae: 1.567376, mean_q: 1.899084, mean_eps: 0.411639\n",
      "  655071/2000000: episode: 952, duration: 17.009s, episode steps: 1002, steps per second:  59, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.012412, mae: 1.568739, mean_q: 1.900856, mean_eps: 0.410887\n",
      "  656000/2000000: episode: 953, duration: 15.660s, episode steps: 929, steps per second:  59, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.890 [0.000, 5.000],  loss: 0.013190, mae: 1.568050, mean_q: 1.900605, mean_eps: 0.410019\n",
      "  656915/2000000: episode: 954, duration: 15.914s, episode steps: 915, steps per second:  57, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.013140, mae: 1.575008, mean_q: 1.907877, mean_eps: 0.409190\n",
      "  657543/2000000: episode: 955, duration: 10.398s, episode steps: 628, steps per second:  60, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.014193, mae: 1.547756, mean_q: 1.873940, mean_eps: 0.408495\n",
      "  658227/2000000: episode: 956, duration: 11.444s, episode steps: 684, steps per second:  60, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.013415, mae: 1.554458, mean_q: 1.882500, mean_eps: 0.407904\n",
      "  658972/2000000: episode: 957, duration: 12.645s, episode steps: 745, steps per second:  59, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.013334, mae: 1.588197, mean_q: 1.921541, mean_eps: 0.407262\n",
      "  659781/2000000: episode: 958, duration: 13.763s, episode steps: 809, steps per second:  59, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.879 [0.000, 5.000],  loss: 0.011702, mae: 1.544084, mean_q: 1.869622, mean_eps: 0.406562\n",
      "  660811/2000000: episode: 959, duration: 17.202s, episode steps: 1030, steps per second:  60, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.014244, mae: 1.597406, mean_q: 1.934374, mean_eps: 0.405734\n",
      "  661640/2000000: episode: 960, duration: 13.953s, episode steps: 829, steps per second:  59, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.873 [0.000, 5.000],  loss: 0.012821, mae: 1.597098, mean_q: 1.935522, mean_eps: 0.404898\n",
      "  662656/2000000: episode: 961, duration: 17.062s, episode steps: 1016, steps per second:  60, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.432 [0.000, 5.000],  loss: 0.014217, mae: 1.611804, mean_q: 1.953330, mean_eps: 0.404069\n",
      "  663236/2000000: episode: 962, duration: 9.704s, episode steps: 580, steps per second:  60, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.013212, mae: 1.594091, mean_q: 1.929636, mean_eps: 0.403350\n",
      "  663809/2000000: episode: 963, duration: 9.896s, episode steps: 573, steps per second:  58, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.012647, mae: 1.596732, mean_q: 1.936548, mean_eps: 0.402830\n",
      "  664853/2000000: episode: 964, duration: 17.805s, episode steps: 1044, steps per second:  59, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.013170, mae: 1.619881, mean_q: 1.964517, mean_eps: 0.402101\n",
      "  665651/2000000: episode: 965, duration: 13.621s, episode steps: 798, steps per second:  59, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.013762, mae: 1.613582, mean_q: 1.954162, mean_eps: 0.401273\n",
      "  666203/2000000: episode: 966, duration: 9.513s, episode steps: 552, steps per second:  58, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.012432, mae: 1.619051, mean_q: 1.959771, mean_eps: 0.400667\n",
      "  666919/2000000: episode: 967, duration: 12.283s, episode steps: 716, steps per second:  58, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.014347, mae: 1.618054, mean_q: 1.961918, mean_eps: 0.400096\n",
      "  667530/2000000: episode: 968, duration: 11.011s, episode steps: 611, steps per second:  55, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.614 [0.000, 5.000],  loss: 0.013018, mae: 1.601235, mean_q: 1.939381, mean_eps: 0.399498\n",
      "  668092/2000000: episode: 969, duration: 10.329s, episode steps: 562, steps per second:  54, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.013456, mae: 1.608281, mean_q: 1.945009, mean_eps: 0.398971\n",
      "  668890/2000000: episode: 970, duration: 13.835s, episode steps: 798, steps per second:  58, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.013116, mae: 1.617590, mean_q: 1.957832, mean_eps: 0.398359\n",
      "  669796/2000000: episode: 971, duration: 15.340s, episode steps: 906, steps per second:  59, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.012162, mae: 1.590099, mean_q: 1.924627, mean_eps: 0.397592\n",
      "  671042/2000000: episode: 972, duration: 21.644s, episode steps: 1246, steps per second:  58, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.266 [0.000, 5.000],  loss: 0.012977, mae: 1.650309, mean_q: 2.000124, mean_eps: 0.396624\n",
      "  671916/2000000: episode: 973, duration: 15.086s, episode steps: 874, steps per second:  58, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.013169, mae: 1.644263, mean_q: 1.995687, mean_eps: 0.395670\n",
      "  672537/2000000: episode: 974, duration: 10.977s, episode steps: 621, steps per second:  57, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.836 [0.000, 5.000],  loss: 0.015207, mae: 1.639891, mean_q: 1.987968, mean_eps: 0.394997\n",
      "  673423/2000000: episode: 975, duration: 15.084s, episode steps: 886, steps per second:  59, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.331 [0.000, 5.000],  loss: 0.013593, mae: 1.648879, mean_q: 1.999054, mean_eps: 0.394318\n",
      "  674083/2000000: episode: 976, duration: 11.538s, episode steps: 660, steps per second:  57, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.015620, mae: 1.656523, mean_q: 2.008419, mean_eps: 0.393623\n",
      "  674531/2000000: episode: 977, duration: 7.886s, episode steps: 448, steps per second:  57, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.938 [0.000, 5.000],  loss: 0.013630, mae: 1.647041, mean_q: 1.993754, mean_eps: 0.393125\n",
      "  674952/2000000: episode: 978, duration: 7.050s, episode steps: 421, steps per second:  60, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.618 [0.000, 5.000],  loss: 0.014811, mae: 1.640100, mean_q: 1.987316, mean_eps: 0.392734\n",
      "  675342/2000000: episode: 979, duration: 6.729s, episode steps: 390, steps per second:  58, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.962 [0.000, 5.000],  loss: 0.013736, mae: 1.659417, mean_q: 2.009507, mean_eps: 0.392369\n",
      "  676034/2000000: episode: 980, duration: 11.923s, episode steps: 692, steps per second:  58, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.013445, mae: 1.655299, mean_q: 2.005757, mean_eps: 0.391881\n",
      "  676860/2000000: episode: 981, duration: 14.152s, episode steps: 826, steps per second:  58, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.013616, mae: 1.654585, mean_q: 2.003858, mean_eps: 0.391199\n",
      "  677787/2000000: episode: 982, duration: 16.320s, episode steps: 927, steps per second:  57, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.014375, mae: 1.652293, mean_q: 2.000648, mean_eps: 0.390410\n",
      "  678420/2000000: episode: 983, duration: 11.186s, episode steps: 633, steps per second:  57, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.959 [0.000, 5.000],  loss: 0.011915, mae: 1.640024, mean_q: 1.985763, mean_eps: 0.389708\n",
      "  678878/2000000: episode: 984, duration: 7.948s, episode steps: 458, steps per second:  58, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.012675, mae: 1.635506, mean_q: 1.982452, mean_eps: 0.389217\n",
      "  679385/2000000: episode: 985, duration: 8.984s, episode steps: 507, steps per second:  56, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.014199, mae: 1.655457, mean_q: 2.004943, mean_eps: 0.388781\n",
      "  679919/2000000: episode: 986, duration: 9.464s, episode steps: 534, steps per second:  56, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.979 [0.000, 5.000],  loss: 0.014676, mae: 1.655848, mean_q: 2.008024, mean_eps: 0.388313\n",
      "  680391/2000000: episode: 987, duration: 7.953s, episode steps: 472, steps per second:  59, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.680 [0.000, 5.000],  loss: 0.014177, mae: 1.657472, mean_q: 2.008287, mean_eps: 0.387861\n",
      "  681555/2000000: episode: 988, duration: 20.068s, episode steps: 1164, steps per second:  58, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.013609, mae: 1.660671, mean_q: 2.012218, mean_eps: 0.387125\n",
      "  682378/2000000: episode: 989, duration: 14.334s, episode steps: 823, steps per second:  57, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.027 [0.000, 5.000],  loss: 0.012697, mae: 1.671815, mean_q: 2.027791, mean_eps: 0.386231\n",
      "  683084/2000000: episode: 990, duration: 12.402s, episode steps: 706, steps per second:  57, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.013819, mae: 1.674892, mean_q: 2.027475, mean_eps: 0.385543\n",
      "  683538/2000000: episode: 991, duration: 7.748s, episode steps: 454, steps per second:  59, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.731 [0.000, 5.000],  loss: 0.012820, mae: 1.652029, mean_q: 2.003306, mean_eps: 0.385021\n",
      "  684435/2000000: episode: 992, duration: 15.487s, episode steps: 897, steps per second:  58, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.013928, mae: 1.684156, mean_q: 2.041409, mean_eps: 0.384413\n",
      "  685083/2000000: episode: 993, duration: 11.223s, episode steps: 648, steps per second:  58, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.532 [0.000, 5.000],  loss: 0.013683, mae: 1.662008, mean_q: 2.013342, mean_eps: 0.383718\n",
      "  685929/2000000: episode: 994, duration: 14.933s, episode steps: 846, steps per second:  57, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.014939, mae: 1.691586, mean_q: 2.048792, mean_eps: 0.383045\n",
      "  686306/2000000: episode: 995, duration: 6.662s, episode steps: 377, steps per second:  57, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.512 [0.000, 5.000],  loss: 0.014797, mae: 1.711970, mean_q: 2.074572, mean_eps: 0.382494\n",
      "  687284/2000000: episode: 996, duration: 16.988s, episode steps: 978, steps per second:  58, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.570 [0.000, 5.000],  loss: 0.015429, mae: 1.671689, mean_q: 2.024291, mean_eps: 0.381885\n",
      "  687713/2000000: episode: 997, duration: 7.500s, episode steps: 429, steps per second:  57, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.014352, mae: 1.681480, mean_q: 2.035467, mean_eps: 0.381252\n",
      "  688142/2000000: episode: 998, duration: 7.449s, episode steps: 429, steps per second:  58, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.949 [0.000, 5.000],  loss: 0.013651, mae: 1.679086, mean_q: 2.032988, mean_eps: 0.380865\n",
      "  688625/2000000: episode: 999, duration: 8.320s, episode steps: 483, steps per second:  58, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.037 [0.000, 5.000],  loss: 0.014676, mae: 1.676592, mean_q: 2.027729, mean_eps: 0.380454\n",
      "  689277/2000000: episode: 1000, duration: 11.443s, episode steps: 652, steps per second:  57, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.014168, mae: 1.682409, mean_q: 2.037732, mean_eps: 0.379943\n",
      "  690019/2000000: episode: 1001, duration: 12.806s, episode steps: 742, steps per second:  58, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.926 [0.000, 5.000],  loss: 0.017201, mae: 1.673953, mean_q: 2.025540, mean_eps: 0.379317\n",
      "  690514/2000000: episode: 1002, duration: 8.320s, episode steps: 495, steps per second:  59, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.013699, mae: 1.716075, mean_q: 2.083232, mean_eps: 0.378761\n",
      "  691180/2000000: episode: 1003, duration: 11.633s, episode steps: 666, steps per second:  57, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.014102, mae: 1.715337, mean_q: 2.079272, mean_eps: 0.378239\n",
      "  692035/2000000: episode: 1004, duration: 15.115s, episode steps: 855, steps per second:  57, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.117 [0.000, 5.000],  loss: 0.013682, mae: 1.714881, mean_q: 2.077993, mean_eps: 0.377555\n",
      "  692692/2000000: episode: 1005, duration: 11.436s, episode steps: 657, steps per second:  57, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.013307, mae: 1.714603, mean_q: 2.079558, mean_eps: 0.376874\n",
      "  693305/2000000: episode: 1006, duration: 10.899s, episode steps: 613, steps per second:  56, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.013543, mae: 1.722062, mean_q: 2.084966, mean_eps: 0.376302\n",
      "  694293/2000000: episode: 1007, duration: 16.982s, episode steps: 988, steps per second:  58, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.810 [0.000, 5.000],  loss: 0.014273, mae: 1.721873, mean_q: 2.084953, mean_eps: 0.375580\n",
      "  694829/2000000: episode: 1008, duration: 9.448s, episode steps: 536, steps per second:  57, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.014050, mae: 1.726946, mean_q: 2.093844, mean_eps: 0.374894\n",
      "  695738/2000000: episode: 1009, duration: 15.671s, episode steps: 909, steps per second:  58, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.015687, mae: 1.725001, mean_q: 2.088686, mean_eps: 0.374244\n",
      "  696191/2000000: episode: 1010, duration: 7.803s, episode steps: 453, steps per second:  58, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.016058, mae: 1.716844, mean_q: 2.079502, mean_eps: 0.373632\n",
      "  696585/2000000: episode: 1011, duration: 7.059s, episode steps: 394, steps per second:  56, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.914 [0.000, 5.000],  loss: 0.012623, mae: 1.689665, mean_q: 2.045902, mean_eps: 0.373251\n",
      "  697397/2000000: episode: 1012, duration: 14.278s, episode steps: 812, steps per second:  57, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.013362, mae: 1.712514, mean_q: 2.078231, mean_eps: 0.372707\n",
      "  697891/2000000: episode: 1013, duration: 8.423s, episode steps: 494, steps per second:  59, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.012626, mae: 1.703074, mean_q: 2.063069, mean_eps: 0.372120\n",
      "  698681/2000000: episode: 1014, duration: 14.153s, episode steps: 790, steps per second:  56, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.013511, mae: 1.722077, mean_q: 2.086430, mean_eps: 0.371543\n",
      "  699055/2000000: episode: 1015, duration: 6.503s, episode steps: 374, steps per second:  58, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.014070, mae: 1.732755, mean_q: 2.098704, mean_eps: 0.371019\n",
      "  699733/2000000: episode: 1016, duration: 12.120s, episode steps: 678, steps per second:  56, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.015219, mae: 1.718254, mean_q: 2.079373, mean_eps: 0.370545\n",
      "  700259/2000000: episode: 1017, duration: 9.352s, episode steps: 526, steps per second:  56, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.012588, mae: 1.723335, mean_q: 2.088378, mean_eps: 0.370004\n",
      "  700638/2000000: episode: 1018, duration: 6.740s, episode steps: 379, steps per second:  56, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.016298, mae: 1.747823, mean_q: 2.114729, mean_eps: 0.369597\n",
      "  701601/2000000: episode: 1019, duration: 16.762s, episode steps: 963, steps per second:  57, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.012674, mae: 1.748557, mean_q: 2.118836, mean_eps: 0.368992\n",
      "  702108/2000000: episode: 1020, duration: 9.087s, episode steps: 507, steps per second:  56, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.013307, mae: 1.730410, mean_q: 2.096346, mean_eps: 0.368331\n",
      "  703012/2000000: episode: 1021, duration: 15.896s, episode steps: 904, steps per second:  57, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.013146, mae: 1.743331, mean_q: 2.111467, mean_eps: 0.367698\n",
      "  703463/2000000: episode: 1022, duration: 7.778s, episode steps: 451, steps per second:  58, episode reward: 12.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.013557, mae: 1.724637, mean_q: 2.088000, mean_eps: 0.367088\n",
      "  703918/2000000: episode: 1023, duration: 8.015s, episode steps: 455, steps per second:  57, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.015801, mae: 1.748427, mean_q: 2.121616, mean_eps: 0.366679\n",
      "  704556/2000000: episode: 1024, duration: 11.124s, episode steps: 638, steps per second:  57, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.014607, mae: 1.748846, mean_q: 2.117160, mean_eps: 0.366188\n",
      "  705205/2000000: episode: 1025, duration: 11.555s, episode steps: 649, steps per second:  56, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.014521, mae: 1.748719, mean_q: 2.120079, mean_eps: 0.365608\n",
      "  706223/2000000: episode: 1026, duration: 17.975s, episode steps: 1018, steps per second:  57, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.014665, mae: 1.737206, mean_q: 2.104089, mean_eps: 0.364857\n",
      "  706725/2000000: episode: 1027, duration: 9.010s, episode steps: 502, steps per second:  56, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.014340, mae: 1.740502, mean_q: 2.107923, mean_eps: 0.364173\n",
      "  707386/2000000: episode: 1028, duration: 11.580s, episode steps: 661, steps per second:  57, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.012967, mae: 1.730455, mean_q: 2.097767, mean_eps: 0.363650\n",
      "  708365/2000000: episode: 1029, duration: 17.378s, episode steps: 979, steps per second:  56, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.464 [0.000, 5.000],  loss: 0.013880, mae: 1.737951, mean_q: 2.106330, mean_eps: 0.362912\n",
      "  708850/2000000: episode: 1030, duration: 8.452s, episode steps: 485, steps per second:  57, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 3.235 [0.000, 5.000],  loss: 0.013444, mae: 1.733950, mean_q: 2.100826, mean_eps: 0.362253\n",
      "  709558/2000000: episode: 1031, duration: 12.521s, episode steps: 708, steps per second:  57, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.016869, mae: 1.749468, mean_q: 2.119668, mean_eps: 0.361716\n",
      "  710474/2000000: episode: 1032, duration: 16.128s, episode steps: 916, steps per second:  57, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.550 [0.000, 5.000],  loss: 0.013545, mae: 1.755542, mean_q: 2.126770, mean_eps: 0.360986\n",
      "  710829/2000000: episode: 1033, duration: 6.181s, episode steps: 355, steps per second:  57, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.445 [0.000, 5.000],  loss: 0.014483, mae: 1.780353, mean_q: 2.162290, mean_eps: 0.360413\n",
      "  711796/2000000: episode: 1034, duration: 17.296s, episode steps: 967, steps per second:  56, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.014657, mae: 1.771207, mean_q: 2.143701, mean_eps: 0.359819\n",
      "  712943/2000000: episode: 1035, duration: 20.523s, episode steps: 1147, steps per second:  56, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.845 [0.000, 5.000],  loss: 0.015272, mae: 1.778218, mean_q: 2.153064, mean_eps: 0.358869\n",
      "  713764/2000000: episode: 1036, duration: 14.860s, episode steps: 821, steps per second:  55, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.014142, mae: 1.775564, mean_q: 2.148007, mean_eps: 0.357983\n",
      "  714318/2000000: episode: 1037, duration: 9.948s, episode steps: 554, steps per second:  56, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.764 [0.000, 5.000],  loss: 0.014667, mae: 1.754397, mean_q: 2.125582, mean_eps: 0.357364\n",
      "  715142/2000000: episode: 1038, duration: 15.110s, episode steps: 824, steps per second:  55, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.013268, mae: 1.767901, mean_q: 2.140309, mean_eps: 0.356743\n",
      "  715823/2000000: episode: 1039, duration: 12.288s, episode steps: 681, steps per second:  55, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.014473, mae: 1.765245, mean_q: 2.136977, mean_eps: 0.356066\n",
      "  717075/2000000: episode: 1040, duration: 22.459s, episode steps: 1252, steps per second:  56, episode reward: 22.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.014831, mae: 1.766345, mean_q: 2.140827, mean_eps: 0.355197\n",
      "  717459/2000000: episode: 1041, duration: 7.185s, episode steps: 384, steps per second:  53, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.014668, mae: 1.769728, mean_q: 2.142720, mean_eps: 0.354461\n",
      "  718011/2000000: episode: 1042, duration: 9.764s, episode steps: 552, steps per second:  57, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.015107, mae: 1.777844, mean_q: 2.151602, mean_eps: 0.354039\n",
      "  718899/2000000: episode: 1043, duration: 16.139s, episode steps: 888, steps per second:  55, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.600 [0.000, 5.000],  loss: 0.014437, mae: 1.768315, mean_q: 2.138533, mean_eps: 0.353391\n",
      "  719383/2000000: episode: 1044, duration: 8.858s, episode steps: 484, steps per second:  55, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.016857, mae: 1.764777, mean_q: 2.137770, mean_eps: 0.352774\n",
      "  719940/2000000: episode: 1045, duration: 10.167s, episode steps: 557, steps per second:  55, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.016865, mae: 1.778216, mean_q: 2.151377, mean_eps: 0.352306\n",
      "  720530/2000000: episode: 1046, duration: 10.589s, episode steps: 590, steps per second:  56, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.012275, mae: 1.780498, mean_q: 2.156660, mean_eps: 0.351789\n",
      "  721110/2000000: episode: 1047, duration: 10.559s, episode steps: 580, steps per second:  55, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.015153, mae: 1.819611, mean_q: 2.201096, mean_eps: 0.351262\n",
      "  721967/2000000: episode: 1048, duration: 15.661s, episode steps: 857, steps per second:  55, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.784 [0.000, 5.000],  loss: 0.015291, mae: 1.803826, mean_q: 2.186042, mean_eps: 0.350616\n",
      "  722547/2000000: episode: 1049, duration: 10.671s, episode steps: 580, steps per second:  54, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.015264, mae: 1.793905, mean_q: 2.174485, mean_eps: 0.349970\n",
      "  722958/2000000: episode: 1050, duration: 7.425s, episode steps: 411, steps per second:  55, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.014915, mae: 1.802698, mean_q: 2.183318, mean_eps: 0.349523\n",
      "  724050/2000000: episode: 1051, duration: 19.804s, episode steps: 1092, steps per second:  55, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.015944, mae: 1.807037, mean_q: 2.189773, mean_eps: 0.348846\n",
      "  724767/2000000: episode: 1052, duration: 13.090s, episode steps: 717, steps per second:  55, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.015499, mae: 1.795894, mean_q: 2.173693, mean_eps: 0.348033\n",
      "  725496/2000000: episode: 1053, duration: 13.473s, episode steps: 729, steps per second:  54, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.015558, mae: 1.801965, mean_q: 2.182660, mean_eps: 0.347383\n",
      "  725865/2000000: episode: 1054, duration: 6.789s, episode steps: 369, steps per second:  54, episode reward: 10.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.870 [0.000, 5.000],  loss: 0.013998, mae: 1.807061, mean_q: 2.190028, mean_eps: 0.346888\n",
      "  726640/2000000: episode: 1055, duration: 14.317s, episode steps: 775, steps per second:  54, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.925 [0.000, 5.000],  loss: 0.014937, mae: 1.810283, mean_q: 2.192194, mean_eps: 0.346373\n",
      "  727069/2000000: episode: 1056, duration: 7.996s, episode steps: 429, steps per second:  54, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.015257, mae: 1.789225, mean_q: 2.165021, mean_eps: 0.345831\n",
      "  727498/2000000: episode: 1057, duration: 7.703s, episode steps: 429, steps per second:  56, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.015842, mae: 1.790143, mean_q: 2.165556, mean_eps: 0.345444\n",
      "  728023/2000000: episode: 1058, duration: 9.569s, episode steps: 525, steps per second:  55, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.950 [0.000, 5.000],  loss: 0.013862, mae: 1.801599, mean_q: 2.179589, mean_eps: 0.345016\n",
      "  728797/2000000: episode: 1059, duration: 14.348s, episode steps: 774, steps per second:  54, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.014584, mae: 1.797544, mean_q: 2.174655, mean_eps: 0.344431\n",
      "  729283/2000000: episode: 1060, duration: 8.791s, episode steps: 486, steps per second:  55, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.015557, mae: 1.805962, mean_q: 2.187385, mean_eps: 0.343864\n",
      "  729746/2000000: episode: 1061, duration: 8.510s, episode steps: 463, steps per second:  54, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.013370, mae: 1.798296, mean_q: 2.174724, mean_eps: 0.343437\n",
      "  730423/2000000: episode: 1062, duration: 12.302s, episode steps: 677, steps per second:  55, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.013244, mae: 1.803913, mean_q: 2.184331, mean_eps: 0.342924\n",
      "  730833/2000000: episode: 1063, duration: 7.245s, episode steps: 410, steps per second:  57, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.013780, mae: 1.802629, mean_q: 2.180767, mean_eps: 0.342435\n",
      "  732287/2000000: episode: 1064, duration: 26.832s, episode steps: 1454, steps per second:  54, episode reward: 28.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.015971, mae: 1.826414, mean_q: 2.211024, mean_eps: 0.341596\n",
      "  733184/2000000: episode: 1065, duration: 16.723s, episode steps: 897, steps per second:  54, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.016294, mae: 1.840764, mean_q: 2.227202, mean_eps: 0.340539\n",
      "  733827/2000000: episode: 1066, duration: 11.992s, episode steps: 643, steps per second:  54, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.778 [0.000, 5.000],  loss: 0.015671, mae: 1.830273, mean_q: 2.214512, mean_eps: 0.339846\n",
      "  734607/2000000: episode: 1067, duration: 14.304s, episode steps: 780, steps per second:  55, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.014675, mae: 1.817061, mean_q: 2.201016, mean_eps: 0.339206\n",
      "  735258/2000000: episode: 1068, duration: 11.864s, episode steps: 651, steps per second:  55, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.014049, mae: 1.836473, mean_q: 2.222658, mean_eps: 0.338561\n",
      "  736377/2000000: episode: 1069, duration: 20.260s, episode steps: 1119, steps per second:  55, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.014638, mae: 1.825023, mean_q: 2.209671, mean_eps: 0.337764\n",
      "  737398/2000000: episode: 1070, duration: 18.732s, episode steps: 1021, steps per second:  55, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.014298, mae: 1.842052, mean_q: 2.228623, mean_eps: 0.336801\n",
      "  737877/2000000: episode: 1071, duration: 8.860s, episode steps: 479, steps per second:  54, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.002 [0.000, 5.000],  loss: 0.015244, mae: 1.830210, mean_q: 2.217042, mean_eps: 0.336126\n",
      "  738229/2000000: episode: 1072, duration: 6.619s, episode steps: 352, steps per second:  53, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.347 [0.000, 5.000],  loss: 0.012415, mae: 1.812935, mean_q: 2.194463, mean_eps: 0.335751\n",
      "  738588/2000000: episode: 1073, duration: 6.581s, episode steps: 359, steps per second:  55, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.688 [0.000, 5.000],  loss: 0.013574, mae: 1.830444, mean_q: 2.218607, mean_eps: 0.335433\n",
      "  739607/2000000: episode: 1074, duration: 18.983s, episode steps: 1019, steps per second:  54, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.799 [0.000, 5.000],  loss: 0.014931, mae: 1.824882, mean_q: 2.211092, mean_eps: 0.334814\n",
      "  740246/2000000: episode: 1075, duration: 11.627s, episode steps: 639, steps per second:  55, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.013787, mae: 1.844597, mean_q: 2.233165, mean_eps: 0.334067\n",
      "  740606/2000000: episode: 1076, duration: 6.464s, episode steps: 360, steps per second:  56, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.013054, mae: 1.838810, mean_q: 2.227767, mean_eps: 0.333617\n",
      "  741103/2000000: episode: 1077, duration: 9.207s, episode steps: 497, steps per second:  54, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.276 [0.000, 5.000],  loss: 0.015443, mae: 1.849068, mean_q: 2.242006, mean_eps: 0.333231\n",
      "  742103/2000000: episode: 1078, duration: 18.283s, episode steps: 1000, steps per second:  55, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.015110, mae: 1.843638, mean_q: 2.231278, mean_eps: 0.332558\n",
      "  743174/2000000: episode: 1079, duration: 19.720s, episode steps: 1071, steps per second:  54, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.351 [0.000, 5.000],  loss: 0.015770, mae: 1.856313, mean_q: 2.248071, mean_eps: 0.331626\n",
      "  744204/2000000: episode: 1080, duration: 18.959s, episode steps: 1030, steps per second:  54, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.014909, mae: 1.838467, mean_q: 2.226931, mean_eps: 0.330681\n",
      "  744836/2000000: episode: 1081, duration: 11.861s, episode steps: 632, steps per second:  53, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.655 [0.000, 5.000],  loss: 0.014724, mae: 1.826148, mean_q: 2.210558, mean_eps: 0.329934\n",
      "  745339/2000000: episode: 1082, duration: 9.347s, episode steps: 503, steps per second:  54, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.805 [0.000, 5.000],  loss: 0.015582, mae: 1.842191, mean_q: 2.229151, mean_eps: 0.329423\n",
      "  746197/2000000: episode: 1083, duration: 16.135s, episode steps: 858, steps per second:  53, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.014367, mae: 1.853162, mean_q: 2.242914, mean_eps: 0.328809\n",
      "  747055/2000000: episode: 1084, duration: 16.000s, episode steps: 858, steps per second:  54, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.016345, mae: 1.848557, mean_q: 2.236091, mean_eps: 0.328037\n",
      "  747454/2000000: episode: 1085, duration: 7.197s, episode steps: 399, steps per second:  55, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.013258, mae: 1.867935, mean_q: 2.261149, mean_eps: 0.327471\n",
      "  747897/2000000: episode: 1086, duration: 8.448s, episode steps: 443, steps per second:  52, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.015608, mae: 1.823969, mean_q: 2.208309, mean_eps: 0.327092\n",
      "  748856/2000000: episode: 1087, duration: 17.420s, episode steps: 959, steps per second:  55, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.293 [0.000, 5.000],  loss: 0.014980, mae: 1.842212, mean_q: 2.229250, mean_eps: 0.326462\n",
      "  749503/2000000: episode: 1088, duration: 12.172s, episode steps: 647, steps per second:  53, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.971 [0.000, 5.000],  loss: 0.016297, mae: 1.843855, mean_q: 2.236150, mean_eps: 0.325740\n",
      "  750011/2000000: episode: 1089, duration: 9.173s, episode steps: 508, steps per second:  55, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.015838, mae: 1.833884, mean_q: 2.222138, mean_eps: 0.325220\n",
      "  750391/2000000: episode: 1090, duration: 7.094s, episode steps: 380, steps per second:  54, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.111 [0.000, 5.000],  loss: 0.015948, mae: 1.870767, mean_q: 2.269006, mean_eps: 0.324820\n",
      "  751179/2000000: episode: 1091, duration: 14.775s, episode steps: 788, steps per second:  53, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.014560, mae: 1.876585, mean_q: 2.272981, mean_eps: 0.324294\n",
      "  751664/2000000: episode: 1092, duration: 9.066s, episode steps: 485, steps per second:  53, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.016957, mae: 1.871344, mean_q: 2.265601, mean_eps: 0.323722\n",
      "  752074/2000000: episode: 1093, duration: 7.926s, episode steps: 410, steps per second:  52, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.905 [0.000, 5.000],  loss: 0.012783, mae: 1.847246, mean_q: 2.236749, mean_eps: 0.323319\n",
      "  752723/2000000: episode: 1094, duration: 12.140s, episode steps: 649, steps per second:  53, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.016666, mae: 1.889343, mean_q: 2.285877, mean_eps: 0.322842\n",
      "  753457/2000000: episode: 1095, duration: 13.375s, episode steps: 734, steps per second:  55, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.016244, mae: 1.880352, mean_q: 2.278190, mean_eps: 0.322219\n",
      "  754458/2000000: episode: 1096, duration: 18.554s, episode steps: 1001, steps per second:  54, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.014532, mae: 1.886912, mean_q: 2.286225, mean_eps: 0.321438\n",
      "  755248/2000000: episode: 1097, duration: 14.596s, episode steps: 790, steps per second:  54, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.771 [0.000, 5.000],  loss: 0.014843, mae: 1.889622, mean_q: 2.289442, mean_eps: 0.320633\n",
      "  755951/2000000: episode: 1098, duration: 13.423s, episode steps: 703, steps per second:  52, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.036 [0.000, 5.000],  loss: 0.014237, mae: 1.878255, mean_q: 2.275018, mean_eps: 0.319962\n",
      "  756465/2000000: episode: 1099, duration: 9.531s, episode steps: 514, steps per second:  54, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.765 [0.000, 5.000],  loss: 0.014290, mae: 1.854645, mean_q: 2.241497, mean_eps: 0.319413\n",
      "  756967/2000000: episode: 1100, duration: 8.961s, episode steps: 502, steps per second:  56, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.785 [0.000, 5.000],  loss: 0.014694, mae: 1.878999, mean_q: 2.275674, mean_eps: 0.318956\n",
      "  757409/2000000: episode: 1101, duration: 8.346s, episode steps: 442, steps per second:  53, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.882 [0.000, 5.000],  loss: 0.018057, mae: 1.863448, mean_q: 2.254643, mean_eps: 0.318531\n",
      "  758540/2000000: episode: 1102, duration: 21.050s, episode steps: 1131, steps per second:  54, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.449 [0.000, 5.000],  loss: 0.013881, mae: 1.876406, mean_q: 2.270722, mean_eps: 0.317823\n",
      "  758901/2000000: episode: 1103, duration: 6.929s, episode steps: 361, steps per second:  52, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.014804, mae: 1.876045, mean_q: 2.269850, mean_eps: 0.317152\n",
      "  760185/2000000: episode: 1104, duration: 23.897s, episode steps: 1284, steps per second:  54, episode reward: 21.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.015346, mae: 1.890606, mean_q: 2.287629, mean_eps: 0.316410\n",
      "  760773/2000000: episode: 1105, duration: 11.269s, episode steps: 588, steps per second:  52, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.733 [0.000, 5.000],  loss: 0.014127, mae: 1.888290, mean_q: 2.285762, mean_eps: 0.315568\n",
      "  761843/2000000: episode: 1106, duration: 20.090s, episode steps: 1070, steps per second:  53, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.015128, mae: 1.900440, mean_q: 2.298658, mean_eps: 0.314823\n",
      "  762258/2000000: episode: 1107, duration: 7.902s, episode steps: 415, steps per second:  53, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.848 [0.000, 5.000],  loss: 0.013943, mae: 1.882446, mean_q: 2.276022, mean_eps: 0.314155\n",
      "  763243/2000000: episode: 1108, duration: 18.156s, episode steps: 985, steps per second:  54, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.016053, mae: 1.892560, mean_q: 2.287309, mean_eps: 0.313525\n",
      "  764153/2000000: episode: 1109, duration: 17.166s, episode steps: 910, steps per second:  53, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.807 [0.000, 5.000],  loss: 0.015336, mae: 1.904937, mean_q: 2.305580, mean_eps: 0.312672\n",
      "  764486/2000000: episode: 1110, duration: 6.155s, episode steps: 333, steps per second:  54, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.676 [0.000, 5.000],  loss: 0.015967, mae: 1.904889, mean_q: 2.303483, mean_eps: 0.312112\n",
      "  765130/2000000: episode: 1111, duration: 12.286s, episode steps: 644, steps per second:  52, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.014927, mae: 1.887715, mean_q: 2.282983, mean_eps: 0.311673\n",
      "  765674/2000000: episode: 1112, duration: 10.525s, episode steps: 544, steps per second:  52, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.017966, mae: 1.921743, mean_q: 2.326546, mean_eps: 0.311138\n",
      "  766472/2000000: episode: 1113, duration: 14.608s, episode steps: 798, steps per second:  55, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.430 [0.000, 5.000],  loss: 0.016480, mae: 1.900069, mean_q: 2.301522, mean_eps: 0.310535\n",
      "  767535/2000000: episode: 1114, duration: 20.356s, episode steps: 1063, steps per second:  52, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.014610, mae: 1.891378, mean_q: 2.289968, mean_eps: 0.309698\n",
      "  768054/2000000: episode: 1115, duration: 9.373s, episode steps: 519, steps per second:  55, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.981 [0.000, 5.000],  loss: 0.016991, mae: 1.914347, mean_q: 2.316468, mean_eps: 0.308985\n",
      "  768715/2000000: episode: 1116, duration: 12.491s, episode steps: 661, steps per second:  53, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.016389, mae: 1.890317, mean_q: 2.286511, mean_eps: 0.308454\n",
      "  769951/2000000: episode: 1117, duration: 23.140s, episode steps: 1236, steps per second:  53, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.014867, mae: 1.893241, mean_q: 2.290245, mean_eps: 0.307601\n",
      "  770898/2000000: episode: 1118, duration: 17.932s, episode steps: 947, steps per second:  53, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.013730, mae: 1.935167, mean_q: 2.342072, mean_eps: 0.306618\n",
      "  771609/2000000: episode: 1119, duration: 13.298s, episode steps: 711, steps per second:  53, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.017761, mae: 1.937723, mean_q: 2.344385, mean_eps: 0.305871\n",
      "  772036/2000000: episode: 1120, duration: 8.196s, episode steps: 427, steps per second:  52, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.012569, mae: 1.914487, mean_q: 2.316407, mean_eps: 0.305360\n",
      "  773124/2000000: episode: 1121, duration: 20.625s, episode steps: 1088, steps per second:  53, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.871 [0.000, 5.000],  loss: 0.015365, mae: 1.920800, mean_q: 2.322349, mean_eps: 0.304680\n",
      "  773704/2000000: episode: 1122, duration: 11.090s, episode steps: 580, steps per second:  52, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.014181, mae: 1.928914, mean_q: 2.330758, mean_eps: 0.303929\n",
      "  774219/2000000: episode: 1123, duration: 9.646s, episode steps: 515, steps per second:  53, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.668 [0.000, 5.000],  loss: 0.015070, mae: 1.919895, mean_q: 2.324168, mean_eps: 0.303436\n",
      "  774981/2000000: episode: 1124, duration: 14.459s, episode steps: 762, steps per second:  53, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.808 [0.000, 5.000],  loss: 0.015986, mae: 1.925204, mean_q: 2.331282, mean_eps: 0.302860\n",
      "  775756/2000000: episode: 1125, duration: 14.625s, episode steps: 775, steps per second:  53, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.014709, mae: 1.928395, mean_q: 2.332493, mean_eps: 0.302169\n",
      "  776257/2000000: episode: 1126, duration: 9.733s, episode steps: 501, steps per second:  51, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.015942, mae: 1.953171, mean_q: 2.360868, mean_eps: 0.301595\n",
      "  776899/2000000: episode: 1127, duration: 12.484s, episode steps: 642, steps per second:  51, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.913 [0.000, 5.000],  loss: 0.015843, mae: 1.930918, mean_q: 2.333720, mean_eps: 0.301080\n",
      "  777791/2000000: episode: 1128, duration: 16.963s, episode steps: 892, steps per second:  53, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.017255, mae: 1.938015, mean_q: 2.343328, mean_eps: 0.300390\n",
      "  778508/2000000: episode: 1129, duration: 13.740s, episode steps: 717, steps per second:  52, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.015015, mae: 1.931518, mean_q: 2.335604, mean_eps: 0.299667\n",
      "  779259/2000000: episode: 1130, duration: 14.139s, episode steps: 751, steps per second:  53, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.014824, mae: 1.931857, mean_q: 2.337254, mean_eps: 0.299006\n",
      "  779929/2000000: episode: 1131, duration: 12.914s, episode steps: 670, steps per second:  52, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.014707, mae: 1.925981, mean_q: 2.331620, mean_eps: 0.298365\n",
      "  780466/2000000: episode: 1132, duration: 10.189s, episode steps: 537, steps per second:  53, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.013289, mae: 1.945708, mean_q: 2.357895, mean_eps: 0.297822\n",
      "  780979/2000000: episode: 1133, duration: 9.684s, episode steps: 513, steps per second:  53, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.013842, mae: 1.957102, mean_q: 2.368295, mean_eps: 0.297350\n",
      "  781587/2000000: episode: 1134, duration: 11.581s, episode steps: 608, steps per second:  52, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.015020, mae: 1.961809, mean_q: 2.374554, mean_eps: 0.296846\n",
      "  782301/2000000: episode: 1135, duration: 13.412s, episode steps: 714, steps per second:  53, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.015582, mae: 1.978684, mean_q: 2.394840, mean_eps: 0.296250\n",
      "  783337/2000000: episode: 1136, duration: 19.962s, episode steps: 1036, steps per second:  52, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.014924, mae: 1.956928, mean_q: 2.368188, mean_eps: 0.295462\n",
      "  783879/2000000: episode: 1137, duration: 10.217s, episode steps: 542, steps per second:  53, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.421 [0.000, 5.000],  loss: 0.014089, mae: 1.952132, mean_q: 2.359888, mean_eps: 0.294753\n",
      "  784542/2000000: episode: 1138, duration: 12.790s, episode steps: 663, steps per second:  52, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.015985, mae: 1.958606, mean_q: 2.367583, mean_eps: 0.294211\n",
      "  785427/2000000: episode: 1139, duration: 16.981s, episode steps: 885, steps per second:  52, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.827 [0.000, 5.000],  loss: 0.015230, mae: 1.962717, mean_q: 2.375092, mean_eps: 0.293514\n",
      "  785823/2000000: episode: 1140, duration: 7.780s, episode steps: 396, steps per second:  51, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.364 [0.000, 5.000],  loss: 0.012743, mae: 1.956827, mean_q: 2.368997, mean_eps: 0.292938\n",
      "  786611/2000000: episode: 1141, duration: 15.341s, episode steps: 788, steps per second:  51, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.015805, mae: 1.969307, mean_q: 2.382696, mean_eps: 0.292406\n",
      "  787275/2000000: episode: 1142, duration: 12.651s, episode steps: 664, steps per second:  52, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.014811, mae: 1.952919, mean_q: 2.363946, mean_eps: 0.291752\n",
      "  787878/2000000: episode: 1143, duration: 11.669s, episode steps: 603, steps per second:  52, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.017849, mae: 1.981126, mean_q: 2.396701, mean_eps: 0.291182\n",
      "  788445/2000000: episode: 1144, duration: 10.865s, episode steps: 567, steps per second:  52, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.013909, mae: 1.974306, mean_q: 2.388170, mean_eps: 0.290654\n",
      "  789061/2000000: episode: 1145, duration: 11.883s, episode steps: 616, steps per second:  52, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.015861, mae: 1.964754, mean_q: 2.376033, mean_eps: 0.290121\n",
      "  789955/2000000: episode: 1146, duration: 17.341s, episode steps: 894, steps per second:  52, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.015351, mae: 1.964233, mean_q: 2.376825, mean_eps: 0.289443\n",
      "  790656/2000000: episode: 1147, duration: 13.844s, episode steps: 701, steps per second:  51, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.014439, mae: 1.944812, mean_q: 2.355898, mean_eps: 0.288726\n",
      "  791375/2000000: episode: 1148, duration: 13.884s, episode steps: 719, steps per second:  52, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.014470, mae: 1.953393, mean_q: 2.363572, mean_eps: 0.288087\n",
      "  792162/2000000: episode: 1149, duration: 15.529s, episode steps: 787, steps per second:  51, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.015510, mae: 1.960360, mean_q: 2.372682, mean_eps: 0.287409\n",
      "  793466/2000000: episode: 1150, duration: 25.029s, episode steps: 1304, steps per second:  52, episode reward: 22.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.015403, mae: 1.969617, mean_q: 2.384538, mean_eps: 0.286467\n",
      "  794104/2000000: episode: 1151, duration: 12.335s, episode steps: 638, steps per second:  52, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.541 [0.000, 5.000],  loss: 0.015717, mae: 1.946080, mean_q: 2.354997, mean_eps: 0.285594\n",
      "  795058/2000000: episode: 1152, duration: 18.323s, episode steps: 954, steps per second:  52, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.827 [0.000, 5.000],  loss: 0.016752, mae: 1.964674, mean_q: 2.376500, mean_eps: 0.284878\n",
      "  796348/2000000: episode: 1153, duration: 25.636s, episode steps: 1290, steps per second:  50, episode reward: 34.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.015978, mae: 1.948942, mean_q: 2.356114, mean_eps: 0.283868\n",
      "  797323/2000000: episode: 1154, duration: 19.341s, episode steps: 975, steps per second:  50, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.014420, mae: 1.964436, mean_q: 2.374803, mean_eps: 0.282849\n",
      "  797974/2000000: episode: 1155, duration: 12.437s, episode steps: 651, steps per second:  52, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.593 [0.000, 5.000],  loss: 0.018259, mae: 1.967801, mean_q: 2.377695, mean_eps: 0.282117\n",
      "  798485/2000000: episode: 1156, duration: 10.209s, episode steps: 511, steps per second:  50, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.016258, mae: 1.984835, mean_q: 2.402148, mean_eps: 0.281593\n",
      "  799388/2000000: episode: 1157, duration: 17.434s, episode steps: 903, steps per second:  52, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.604 [0.000, 5.000],  loss: 0.015028, mae: 1.954766, mean_q: 2.367129, mean_eps: 0.280958\n",
      "  799982/2000000: episode: 1158, duration: 11.827s, episode steps: 594, steps per second:  50, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.018913, mae: 1.970308, mean_q: 2.384521, mean_eps: 0.280284\n",
      "  801010/2000000: episode: 1159, duration: 19.870s, episode steps: 1028, steps per second:  52, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.014843, mae: 1.989700, mean_q: 2.410535, mean_eps: 0.279554\n",
      "  802020/2000000: episode: 1160, duration: 19.703s, episode steps: 1010, steps per second:  51, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.014483, mae: 1.981909, mean_q: 2.398770, mean_eps: 0.278637\n",
      "  802505/2000000: episode: 1161, duration: 9.570s, episode steps: 485, steps per second:  51, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.959 [0.000, 5.000],  loss: 0.015163, mae: 1.990838, mean_q: 2.408060, mean_eps: 0.277964\n",
      "  803183/2000000: episode: 1162, duration: 13.629s, episode steps: 678, steps per second:  50, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.720 [0.000, 5.000],  loss: 0.016664, mae: 1.978184, mean_q: 2.396039, mean_eps: 0.277440\n",
      "  803806/2000000: episode: 1163, duration: 11.930s, episode steps: 623, steps per second:  52, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.014430, mae: 2.007341, mean_q: 2.430011, mean_eps: 0.276855\n",
      "  804176/2000000: episode: 1164, duration: 7.443s, episode steps: 370, steps per second:  50, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.100 [0.000, 5.000],  loss: 0.018164, mae: 1.996308, mean_q: 2.414599, mean_eps: 0.276409\n",
      "  804728/2000000: episode: 1165, duration: 10.866s, episode steps: 552, steps per second:  51, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.016500, mae: 1.987390, mean_q: 2.404559, mean_eps: 0.275995\n",
      "  805734/2000000: episode: 1166, duration: 19.693s, episode steps: 1006, steps per second:  51, episode reward: 16.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.014998, mae: 1.982040, mean_q: 2.402751, mean_eps: 0.275293\n",
      "  807247/2000000: episode: 1167, duration: 29.739s, episode steps: 1513, steps per second:  51, episode reward: 30.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.016640, mae: 1.986353, mean_q: 2.403990, mean_eps: 0.274159\n",
      "  807939/2000000: episode: 1168, duration: 13.537s, episode steps: 692, steps per second:  51, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.015963, mae: 1.987834, mean_q: 2.405743, mean_eps: 0.273167\n",
      "  808851/2000000: episode: 1169, duration: 18.175s, episode steps: 912, steps per second:  50, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.016291, mae: 1.971251, mean_q: 2.385432, mean_eps: 0.272445\n",
      "  809555/2000000: episode: 1170, duration: 13.922s, episode steps: 704, steps per second:  51, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.014154, mae: 1.985374, mean_q: 2.402381, mean_eps: 0.271718\n",
      "  810613/2000000: episode: 1171, duration: 21.089s, episode steps: 1058, steps per second:  50, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.014918, mae: 2.005800, mean_q: 2.427061, mean_eps: 0.270924\n",
      "  811276/2000000: episode: 1172, duration: 12.907s, episode steps: 663, steps per second:  51, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.015658, mae: 1.994687, mean_q: 2.412669, mean_eps: 0.270150\n",
      "  812069/2000000: episode: 1173, duration: 15.360s, episode steps: 793, steps per second:  52, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.016593, mae: 1.993214, mean_q: 2.410377, mean_eps: 0.269495\n",
      "  812960/2000000: episode: 1174, duration: 17.320s, episode steps: 891, steps per second:  51, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.017243, mae: 2.034185, mean_q: 2.460846, mean_eps: 0.268737\n",
      "  814218/2000000: episode: 1175, duration: 24.629s, episode steps: 1258, steps per second:  51, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.015485, mae: 2.011307, mean_q: 2.433881, mean_eps: 0.267771\n",
      "  814838/2000000: episode: 1176, duration: 12.134s, episode steps: 620, steps per second:  51, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.887 [0.000, 5.000],  loss: 0.016612, mae: 2.034471, mean_q: 2.461343, mean_eps: 0.266925\n",
      "  815530/2000000: episode: 1177, duration: 13.941s, episode steps: 692, steps per second:  50, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.015005, mae: 2.020716, mean_q: 2.444752, mean_eps: 0.266334\n",
      "  816502/2000000: episode: 1178, duration: 19.407s, episode steps: 972, steps per second:  50, episode reward: 14.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.466 [0.000, 5.000],  loss: 0.016364, mae: 2.001605, mean_q: 2.419113, mean_eps: 0.265586\n",
      "  817196/2000000: episode: 1179, duration: 13.722s, episode steps: 694, steps per second:  51, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.016413, mae: 2.010406, mean_q: 2.431250, mean_eps: 0.264837\n",
      "  817701/2000000: episode: 1180, duration: 9.778s, episode steps: 505, steps per second:  52, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.014091, mae: 2.033925, mean_q: 2.458925, mean_eps: 0.264297\n",
      "  818136/2000000: episode: 1181, duration: 8.661s, episode steps: 435, steps per second:  50, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.924 [0.000, 5.000],  loss: 0.017727, mae: 2.025185, mean_q: 2.449016, mean_eps: 0.263874\n",
      "  819056/2000000: episode: 1182, duration: 17.819s, episode steps: 920, steps per second:  52, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.141 [0.000, 5.000],  loss: 0.016806, mae: 2.015951, mean_q: 2.437236, mean_eps: 0.263265\n",
      "  819723/2000000: episode: 1183, duration: 13.373s, episode steps: 667, steps per second:  50, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.649 [0.000, 5.000],  loss: 0.015811, mae: 2.027358, mean_q: 2.452145, mean_eps: 0.262551\n",
      "  820505/2000000: episode: 1184, duration: 15.510s, episode steps: 782, steps per second:  50, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.052 [0.000, 5.000],  loss: 0.014525, mae: 2.021573, mean_q: 2.446833, mean_eps: 0.261897\n",
      "  821521/2000000: episode: 1185, duration: 20.453s, episode steps: 1016, steps per second:  50, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.014955, mae: 2.057228, mean_q: 2.487487, mean_eps: 0.261087\n",
      "  822394/2000000: episode: 1186, duration: 17.028s, episode steps: 873, steps per second:  51, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.598 [0.000, 5.000],  loss: 0.014850, mae: 2.041354, mean_q: 2.468353, mean_eps: 0.260238\n",
      "  823118/2000000: episode: 1187, duration: 15.773s, episode steps: 724, steps per second:  46, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.015222, mae: 2.053362, mean_q: 2.482604, mean_eps: 0.259520\n",
      "  823939/2000000: episode: 1188, duration: 16.730s, episode steps: 821, steps per second:  49, episode reward: 23.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.015562, mae: 2.057097, mean_q: 2.488085, mean_eps: 0.258825\n",
      "  824748/2000000: episode: 1189, duration: 16.022s, episode steps: 809, steps per second:  50, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.017015, mae: 2.044018, mean_q: 2.470874, mean_eps: 0.258092\n",
      "  825502/2000000: episode: 1190, duration: 14.875s, episode steps: 754, steps per second:  51, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.809 [0.000, 5.000],  loss: 0.016358, mae: 2.065832, mean_q: 2.497238, mean_eps: 0.257388\n",
      "  826162/2000000: episode: 1191, duration: 13.142s, episode steps: 660, steps per second:  50, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.017573, mae: 2.043697, mean_q: 2.468664, mean_eps: 0.256751\n",
      "  827166/2000000: episode: 1192, duration: 20.249s, episode steps: 1004, steps per second:  50, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.151 [0.000, 5.000],  loss: 0.017871, mae: 2.053073, mean_q: 2.483115, mean_eps: 0.256002\n",
      "  828013/2000000: episode: 1193, duration: 16.683s, episode steps: 847, steps per second:  51, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.015771, mae: 2.060436, mean_q: 2.492318, mean_eps: 0.255169\n",
      "  828649/2000000: episode: 1194, duration: 12.865s, episode steps: 636, steps per second:  49, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.017685, mae: 2.055347, mean_q: 2.484933, mean_eps: 0.254501\n",
      "  829528/2000000: episode: 1195, duration: 17.230s, episode steps: 879, steps per second:  51, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.015380, mae: 2.059229, mean_q: 2.488268, mean_eps: 0.253821\n",
      "  830411/2000000: episode: 1196, duration: 18.028s, episode steps: 883, steps per second:  49, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.766 [0.000, 5.000],  loss: 0.016090, mae: 2.072310, mean_q: 2.506082, mean_eps: 0.253029\n",
      "  831249/2000000: episode: 1197, duration: 16.893s, episode steps: 838, steps per second:  50, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.016221, mae: 2.059699, mean_q: 2.491765, mean_eps: 0.252253\n",
      "  831958/2000000: episode: 1198, duration: 14.430s, episode steps: 709, steps per second:  49, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.014934, mae: 2.078217, mean_q: 2.514113, mean_eps: 0.251556\n",
      "  832866/2000000: episode: 1199, duration: 18.322s, episode steps: 908, steps per second:  50, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.016092, mae: 2.054417, mean_q: 2.485819, mean_eps: 0.250829\n",
      "  834021/2000000: episode: 1200, duration: 23.682s, episode steps: 1155, steps per second:  49, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.019239, mae: 2.085340, mean_q: 2.524412, mean_eps: 0.249900\n",
      "  834645/2000000: episode: 1201, duration: 12.779s, episode steps: 624, steps per second:  49, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.016680, mae: 2.067297, mean_q: 2.502105, mean_eps: 0.249099\n",
      "  835494/2000000: episode: 1202, duration: 17.248s, episode steps: 849, steps per second:  49, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.737 [0.000, 5.000],  loss: 0.016499, mae: 2.046016, mean_q: 2.472641, mean_eps: 0.248437\n",
      "  836136/2000000: episode: 1203, duration: 13.035s, episode steps: 642, steps per second:  49, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.015096, mae: 2.099699, mean_q: 2.541379, mean_eps: 0.247767\n",
      "  837137/2000000: episode: 1204, duration: 20.380s, episode steps: 1001, steps per second:  49, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.017488, mae: 2.083145, mean_q: 2.516887, mean_eps: 0.247028\n",
      "  837883/2000000: episode: 1205, duration: 15.138s, episode steps: 746, steps per second:  49, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.016382, mae: 2.070599, mean_q: 2.502835, mean_eps: 0.246241\n",
      "  839639/2000000: episode: 1206, duration: 35.296s, episode steps: 1756, steps per second:  50, episode reward: 32.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.039 [0.000, 5.000],  loss: 0.016547, mae: 2.075945, mean_q: 2.509851, mean_eps: 0.245116\n",
      "  840361/2000000: episode: 1207, duration: 14.804s, episode steps: 722, steps per second:  49, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.889 [0.000, 5.000],  loss: 0.015463, mae: 2.088367, mean_q: 2.526930, mean_eps: 0.244000\n",
      "  840893/2000000: episode: 1208, duration: 11.091s, episode steps: 532, steps per second:  48, episode reward: 14.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.017155, mae: 2.117329, mean_q: 2.560021, mean_eps: 0.243435\n",
      "  841488/2000000: episode: 1209, duration: 11.820s, episode steps: 595, steps per second:  50, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.016104, mae: 2.088838, mean_q: 2.528930, mean_eps: 0.242929\n",
      "  842225/2000000: episode: 1210, duration: 15.082s, episode steps: 737, steps per second:  49, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.019977, mae: 2.098461, mean_q: 2.537284, mean_eps: 0.242330\n",
      "  843421/2000000: episode: 1211, duration: 23.915s, episode steps: 1196, steps per second:  50, episode reward: 18.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.273 [0.000, 5.000],  loss: 0.015484, mae: 2.092359, mean_q: 2.531720, mean_eps: 0.241458\n",
      "  844485/2000000: episode: 1212, duration: 21.083s, episode steps: 1064, steps per second:  50, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.922 [0.000, 5.000],  loss: 0.015138, mae: 2.075448, mean_q: 2.509344, mean_eps: 0.240441\n",
      "  845181/2000000: episode: 1213, duration: 14.069s, episode steps: 696, steps per second:  49, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.016313, mae: 2.083170, mean_q: 2.518031, mean_eps: 0.239649\n",
      "  845985/2000000: episode: 1214, duration: 15.887s, episode steps: 804, steps per second:  51, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.252 [0.000, 5.000],  loss: 0.017041, mae: 2.083529, mean_q: 2.520246, mean_eps: 0.238974\n",
      "  846610/2000000: episode: 1215, duration: 12.557s, episode steps: 625, steps per second:  50, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.637 [0.000, 5.000],  loss: 0.013648, mae: 2.093884, mean_q: 2.531952, mean_eps: 0.238332\n",
      "  847147/2000000: episode: 1216, duration: 10.736s, episode steps: 537, steps per second:  50, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.017634, mae: 2.097366, mean_q: 2.533224, mean_eps: 0.237810\n",
      "  848216/2000000: episode: 1217, duration: 21.324s, episode steps: 1069, steps per second:  50, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.016153, mae: 2.116694, mean_q: 2.557086, mean_eps: 0.237088\n",
      "  848857/2000000: episode: 1218, duration: 12.691s, episode steps: 641, steps per second:  51, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.015731, mae: 2.078538, mean_q: 2.511403, mean_eps: 0.236318\n",
      "  849375/2000000: episode: 1219, duration: 10.258s, episode steps: 518, steps per second:  50, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.247 [0.000, 5.000],  loss: 0.016081, mae: 2.105773, mean_q: 2.546443, mean_eps: 0.235796\n",
      "  850527/2000000: episode: 1220, duration: 23.110s, episode steps: 1152, steps per second:  50, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.018243, mae: 2.104679, mean_q: 2.544450, mean_eps: 0.235045\n",
      "  852230/2000000: episode: 1221, duration: 34.071s, episode steps: 1703, steps per second:  50, episode reward: 33.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.017196, mae: 2.115685, mean_q: 2.558530, mean_eps: 0.233760\n",
      "  852766/2000000: episode: 1222, duration: 11.119s, episode steps: 536, steps per second:  48, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.271 [0.000, 5.000],  loss: 0.019995, mae: 2.132625, mean_q: 2.578746, mean_eps: 0.232752\n",
      "  853457/2000000: episode: 1223, duration: 13.719s, episode steps: 691, steps per second:  50, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.505 [0.000, 5.000],  loss: 0.015905, mae: 2.130231, mean_q: 2.574403, mean_eps: 0.232199\n",
      "  854232/2000000: episode: 1224, duration: 15.720s, episode steps: 775, steps per second:  49, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.015551, mae: 2.088708, mean_q: 2.526086, mean_eps: 0.231540\n",
      "  854877/2000000: episode: 1225, duration: 12.709s, episode steps: 645, steps per second:  51, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.800 [0.000, 5.000],  loss: 0.015055, mae: 2.117094, mean_q: 2.560924, mean_eps: 0.230901\n",
      "  855446/2000000: episode: 1226, duration: 11.433s, episode steps: 569, steps per second:  50, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.243 [0.000, 5.000],  loss: 0.018246, mae: 2.102502, mean_q: 2.540279, mean_eps: 0.230354\n",
      "  856598/2000000: episode: 1227, duration: 22.896s, episode steps: 1152, steps per second:  50, episode reward: 16.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.018029, mae: 2.121977, mean_q: 2.566869, mean_eps: 0.229580\n",
      "  857569/2000000: episode: 1228, duration: 19.795s, episode steps: 971, steps per second:  49, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.766 [0.000, 5.000],  loss: 0.015660, mae: 2.122877, mean_q: 2.567049, mean_eps: 0.228624\n",
      "  858477/2000000: episode: 1229, duration: 18.304s, episode steps: 908, steps per second:  50, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.015092, mae: 2.103061, mean_q: 2.542090, mean_eps: 0.227778\n",
      "  858977/2000000: episode: 1230, duration: 10.154s, episode steps: 500, steps per second:  49, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.924 [0.000, 5.000],  loss: 0.014596, mae: 2.102908, mean_q: 2.542805, mean_eps: 0.227145\n",
      "  859910/2000000: episode: 1231, duration: 18.895s, episode steps: 933, steps per second:  49, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.015921, mae: 2.114839, mean_q: 2.560116, mean_eps: 0.226500\n",
      "  860833/2000000: episode: 1232, duration: 18.599s, episode steps: 923, steps per second:  50, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.015717, mae: 2.114946, mean_q: 2.555812, mean_eps: 0.225665\n",
      "  861490/2000000: episode: 1233, duration: 13.126s, episode steps: 657, steps per second:  50, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.016652, mae: 2.129238, mean_q: 2.580070, mean_eps: 0.224954\n",
      "  861954/2000000: episode: 1234, duration: 9.379s, episode steps: 464, steps per second:  49, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.873 [0.000, 5.000],  loss: 0.018344, mae: 2.092842, mean_q: 2.530853, mean_eps: 0.224450\n",
      "  862737/2000000: episode: 1235, duration: 15.899s, episode steps: 783, steps per second:  49, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.049 [0.000, 5.000],  loss: 0.016454, mae: 2.123424, mean_q: 2.568274, mean_eps: 0.223889\n",
      "  863716/2000000: episode: 1236, duration: 19.962s, episode steps: 979, steps per second:  49, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.016587, mae: 2.117723, mean_q: 2.557902, mean_eps: 0.223097\n",
      "  864819/2000000: episode: 1237, duration: 22.679s, episode steps: 1103, steps per second:  49, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.024 [0.000, 5.000],  loss: 0.015436, mae: 2.116983, mean_q: 2.558994, mean_eps: 0.222161\n",
      "  865862/2000000: episode: 1238, duration: 21.453s, episode steps: 1043, steps per second:  49, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.015605, mae: 2.102179, mean_q: 2.540353, mean_eps: 0.221194\n",
      "  866684/2000000: episode: 1239, duration: 16.530s, episode steps: 822, steps per second:  50, episode reward: 11.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.015539, mae: 2.112062, mean_q: 2.553356, mean_eps: 0.220355\n",
      "  867687/2000000: episode: 1240, duration: 20.616s, episode steps: 1003, steps per second:  49, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.017865, mae: 2.126720, mean_q: 2.568824, mean_eps: 0.219534\n",
      "  868620/2000000: episode: 1241, duration: 19.056s, episode steps: 933, steps per second:  49, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.018225, mae: 2.135633, mean_q: 2.582962, mean_eps: 0.218663\n",
      "  869732/2000000: episode: 1242, duration: 22.874s, episode steps: 1112, steps per second:  49, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.016823, mae: 2.123935, mean_q: 2.567883, mean_eps: 0.217743\n",
      "  870103/2000000: episode: 1243, duration: 7.584s, episode steps: 371, steps per second:  49, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.968 [0.000, 5.000],  loss: 0.015477, mae: 2.105651, mean_q: 2.546337, mean_eps: 0.217076\n",
      "  871081/2000000: episode: 1244, duration: 20.204s, episode steps: 978, steps per second:  48, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.800 [0.000, 5.000],  loss: 0.015476, mae: 2.126643, mean_q: 2.570903, mean_eps: 0.216467\n",
      "  872137/2000000: episode: 1245, duration: 21.638s, episode steps: 1056, steps per second:  49, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.015534, mae: 2.154464, mean_q: 2.606174, mean_eps: 0.215551\n",
      "  873230/2000000: episode: 1246, duration: 22.514s, episode steps: 1093, steps per second:  49, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.016444, mae: 2.131566, mean_q: 2.576860, mean_eps: 0.214584\n",
      "  873946/2000000: episode: 1247, duration: 14.507s, episode steps: 716, steps per second:  49, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.113 [0.000, 5.000],  loss: 0.015953, mae: 2.175313, mean_q: 2.630354, mean_eps: 0.213771\n",
      "  874774/2000000: episode: 1248, duration: 17.059s, episode steps: 828, steps per second:  49, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.015886, mae: 2.139841, mean_q: 2.585615, mean_eps: 0.213076\n",
      "  875339/2000000: episode: 1249, duration: 11.484s, episode steps: 565, steps per second:  49, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.016145, mae: 2.150520, mean_q: 2.597016, mean_eps: 0.212450\n",
      "  875811/2000000: episode: 1250, duration: 9.787s, episode steps: 472, steps per second:  48, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.744 [0.000, 5.000],  loss: 0.016010, mae: 2.141317, mean_q: 2.585947, mean_eps: 0.211983\n",
      "  876495/2000000: episode: 1251, duration: 14.315s, episode steps: 684, steps per second:  48, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.020056, mae: 2.152054, mean_q: 2.601929, mean_eps: 0.211463\n",
      "  876982/2000000: episode: 1252, duration: 9.900s, episode steps: 487, steps per second:  49, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.016571, mae: 2.144751, mean_q: 2.592963, mean_eps: 0.210936\n",
      "  877469/2000000: episode: 1253, duration: 10.160s, episode steps: 487, steps per second:  48, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.520 [0.000, 5.000],  loss: 0.016402, mae: 2.137726, mean_q: 2.583083, mean_eps: 0.210497\n",
      "  878614/2000000: episode: 1254, duration: 22.955s, episode steps: 1145, steps per second:  50, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.017762, mae: 2.138401, mean_q: 2.581943, mean_eps: 0.209762\n",
      "  879381/2000000: episode: 1255, duration: 15.679s, episode steps: 767, steps per second:  49, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.554 [0.000, 5.000],  loss: 0.020330, mae: 2.142971, mean_q: 2.587395, mean_eps: 0.208902\n",
      "  880091/2000000: episode: 1256, duration: 14.471s, episode steps: 710, steps per second:  49, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.937 [0.000, 5.000],  loss: 0.017748, mae: 2.134853, mean_q: 2.578857, mean_eps: 0.208238\n",
      "  880690/2000000: episode: 1257, duration: 12.333s, episode steps: 599, steps per second:  49, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.760 [0.000, 5.000],  loss: 0.014938, mae: 2.151489, mean_q: 2.601310, mean_eps: 0.207649\n",
      "  881255/2000000: episode: 1258, duration: 11.651s, episode steps: 565, steps per second:  48, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.016407, mae: 2.156350, mean_q: 2.606461, mean_eps: 0.207125\n",
      "  882021/2000000: episode: 1259, duration: 15.955s, episode steps: 766, steps per second:  48, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.016702, mae: 2.170640, mean_q: 2.624563, mean_eps: 0.206526\n",
      "  882789/2000000: episode: 1260, duration: 15.864s, episode steps: 768, steps per second:  48, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.016813, mae: 2.160568, mean_q: 2.610716, mean_eps: 0.205835\n",
      "  883291/2000000: episode: 1261, duration: 10.013s, episode steps: 502, steps per second:  50, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.647 [0.000, 5.000],  loss: 0.017121, mae: 2.187572, mean_q: 2.642320, mean_eps: 0.205264\n",
      "  883931/2000000: episode: 1262, duration: 13.172s, episode steps: 640, steps per second:  49, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.017201, mae: 2.164734, mean_q: 2.616191, mean_eps: 0.204751\n",
      "  884551/2000000: episode: 1263, duration: 12.797s, episode steps: 620, steps per second:  48, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.019177, mae: 2.165955, mean_q: 2.620044, mean_eps: 0.204184\n",
      "  885308/2000000: episode: 1264, duration: 15.834s, episode steps: 757, steps per second:  48, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.015733, mae: 2.173000, mean_q: 2.624346, mean_eps: 0.203565\n",
      "  886131/2000000: episode: 1265, duration: 16.722s, episode steps: 823, steps per second:  49, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.016820, mae: 2.167503, mean_q: 2.622093, mean_eps: 0.202854\n",
      "  887059/2000000: episode: 1266, duration: 19.161s, episode steps: 928, steps per second:  48, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.908 [0.000, 5.000],  loss: 0.016438, mae: 2.175798, mean_q: 2.629660, mean_eps: 0.202065\n",
      "  887695/2000000: episode: 1267, duration: 13.239s, episode steps: 636, steps per second:  48, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.093 [0.000, 5.000],  loss: 0.017632, mae: 2.167908, mean_q: 2.622234, mean_eps: 0.201362\n",
      "  888604/2000000: episode: 1268, duration: 18.872s, episode steps: 909, steps per second:  48, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.016175, mae: 2.161509, mean_q: 2.614912, mean_eps: 0.200667\n",
      "  889288/2000000: episode: 1269, duration: 14.437s, episode steps: 684, steps per second:  47, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.019 [0.000, 5.000],  loss: 0.016467, mae: 2.165364, mean_q: 2.618075, mean_eps: 0.199950\n",
      "  890205/2000000: episode: 1270, duration: 18.754s, episode steps: 917, steps per second:  49, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.144 [0.000, 5.000],  loss: 0.018082, mae: 2.179171, mean_q: 2.635439, mean_eps: 0.199229\n",
      "  891135/2000000: episode: 1271, duration: 19.457s, episode steps: 930, steps per second:  48, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.019230, mae: 2.176788, mean_q: 2.635250, mean_eps: 0.198397\n",
      "  891912/2000000: episode: 1272, duration: 16.124s, episode steps: 777, steps per second:  48, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.798 [0.000, 5.000],  loss: 0.015877, mae: 2.179276, mean_q: 2.636021, mean_eps: 0.197630\n",
      "  892916/2000000: episode: 1273, duration: 20.968s, episode steps: 1004, steps per second:  48, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.792 [0.000, 5.000],  loss: 0.016332, mae: 2.178141, mean_q: 2.632022, mean_eps: 0.196829\n",
      "  894125/2000000: episode: 1274, duration: 25.195s, episode steps: 1209, steps per second:  48, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.018161, mae: 2.182850, mean_q: 2.639342, mean_eps: 0.195832\n",
      "  895033/2000000: episode: 1275, duration: 19.150s, episode steps: 908, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.016720, mae: 2.172023, mean_q: 2.625405, mean_eps: 0.194878\n",
      "  895806/2000000: episode: 1276, duration: 16.122s, episode steps: 773, steps per second:  48, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.018674, mae: 2.203589, mean_q: 2.663543, mean_eps: 0.194122\n",
      "  896672/2000000: episode: 1277, duration: 18.088s, episode steps: 866, steps per second:  48, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.016700, mae: 2.185765, mean_q: 2.642055, mean_eps: 0.193386\n",
      "  897642/2000000: episode: 1278, duration: 20.059s, episode steps: 970, steps per second:  48, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.017321, mae: 2.177010, mean_q: 2.632354, mean_eps: 0.192560\n",
      "  898826/2000000: episode: 1279, duration: 24.786s, episode steps: 1184, steps per second:  48, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.069 [0.000, 5.000],  loss: 0.017782, mae: 2.183777, mean_q: 2.638154, mean_eps: 0.191589\n",
      "  899548/2000000: episode: 1280, duration: 15.265s, episode steps: 722, steps per second:  47, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.431 [0.000, 5.000],  loss: 0.019067, mae: 2.183790, mean_q: 2.639356, mean_eps: 0.190733\n",
      "  900308/2000000: episode: 1281, duration: 15.800s, episode steps: 760, steps per second:  48, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.153 [0.000, 5.000],  loss: 0.018342, mae: 2.191038, mean_q: 2.651446, mean_eps: 0.190067\n",
      "  901314/2000000: episode: 1282, duration: 21.264s, episode steps: 1006, steps per second:  47, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.018232, mae: 2.191560, mean_q: 2.650012, mean_eps: 0.189271\n",
      "  902397/2000000: episode: 1283, duration: 22.514s, episode steps: 1083, steps per second:  48, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.016493, mae: 2.204021, mean_q: 2.666423, mean_eps: 0.188330\n",
      "  903125/2000000: episode: 1284, duration: 14.954s, episode steps: 728, steps per second:  49, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.788 [0.000, 5.000],  loss: 0.014692, mae: 2.191589, mean_q: 2.654823, mean_eps: 0.187514\n",
      "  904035/2000000: episode: 1285, duration: 18.978s, episode steps: 910, steps per second:  48, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.870 [0.000, 5.000],  loss: 0.017433, mae: 2.205012, mean_q: 2.665236, mean_eps: 0.186778\n",
      "  904889/2000000: episode: 1286, duration: 18.040s, episode steps: 854, steps per second:  47, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.019583, mae: 2.206515, mean_q: 2.666281, mean_eps: 0.185984\n",
      "  906408/2000000: episode: 1287, duration: 31.855s, episode steps: 1519, steps per second:  48, episode reward: 28.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.018955, mae: 2.210431, mean_q: 2.671477, mean_eps: 0.184917\n",
      "  907905/2000000: episode: 1288, duration: 31.283s, episode steps: 1497, steps per second:  48, episode reward: 31.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.404 [0.000, 5.000],  loss: 0.017921, mae: 2.210428, mean_q: 2.673199, mean_eps: 0.183560\n",
      "  909299/2000000: episode: 1289, duration: 28.927s, episode steps: 1394, steps per second:  48, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.018415, mae: 2.198851, mean_q: 2.657581, mean_eps: 0.182258\n",
      "  910626/2000000: episode: 1290, duration: 28.238s, episode steps: 1327, steps per second:  47, episode reward: 27.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.016966, mae: 2.215175, mean_q: 2.678092, mean_eps: 0.181034\n",
      "  911353/2000000: episode: 1291, duration: 15.749s, episode steps: 727, steps per second:  46, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.017211, mae: 2.231493, mean_q: 2.698218, mean_eps: 0.180109\n",
      "  912293/2000000: episode: 1292, duration: 20.080s, episode steps: 940, steps per second:  47, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.017955, mae: 2.243259, mean_q: 2.709913, mean_eps: 0.179358\n",
      "  912837/2000000: episode: 1293, duration: 11.511s, episode steps: 544, steps per second:  47, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.796 [0.000, 5.000],  loss: 0.016707, mae: 2.242786, mean_q: 2.708005, mean_eps: 0.178691\n",
      "  913675/2000000: episode: 1294, duration: 17.902s, episode steps: 838, steps per second:  47, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.018065, mae: 2.218342, mean_q: 2.682555, mean_eps: 0.178070\n",
      "  914359/2000000: episode: 1295, duration: 14.585s, episode steps: 684, steps per second:  47, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.183 [0.000, 5.000],  loss: 0.017188, mae: 2.227481, mean_q: 2.693164, mean_eps: 0.177386\n",
      "  914899/2000000: episode: 1296, duration: 11.521s, episode steps: 540, steps per second:  47, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.115 [0.000, 5.000],  loss: 0.016751, mae: 2.211034, mean_q: 2.672826, mean_eps: 0.176835\n",
      "  915513/2000000: episode: 1297, duration: 13.078s, episode steps: 614, steps per second:  47, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.018389, mae: 2.220483, mean_q: 2.682154, mean_eps: 0.176315\n",
      "  916030/2000000: episode: 1298, duration: 10.769s, episode steps: 517, steps per second:  48, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.990 [0.000, 5.000],  loss: 0.018499, mae: 2.257029, mean_q: 2.727357, mean_eps: 0.175805\n",
      "  916956/2000000: episode: 1299, duration: 19.770s, episode steps: 926, steps per second:  47, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.018051, mae: 2.239873, mean_q: 2.708117, mean_eps: 0.175157\n",
      "  917718/2000000: episode: 1300, duration: 15.953s, episode steps: 762, steps per second:  48, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.018229, mae: 2.250105, mean_q: 2.720147, mean_eps: 0.174398\n",
      "  918669/2000000: episode: 1301, duration: 20.467s, episode steps: 951, steps per second:  46, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.739 [0.000, 5.000],  loss: 0.018129, mae: 2.227849, mean_q: 2.691470, mean_eps: 0.173625\n",
      "  919499/2000000: episode: 1302, duration: 17.784s, episode steps: 830, steps per second:  47, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.018349, mae: 2.232643, mean_q: 2.696888, mean_eps: 0.172824\n",
      "  920400/2000000: episode: 1303, duration: 18.939s, episode steps: 901, steps per second:  48, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.018542, mae: 2.236203, mean_q: 2.704483, mean_eps: 0.172047\n",
      "  921363/2000000: episode: 1304, duration: 20.511s, episode steps: 963, steps per second:  47, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.053 [0.000, 5.000],  loss: 0.018401, mae: 2.247958, mean_q: 2.717102, mean_eps: 0.171208\n",
      "  922052/2000000: episode: 1305, duration: 14.789s, episode steps: 689, steps per second:  47, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.019126, mae: 2.246786, mean_q: 2.717018, mean_eps: 0.170465\n",
      "  922746/2000000: episode: 1306, duration: 15.168s, episode steps: 694, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.018209, mae: 2.260457, mean_q: 2.733204, mean_eps: 0.169842\n",
      "  923461/2000000: episode: 1307, duration: 15.317s, episode steps: 715, steps per second:  47, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.016714, mae: 2.258038, mean_q: 2.730842, mean_eps: 0.169206\n",
      "  924481/2000000: episode: 1308, duration: 21.375s, episode steps: 1020, steps per second:  48, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.018442, mae: 2.248369, mean_q: 2.717056, mean_eps: 0.168425\n",
      "  925057/2000000: episode: 1309, duration: 12.524s, episode steps: 576, steps per second:  46, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.832 [0.000, 5.000],  loss: 0.016638, mae: 2.236331, mean_q: 2.701626, mean_eps: 0.167707\n",
      "  925793/2000000: episode: 1310, duration: 15.643s, episode steps: 736, steps per second:  47, episode reward: 21.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.018228, mae: 2.272041, mean_q: 2.746124, mean_eps: 0.167117\n",
      "  926176/2000000: episode: 1311, duration: 8.224s, episode steps: 383, steps per second:  47, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.637 [0.000, 5.000],  loss: 0.017706, mae: 2.268605, mean_q: 2.746065, mean_eps: 0.166614\n",
      "  927034/2000000: episode: 1312, duration: 18.162s, episode steps: 858, steps per second:  47, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.407 [0.000, 5.000],  loss: 0.015779, mae: 2.234335, mean_q: 2.701358, mean_eps: 0.166056\n",
      "  927555/2000000: episode: 1313, duration: 11.055s, episode steps: 521, steps per second:  47, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.019837, mae: 2.261299, mean_q: 2.733151, mean_eps: 0.165435\n",
      "  928241/2000000: episode: 1314, duration: 14.644s, episode steps: 686, steps per second:  47, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.018036, mae: 2.245482, mean_q: 2.714206, mean_eps: 0.164892\n",
      "  929025/2000000: episode: 1315, duration: 16.815s, episode steps: 784, steps per second:  47, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.003 [0.000, 5.000],  loss: 0.017129, mae: 2.239897, mean_q: 2.706658, mean_eps: 0.164229\n",
      "  929656/2000000: episode: 1316, duration: 13.424s, episode steps: 631, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.027 [0.000, 5.000],  loss: 0.017383, mae: 2.266766, mean_q: 2.740618, mean_eps: 0.163594\n",
      "  930458/2000000: episode: 1317, duration: 17.121s, episode steps: 802, steps per second:  47, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.018392, mae: 2.285244, mean_q: 2.763996, mean_eps: 0.162950\n",
      "  931474/2000000: episode: 1318, duration: 21.462s, episode steps: 1016, steps per second:  47, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.781 [0.000, 5.000],  loss: 0.016847, mae: 2.295188, mean_q: 2.775438, mean_eps: 0.162131\n",
      "  932297/2000000: episode: 1319, duration: 17.787s, episode steps: 823, steps per second:  46, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.018099, mae: 2.280600, mean_q: 2.757075, mean_eps: 0.161303\n",
      "  932880/2000000: episode: 1320, duration: 12.484s, episode steps: 583, steps per second:  47, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.016012, mae: 2.306127, mean_q: 2.788687, mean_eps: 0.160671\n",
      "  933532/2000000: episode: 1321, duration: 14.216s, episode steps: 652, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.018164, mae: 2.285066, mean_q: 2.763027, mean_eps: 0.160116\n",
      "  934567/2000000: episode: 1322, duration: 22.169s, episode steps: 1035, steps per second:  47, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.018284, mae: 2.282716, mean_q: 2.759346, mean_eps: 0.159357\n",
      "  935465/2000000: episode: 1323, duration: 19.070s, episode steps: 898, steps per second:  47, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.947 [0.000, 5.000],  loss: 0.017428, mae: 2.282174, mean_q: 2.758544, mean_eps: 0.158486\n",
      "  936411/2000000: episode: 1324, duration: 20.377s, episode steps: 946, steps per second:  46, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.018478, mae: 2.319956, mean_q: 2.803931, mean_eps: 0.157656\n",
      "  937214/2000000: episode: 1325, duration: 17.254s, episode steps: 803, steps per second:  47, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.019583, mae: 2.295455, mean_q: 2.771724, mean_eps: 0.156869\n",
      "  938132/2000000: episode: 1326, duration: 22.018s, episode steps: 918, steps per second:  42, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.018514, mae: 2.293111, mean_q: 2.772035, mean_eps: 0.156095\n",
      "  939254/2000000: episode: 1327, duration: 26.953s, episode steps: 1122, steps per second:  42, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.625 [0.000, 5.000],  loss: 0.019564, mae: 2.314055, mean_q: 2.794856, mean_eps: 0.155177\n",
      "  939912/2000000: episode: 1328, duration: 15.632s, episode steps: 658, steps per second:  42, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.015738, mae: 2.287560, mean_q: 2.763871, mean_eps: 0.154376\n",
      "  940573/2000000: episode: 1329, duration: 15.714s, episode steps: 661, steps per second:  42, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.016181, mae: 2.301504, mean_q: 2.784487, mean_eps: 0.153782\n",
      "  942086/2000000: episode: 1330, duration: 35.060s, episode steps: 1513, steps per second:  43, episode reward: 26.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.018569, mae: 2.319866, mean_q: 2.804387, mean_eps: 0.152803\n",
      "  942594/2000000: episode: 1331, duration: 11.850s, episode steps: 508, steps per second:  43, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.016 [0.000, 5.000],  loss: 0.018256, mae: 2.327725, mean_q: 2.813039, mean_eps: 0.151894\n",
      "  943468/2000000: episode: 1332, duration: 20.427s, episode steps: 874, steps per second:  43, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.333 [0.000, 5.000],  loss: 0.018332, mae: 2.333048, mean_q: 2.819491, mean_eps: 0.151273\n",
      "  943908/2000000: episode: 1333, duration: 10.562s, episode steps: 440, steps per second:  42, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.050 [0.000, 5.000],  loss: 0.017273, mae: 2.331405, mean_q: 2.818062, mean_eps: 0.150683\n",
      "  944639/2000000: episode: 1334, duration: 17.431s, episode steps: 731, steps per second:  42, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.016603, mae: 2.312785, mean_q: 2.793713, mean_eps: 0.150155\n",
      "  945487/2000000: episode: 1335, duration: 20.210s, episode steps: 848, steps per second:  42, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.018073, mae: 2.336593, mean_q: 2.827526, mean_eps: 0.149444\n",
      "  946421/2000000: episode: 1336, duration: 20.267s, episode steps: 934, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.020586, mae: 2.316436, mean_q: 2.799352, mean_eps: 0.148641\n",
      "  946996/2000000: episode: 1337, duration: 12.759s, episode steps: 575, steps per second:  45, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.454 [0.000, 5.000],  loss: 0.019308, mae: 2.309940, mean_q: 2.791058, mean_eps: 0.147963\n",
      "  947692/2000000: episode: 1338, duration: 15.136s, episode steps: 696, steps per second:  46, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.609 [0.000, 5.000],  loss: 0.017218, mae: 2.330912, mean_q: 2.817533, mean_eps: 0.147392\n",
      "  948208/2000000: episode: 1339, duration: 11.272s, episode steps: 516, steps per second:  46, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.771 [0.000, 5.000],  loss: 0.018206, mae: 2.325358, mean_q: 2.808865, mean_eps: 0.146847\n",
      "  948763/2000000: episode: 1340, duration: 11.932s, episode steps: 555, steps per second:  47, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.018874, mae: 2.324620, mean_q: 2.806467, mean_eps: 0.146364\n",
      "  949322/2000000: episode: 1341, duration: 12.297s, episode steps: 559, steps per second:  45, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.036 [0.000, 5.000],  loss: 0.017990, mae: 2.335888, mean_q: 2.822341, mean_eps: 0.145862\n",
      "  950155/2000000: episode: 1342, duration: 18.394s, episode steps: 833, steps per second:  45, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.020026, mae: 2.343716, mean_q: 2.835213, mean_eps: 0.145236\n",
      "  951012/2000000: episode: 1343, duration: 18.716s, episode steps: 857, steps per second:  46, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.903 [0.000, 5.000],  loss: 0.019056, mae: 2.368223, mean_q: 2.862789, mean_eps: 0.144476\n",
      "  951967/2000000: episode: 1344, duration: 20.830s, episode steps: 955, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.135 [0.000, 5.000],  loss: 0.017778, mae: 2.398510, mean_q: 2.897801, mean_eps: 0.143661\n",
      "  952823/2000000: episode: 1345, duration: 18.441s, episode steps: 856, steps per second:  46, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.703 [0.000, 5.000],  loss: 0.017591, mae: 2.348405, mean_q: 2.837509, mean_eps: 0.142845\n",
      "  953521/2000000: episode: 1346, duration: 15.210s, episode steps: 698, steps per second:  46, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.017401, mae: 2.358399, mean_q: 2.850347, mean_eps: 0.142145\n",
      "  954149/2000000: episode: 1347, duration: 13.747s, episode steps: 628, steps per second:  46, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.018780, mae: 2.367341, mean_q: 2.861562, mean_eps: 0.141548\n",
      "  954788/2000000: episode: 1348, duration: 13.780s, episode steps: 639, steps per second:  46, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.021657, mae: 2.357677, mean_q: 2.847650, mean_eps: 0.140979\n",
      "  955469/2000000: episode: 1349, duration: 14.755s, episode steps: 681, steps per second:  46, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.125 [0.000, 5.000],  loss: 0.020307, mae: 2.351981, mean_q: 2.841995, mean_eps: 0.140385\n",
      "  956337/2000000: episode: 1350, duration: 19.266s, episode steps: 868, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.543 [0.000, 5.000],  loss: 0.019123, mae: 2.365926, mean_q: 2.856351, mean_eps: 0.139686\n",
      "  957584/2000000: episode: 1351, duration: 26.915s, episode steps: 1247, steps per second:  46, episode reward: 23.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.018128, mae: 2.374001, mean_q: 2.867263, mean_eps: 0.138736\n",
      "  958238/2000000: episode: 1352, duration: 14.113s, episode steps: 654, steps per second:  46, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.615 [0.000, 5.000],  loss: 0.020877, mae: 2.342473, mean_q: 2.829496, mean_eps: 0.137881\n",
      "  958935/2000000: episode: 1353, duration: 15.261s, episode steps: 697, steps per second:  46, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.529 [0.000, 5.000],  loss: 0.016755, mae: 2.367036, mean_q: 2.860710, mean_eps: 0.137273\n",
      "  959472/2000000: episode: 1354, duration: 11.708s, episode steps: 537, steps per second:  46, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.778 [0.000, 5.000],  loss: 0.016333, mae: 2.345351, mean_q: 2.835754, mean_eps: 0.136718\n",
      "  960917/2000000: episode: 1355, duration: 32.090s, episode steps: 1445, steps per second:  45, episode reward: 30.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.393 [0.000, 5.000],  loss: 0.020582, mae: 2.391618, mean_q: 2.890410, mean_eps: 0.135825\n",
      "  962545/2000000: episode: 1356, duration: 35.419s, episode steps: 1628, steps per second:  46, episode reward: 26.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.018347, mae: 2.398380, mean_q: 2.898579, mean_eps: 0.134441\n",
      "  963241/2000000: episode: 1357, duration: 15.953s, episode steps: 696, steps per second:  44, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.016943, mae: 2.368983, mean_q: 2.861691, mean_eps: 0.133395\n",
      "  964107/2000000: episode: 1358, duration: 18.880s, episode steps: 866, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.993 [0.000, 5.000],  loss: 0.022825, mae: 2.395448, mean_q: 2.893269, mean_eps: 0.132693\n",
      "  964814/2000000: episode: 1359, duration: 15.679s, episode steps: 707, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.836 [0.000, 5.000],  loss: 0.018583, mae: 2.422703, mean_q: 2.927896, mean_eps: 0.131986\n",
      "  965295/2000000: episode: 1360, duration: 10.891s, episode steps: 481, steps per second:  44, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.200 [0.000, 5.000],  loss: 0.017816, mae: 2.421141, mean_q: 2.927070, mean_eps: 0.131451\n",
      "  966131/2000000: episode: 1361, duration: 18.475s, episode steps: 836, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.019959, mae: 2.372705, mean_q: 2.865685, mean_eps: 0.130859\n",
      "  966662/2000000: episode: 1362, duration: 11.899s, episode steps: 531, steps per second:  45, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.906 [0.000, 5.000],  loss: 0.016949, mae: 2.398807, mean_q: 2.899397, mean_eps: 0.130244\n",
      "  967522/2000000: episode: 1363, duration: 18.841s, episode steps: 860, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.020774, mae: 2.382573, mean_q: 2.877286, mean_eps: 0.129617\n",
      "  968484/2000000: episode: 1364, duration: 21.192s, episode steps: 962, steps per second:  45, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.018355, mae: 2.386828, mean_q: 2.882861, mean_eps: 0.128798\n",
      "  969123/2000000: episode: 1365, duration: 14.285s, episode steps: 639, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.020856, mae: 2.407859, mean_q: 2.907212, mean_eps: 0.128078\n",
      "  969968/2000000: episode: 1366, duration: 18.750s, episode steps: 845, steps per second:  45, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.020612, mae: 2.387431, mean_q: 2.883264, mean_eps: 0.127410\n",
      "  970841/2000000: episode: 1367, duration: 19.734s, episode steps: 873, steps per second:  44, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.292 [0.000, 5.000],  loss: 0.019063, mae: 2.418351, mean_q: 2.920300, mean_eps: 0.126636\n",
      "  971571/2000000: episode: 1368, duration: 16.085s, episode steps: 730, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.018389, mae: 2.443689, mean_q: 2.956526, mean_eps: 0.125915\n",
      "  972607/2000000: episode: 1369, duration: 22.698s, episode steps: 1036, steps per second:  46, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.659 [0.000, 5.000],  loss: 0.021083, mae: 2.441321, mean_q: 2.950501, mean_eps: 0.125121\n",
      "  973530/2000000: episode: 1370, duration: 20.190s, episode steps: 923, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.019043, mae: 2.407714, mean_q: 2.907558, mean_eps: 0.124239\n",
      "  974305/2000000: episode: 1371, duration: 17.024s, episode steps: 775, steps per second:  46, episode reward: 11.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.006 [0.000, 5.000],  loss: 0.020889, mae: 2.440788, mean_q: 2.950696, mean_eps: 0.123474\n",
      "  975037/2000000: episode: 1372, duration: 16.264s, episode steps: 732, steps per second:  45, episode reward:  9.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.020500, mae: 2.444283, mean_q: 2.954395, mean_eps: 0.122795\n",
      "  976036/2000000: episode: 1373, duration: 22.244s, episode steps: 999, steps per second:  45, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.723 [0.000, 5.000],  loss: 0.020306, mae: 2.429596, mean_q: 2.933295, mean_eps: 0.122018\n",
      "  976833/2000000: episode: 1374, duration: 17.860s, episode steps: 797, steps per second:  45, episode reward: 26.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.381 [0.000, 5.000],  loss: 0.022885, mae: 2.407962, mean_q: 2.906560, mean_eps: 0.121209\n",
      "  977767/2000000: episode: 1375, duration: 20.408s, episode steps: 934, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.045 [0.000, 5.000],  loss: 0.018707, mae: 2.430658, mean_q: 2.935424, mean_eps: 0.120430\n",
      "  978594/2000000: episode: 1376, duration: 17.995s, episode steps: 827, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.626 [0.000, 5.000],  loss: 0.019112, mae: 2.406129, mean_q: 2.904415, mean_eps: 0.119638\n",
      "  979214/2000000: episode: 1377, duration: 13.704s, episode steps: 620, steps per second:  45, episode reward: 17.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.022965, mae: 2.442796, mean_q: 2.949267, mean_eps: 0.118986\n",
      "  980382/2000000: episode: 1378, duration: 25.488s, episode steps: 1168, steps per second:  46, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.019910, mae: 2.420464, mean_q: 2.923860, mean_eps: 0.118182\n",
      "  980762/2000000: episode: 1379, duration: 8.350s, episode steps: 380, steps per second:  46, episode reward:  8.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.974 [0.000, 5.000],  loss: 0.017762, mae: 2.414280, mean_q: 2.916685, mean_eps: 0.117485\n",
      "  981386/2000000: episode: 1380, duration: 13.797s, episode steps: 624, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.020523, mae: 2.431352, mean_q: 2.936499, mean_eps: 0.117033\n",
      "  982232/2000000: episode: 1381, duration: 19.133s, episode steps: 846, steps per second:  44, episode reward: 25.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.019707, mae: 2.446186, mean_q: 2.957768, mean_eps: 0.116373\n",
      "  982896/2000000: episode: 1382, duration: 14.906s, episode steps: 664, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.018159, mae: 2.442626, mean_q: 2.949796, mean_eps: 0.115694\n",
      "  983577/2000000: episode: 1383, duration: 15.053s, episode steps: 681, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.966 [0.000, 5.000],  loss: 0.019974, mae: 2.460159, mean_q: 2.972566, mean_eps: 0.115088\n",
      "  984799/2000000: episode: 1384, duration: 27.371s, episode steps: 1222, steps per second:  45, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.379 [0.000, 5.000],  loss: 0.019708, mae: 2.436260, mean_q: 2.943079, mean_eps: 0.114231\n",
      "  985485/2000000: episode: 1385, duration: 15.076s, episode steps: 686, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.020552, mae: 2.443901, mean_q: 2.951830, mean_eps: 0.113372\n",
      "  985896/2000000: episode: 1386, duration: 9.094s, episode steps: 411, steps per second:  45, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.105 [0.000, 5.000],  loss: 0.017968, mae: 2.401053, mean_q: 2.899344, mean_eps: 0.112879\n",
      "  986944/2000000: episode: 1387, duration: 23.527s, episode steps: 1048, steps per second:  45, episode reward: 16.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.097 [0.000, 5.000],  loss: 0.018443, mae: 2.447738, mean_q: 2.957137, mean_eps: 0.112224\n",
      "  987857/2000000: episode: 1388, duration: 20.337s, episode steps: 913, steps per second:  45, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.905 [0.000, 5.000],  loss: 0.020275, mae: 2.437512, mean_q: 2.943438, mean_eps: 0.111340\n",
      "  989193/2000000: episode: 1389, duration: 29.981s, episode steps: 1336, steps per second:  45, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.020708, mae: 2.434451, mean_q: 2.937898, mean_eps: 0.110327\n",
      "  989897/2000000: episode: 1390, duration: 15.588s, episode steps: 704, steps per second:  45, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.000 [0.000, 5.000],  loss: 0.019459, mae: 2.432122, mean_q: 2.934753, mean_eps: 0.109409\n",
      "  990586/2000000: episode: 1391, duration: 15.120s, episode steps: 689, steps per second:  46, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.122 [0.000, 5.000],  loss: 0.016999, mae: 2.450648, mean_q: 2.958856, mean_eps: 0.108782\n",
      "  991383/2000000: episode: 1392, duration: 17.743s, episode steps: 797, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.253 [0.000, 5.000],  loss: 0.018772, mae: 2.459211, mean_q: 2.970013, mean_eps: 0.108114\n",
      "  991997/2000000: episode: 1393, duration: 13.681s, episode steps: 614, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.629 [0.000, 5.000],  loss: 0.020910, mae: 2.450391, mean_q: 2.958639, mean_eps: 0.107479\n",
      "  992947/2000000: episode: 1394, duration: 21.355s, episode steps: 950, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.026 [0.000, 5.000],  loss: 0.019013, mae: 2.436714, mean_q: 2.944516, mean_eps: 0.106775\n",
      "  993650/2000000: episode: 1395, duration: 15.585s, episode steps: 703, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.511 [0.000, 5.000],  loss: 0.018534, mae: 2.426714, mean_q: 2.933917, mean_eps: 0.106032\n",
      "  994276/2000000: episode: 1396, duration: 14.047s, episode steps: 626, steps per second:  45, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.510 [0.000, 5.000],  loss: 0.018099, mae: 2.449869, mean_q: 2.960840, mean_eps: 0.105434\n",
      "  995010/2000000: episode: 1397, duration: 15.967s, episode steps: 734, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.315 [0.000, 5.000],  loss: 0.020499, mae: 2.448893, mean_q: 2.958432, mean_eps: 0.104822\n",
      "  995919/2000000: episode: 1398, duration: 20.087s, episode steps: 909, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.576 [0.000, 5.000],  loss: 0.019342, mae: 2.434888, mean_q: 2.939905, mean_eps: 0.104082\n",
      "  996715/2000000: episode: 1399, duration: 17.770s, episode steps: 796, steps per second:  45, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.020264, mae: 2.447755, mean_q: 2.956543, mean_eps: 0.103316\n",
      "  997761/2000000: episode: 1400, duration: 23.629s, episode steps: 1046, steps per second:  44, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.019487, mae: 2.455107, mean_q: 2.965636, mean_eps: 0.102486\n",
      "  998083/2000000: episode: 1401, duration: 7.284s, episode steps: 322, steps per second:  44, episode reward:  1.000, mean reward:  0.003 [ 0.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: 0.020197, mae: 2.476464, mean_q: 2.993182, mean_eps: 0.101870\n",
      "  998927/2000000: episode: 1402, duration: 18.576s, episode steps: 844, steps per second:  45, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.328 [0.000, 5.000],  loss: 0.018792, mae: 2.446695, mean_q: 2.958393, mean_eps: 0.101346\n",
      "  999541/2000000: episode: 1403, duration: 13.778s, episode steps: 614, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.754 [0.000, 5.000],  loss: 0.019560, mae: 2.451423, mean_q: 2.959755, mean_eps: 0.100689\n",
      " 1000201/2000000: episode: 1404, duration: 14.497s, episode steps: 660, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.892 [0.000, 5.000],  loss: 0.016843, mae: 2.431047, mean_q: 2.932600, mean_eps: 0.100143\n",
      " 1000858/2000000: episode: 1405, duration: 14.547s, episode steps: 657, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.667 [0.000, 5.000],  loss: 0.016440, mae: 2.484315, mean_q: 3.003434, mean_eps: 0.100000\n",
      " 1001506/2000000: episode: 1406, duration: 14.144s, episode steps: 648, steps per second:  46, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.235 [0.000, 5.000],  loss: 0.021916, mae: 2.479378, mean_q: 2.996520, mean_eps: 0.100000\n",
      " 1002161/2000000: episode: 1407, duration: 14.571s, episode steps: 655, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.687 [0.000, 5.000],  loss: 0.019969, mae: 2.459956, mean_q: 2.972544, mean_eps: 0.100000\n",
      " 1003022/2000000: episode: 1408, duration: 18.961s, episode steps: 861, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.017150, mae: 2.465549, mean_q: 2.978965, mean_eps: 0.100000\n",
      " 1003766/2000000: episode: 1409, duration: 16.732s, episode steps: 744, steps per second:  44, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.021462, mae: 2.475071, mean_q: 2.990849, mean_eps: 0.100000\n",
      " 1004308/2000000: episode: 1410, duration: 11.781s, episode steps: 542, steps per second:  46, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.018049, mae: 2.480593, mean_q: 2.993244, mean_eps: 0.100000\n",
      " 1005184/2000000: episode: 1411, duration: 19.631s, episode steps: 876, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.020810, mae: 2.480305, mean_q: 2.995719, mean_eps: 0.100000\n",
      " 1005728/2000000: episode: 1412, duration: 11.996s, episode steps: 544, steps per second:  45, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.018835, mae: 2.478798, mean_q: 2.995094, mean_eps: 0.100000\n",
      " 1006971/2000000: episode: 1413, duration: 27.453s, episode steps: 1243, steps per second:  45, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.228 [0.000, 5.000],  loss: 0.017592, mae: 2.474858, mean_q: 2.991303, mean_eps: 0.100000\n",
      " 1007993/2000000: episode: 1414, duration: 22.499s, episode steps: 1022, steps per second:  45, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.205 [0.000, 5.000],  loss: 0.018578, mae: 2.459505, mean_q: 2.970497, mean_eps: 0.100000\n",
      " 1008655/2000000: episode: 1415, duration: 14.588s, episode steps: 662, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.218 [0.000, 5.000],  loss: 0.022883, mae: 2.488282, mean_q: 3.002941, mean_eps: 0.100000\n",
      " 1009305/2000000: episode: 1416, duration: 14.921s, episode steps: 650, steps per second:  44, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.075 [0.000, 5.000],  loss: 0.019538, mae: 2.507608, mean_q: 3.025548, mean_eps: 0.100000\n",
      " 1009885/2000000: episode: 1417, duration: 12.585s, episode steps: 580, steps per second:  46, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.018861, mae: 2.448067, mean_q: 2.954006, mean_eps: 0.100000\n",
      " 1010549/2000000: episode: 1418, duration: 15.135s, episode steps: 664, steps per second:  44, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.950 [0.000, 5.000],  loss: 0.022307, mae: 2.484076, mean_q: 3.000540, mean_eps: 0.100000\n",
      " 1011239/2000000: episode: 1419, duration: 14.916s, episode steps: 690, steps per second:  46, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.813 [0.000, 5.000],  loss: 0.018356, mae: 2.463168, mean_q: 2.976912, mean_eps: 0.100000\n",
      " 1012038/2000000: episode: 1420, duration: 17.909s, episode steps: 799, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.008 [0.000, 5.000],  loss: 0.018787, mae: 2.481038, mean_q: 2.996653, mean_eps: 0.100000\n",
      " 1013240/2000000: episode: 1421, duration: 26.651s, episode steps: 1202, steps per second:  45, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.978 [0.000, 5.000],  loss: 0.020076, mae: 2.476991, mean_q: 2.993395, mean_eps: 0.100000\n",
      " 1013914/2000000: episode: 1422, duration: 14.678s, episode steps: 674, steps per second:  46, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.592 [0.000, 5.000],  loss: 0.019478, mae: 2.485309, mean_q: 3.000144, mean_eps: 0.100000\n",
      " 1014378/2000000: episode: 1423, duration: 10.700s, episode steps: 464, steps per second:  43, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.023594, mae: 2.485989, mean_q: 2.999938, mean_eps: 0.100000\n",
      " 1015621/2000000: episode: 1424, duration: 27.795s, episode steps: 1243, steps per second:  45, episode reward: 23.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.020298, mae: 2.482345, mean_q: 2.999706, mean_eps: 0.100000\n",
      " 1016090/2000000: episode: 1425, duration: 10.197s, episode steps: 469, steps per second:  46, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.889 [0.000, 5.000],  loss: 0.019136, mae: 2.484608, mean_q: 3.000350, mean_eps: 0.100000\n",
      " 1017017/2000000: episode: 1426, duration: 20.540s, episode steps: 927, steps per second:  45, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.018623, mae: 2.460966, mean_q: 2.972974, mean_eps: 0.100000\n",
      " 1017725/2000000: episode: 1427, duration: 15.528s, episode steps: 708, steps per second:  46, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.018438, mae: 2.474488, mean_q: 2.993600, mean_eps: 0.100000\n",
      " 1018343/2000000: episode: 1428, duration: 13.670s, episode steps: 618, steps per second:  45, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.641 [0.000, 5.000],  loss: 0.018566, mae: 2.483936, mean_q: 2.999513, mean_eps: 0.100000\n",
      " 1019633/2000000: episode: 1429, duration: 28.840s, episode steps: 1290, steps per second:  45, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.020067, mae: 2.488391, mean_q: 3.005537, mean_eps: 0.100000\n",
      " 1020406/2000000: episode: 1430, duration: 17.030s, episode steps: 773, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.006 [0.000, 5.000],  loss: 0.021362, mae: 2.510613, mean_q: 3.032930, mean_eps: 0.100000\n",
      " 1021278/2000000: episode: 1431, duration: 19.488s, episode steps: 872, steps per second:  45, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.841 [0.000, 5.000],  loss: 0.020602, mae: 2.520656, mean_q: 3.045000, mean_eps: 0.100000\n",
      " 1022004/2000000: episode: 1432, duration: 16.119s, episode steps: 726, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.018175, mae: 2.525865, mean_q: 3.051112, mean_eps: 0.100000\n",
      " 1023258/2000000: episode: 1433, duration: 27.501s, episode steps: 1254, steps per second:  46, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.019264, mae: 2.544016, mean_q: 3.071932, mean_eps: 0.100000\n",
      " 1023859/2000000: episode: 1434, duration: 13.283s, episode steps: 601, steps per second:  45, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.020268, mae: 2.506266, mean_q: 3.027064, mean_eps: 0.100000\n",
      " 1024910/2000000: episode: 1435, duration: 23.121s, episode steps: 1051, steps per second:  45, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.020964, mae: 2.545151, mean_q: 3.075433, mean_eps: 0.100000\n",
      " 1025888/2000000: episode: 1436, duration: 23.829s, episode steps: 978, steps per second:  41, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.019157, mae: 2.539144, mean_q: 3.067491, mean_eps: 0.100000\n",
      " 1026911/2000000: episode: 1437, duration: 22.545s, episode steps: 1023, steps per second:  45, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.019481, mae: 2.528137, mean_q: 3.056181, mean_eps: 0.100000\n",
      " 1027828/2000000: episode: 1438, duration: 20.383s, episode steps: 917, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.779 [0.000, 5.000],  loss: 0.019480, mae: 2.520101, mean_q: 3.045651, mean_eps: 0.100000\n",
      " 1028887/2000000: episode: 1439, duration: 23.262s, episode steps: 1059, steps per second:  46, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.020048, mae: 2.511093, mean_q: 3.035838, mean_eps: 0.100000\n",
      " 1029485/2000000: episode: 1440, duration: 13.434s, episode steps: 598, steps per second:  45, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.019687, mae: 2.534769, mean_q: 3.062645, mean_eps: 0.100000\n",
      " 1030746/2000000: episode: 1441, duration: 28.141s, episode steps: 1261, steps per second:  45, episode reward: 15.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.114 [0.000, 5.000],  loss: 0.017614, mae: 2.557001, mean_q: 3.091122, mean_eps: 0.100000\n",
      " 1031269/2000000: episode: 1442, duration: 11.568s, episode steps: 523, steps per second:  45, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.023935, mae: 2.590839, mean_q: 3.128726, mean_eps: 0.100000\n",
      " 1032129/2000000: episode: 1443, duration: 19.203s, episode steps: 860, steps per second:  45, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.653 [0.000, 5.000],  loss: 0.022167, mae: 2.583092, mean_q: 3.121830, mean_eps: 0.100000\n",
      " 1033347/2000000: episode: 1444, duration: 26.733s, episode steps: 1218, steps per second:  46, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.019421, mae: 2.577788, mean_q: 3.113299, mean_eps: 0.100000\n",
      " 1034017/2000000: episode: 1445, duration: 14.658s, episode steps: 670, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.019877, mae: 2.571883, mean_q: 3.104049, mean_eps: 0.100000\n",
      " 1035028/2000000: episode: 1446, duration: 22.368s, episode steps: 1011, steps per second:  45, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.873 [0.000, 5.000],  loss: 0.019422, mae: 2.555204, mean_q: 3.085399, mean_eps: 0.100000\n",
      " 1035925/2000000: episode: 1447, duration: 19.925s, episode steps: 897, steps per second:  45, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.854 [0.000, 5.000],  loss: 0.020847, mae: 2.588492, mean_q: 3.124666, mean_eps: 0.100000\n",
      " 1036903/2000000: episode: 1448, duration: 21.397s, episode steps: 978, steps per second:  46, episode reward: 32.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.019984, mae: 2.587511, mean_q: 3.125155, mean_eps: 0.100000\n",
      " 1037858/2000000: episode: 1449, duration: 21.207s, episode steps: 955, steps per second:  45, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.022896, mae: 2.581639, mean_q: 3.115907, mean_eps: 0.100000\n",
      " 1038660/2000000: episode: 1450, duration: 17.731s, episode steps: 802, steps per second:  45, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.312 [0.000, 5.000],  loss: 0.018558, mae: 2.574057, mean_q: 3.107031, mean_eps: 0.100000\n",
      " 1039463/2000000: episode: 1451, duration: 17.698s, episode steps: 803, steps per second:  45, episode reward: 23.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.210 [0.000, 5.000],  loss: 0.019759, mae: 2.565735, mean_q: 3.096085, mean_eps: 0.100000\n",
      " 1040238/2000000: episode: 1452, duration: 17.026s, episode steps: 775, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.020154, mae: 2.564058, mean_q: 3.098909, mean_eps: 0.100000\n",
      " 1040908/2000000: episode: 1453, duration: 14.692s, episode steps: 670, steps per second:  46, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.019312, mae: 2.586426, mean_q: 3.124729, mean_eps: 0.100000\n",
      " 1041389/2000000: episode: 1454, duration: 10.784s, episode steps: 481, steps per second:  45, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.913 [0.000, 5.000],  loss: 0.017536, mae: 2.597240, mean_q: 3.141146, mean_eps: 0.100000\n",
      " 1042037/2000000: episode: 1455, duration: 14.408s, episode steps: 648, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.021136, mae: 2.576986, mean_q: 3.111992, mean_eps: 0.100000\n",
      " 1043094/2000000: episode: 1456, duration: 23.599s, episode steps: 1057, steps per second:  45, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.021328, mae: 2.596117, mean_q: 3.135807, mean_eps: 0.100000\n",
      " 1043631/2000000: episode: 1457, duration: 11.628s, episode steps: 537, steps per second:  46, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.240 [0.000, 5.000],  loss: 0.021381, mae: 2.575936, mean_q: 3.108880, mean_eps: 0.100000\n",
      " 1044125/2000000: episode: 1458, duration: 11.008s, episode steps: 494, steps per second:  45, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.883 [0.000, 5.000],  loss: 0.020090, mae: 2.576211, mean_q: 3.111493, mean_eps: 0.100000\n",
      " 1045090/2000000: episode: 1459, duration: 21.278s, episode steps: 965, steps per second:  45, episode reward: 26.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.673 [0.000, 5.000],  loss: 0.019932, mae: 2.570036, mean_q: 3.105563, mean_eps: 0.100000\n",
      " 1045642/2000000: episode: 1460, duration: 12.077s, episode steps: 552, steps per second:  46, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.018497, mae: 2.570918, mean_q: 3.106213, mean_eps: 0.100000\n",
      " 1046774/2000000: episode: 1461, duration: 24.877s, episode steps: 1132, steps per second:  46, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.019446, mae: 2.579389, mean_q: 3.113276, mean_eps: 0.100000\n",
      " 1047681/2000000: episode: 1462, duration: 19.951s, episode steps: 907, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.999 [0.000, 5.000],  loss: 0.021794, mae: 2.584286, mean_q: 3.118010, mean_eps: 0.100000\n",
      " 1048636/2000000: episode: 1463, duration: 21.095s, episode steps: 955, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.020854, mae: 2.585326, mean_q: 3.120716, mean_eps: 0.100000\n",
      " 1049775/2000000: episode: 1464, duration: 25.112s, episode steps: 1139, steps per second:  45, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.023188, mae: 2.579756, mean_q: 3.113201, mean_eps: 0.100000\n",
      " 1050331/2000000: episode: 1465, duration: 12.114s, episode steps: 556, steps per second:  46, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.439 [0.000, 5.000],  loss: 0.018686, mae: 2.596570, mean_q: 3.137905, mean_eps: 0.100000\n",
      " 1050696/2000000: episode: 1466, duration: 8.045s, episode steps: 365, steps per second:  45, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.017720, mae: 2.628039, mean_q: 3.178991, mean_eps: 0.100000\n",
      " 1051236/2000000: episode: 1467, duration: 12.217s, episode steps: 540, steps per second:  44, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 0.019261, mae: 2.623850, mean_q: 3.172681, mean_eps: 0.100000\n",
      " 1051713/2000000: episode: 1468, duration: 10.482s, episode steps: 477, steps per second:  46, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.782 [0.000, 5.000],  loss: 0.021784, mae: 2.634739, mean_q: 3.182899, mean_eps: 0.100000\n",
      " 1052664/2000000: episode: 1469, duration: 21.231s, episode steps: 951, steps per second:  45, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.019913, mae: 2.619130, mean_q: 3.164140, mean_eps: 0.100000\n",
      " 1053854/2000000: episode: 1470, duration: 26.462s, episode steps: 1190, steps per second:  45, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.107 [0.000, 5.000],  loss: 0.020996, mae: 2.618598, mean_q: 3.160768, mean_eps: 0.100000\n",
      " 1054856/2000000: episode: 1471, duration: 22.087s, episode steps: 1002, steps per second:  45, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.019899, mae: 2.618178, mean_q: 3.161220, mean_eps: 0.100000\n",
      " 1055573/2000000: episode: 1472, duration: 15.775s, episode steps: 717, steps per second:  45, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.478 [0.000, 5.000],  loss: 0.020766, mae: 2.619576, mean_q: 3.162147, mean_eps: 0.100000\n",
      " 1056374/2000000: episode: 1473, duration: 17.609s, episode steps: 801, steps per second:  45, episode reward: 14.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.020029, mae: 2.616145, mean_q: 3.160166, mean_eps: 0.100000\n",
      " 1057200/2000000: episode: 1474, duration: 18.092s, episode steps: 826, steps per second:  46, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.645 [0.000, 5.000],  loss: 0.019504, mae: 2.601963, mean_q: 3.142137, mean_eps: 0.100000\n",
      " 1058055/2000000: episode: 1475, duration: 19.317s, episode steps: 855, steps per second:  44, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.230 [0.000, 5.000],  loss: 0.023368, mae: 2.618317, mean_q: 3.158426, mean_eps: 0.100000\n",
      " 1058491/2000000: episode: 1476, duration: 9.601s, episode steps: 436, steps per second:  45, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.020232, mae: 2.609427, mean_q: 3.152088, mean_eps: 0.100000\n",
      " 1059420/2000000: episode: 1477, duration: 20.401s, episode steps: 929, steps per second:  46, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.021883, mae: 2.634041, mean_q: 3.179250, mean_eps: 0.100000\n",
      " 1060358/2000000: episode: 1478, duration: 20.886s, episode steps: 938, steps per second:  45, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.021220, mae: 2.614302, mean_q: 3.160489, mean_eps: 0.100000\n",
      " 1061337/2000000: episode: 1479, duration: 21.262s, episode steps: 979, steps per second:  46, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.027 [0.000, 5.000],  loss: 0.019733, mae: 2.632248, mean_q: 3.179378, mean_eps: 0.100000\n",
      " 1062001/2000000: episode: 1480, duration: 14.690s, episode steps: 664, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.021083, mae: 2.633974, mean_q: 3.179921, mean_eps: 0.100000\n",
      " 1062872/2000000: episode: 1481, duration: 19.068s, episode steps: 871, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.785 [0.000, 5.000],  loss: 0.019614, mae: 2.624034, mean_q: 3.171309, mean_eps: 0.100000\n",
      " 1063439/2000000: episode: 1482, duration: 12.696s, episode steps: 567, steps per second:  45, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.020465, mae: 2.634984, mean_q: 3.185403, mean_eps: 0.100000\n",
      " 1063953/2000000: episode: 1483, duration: 11.200s, episode steps: 514, steps per second:  46, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.177 [0.000, 5.000],  loss: 0.019228, mae: 2.631982, mean_q: 3.179429, mean_eps: 0.100000\n",
      " 1064851/2000000: episode: 1484, duration: 19.822s, episode steps: 898, steps per second:  45, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.017511, mae: 2.633336, mean_q: 3.181692, mean_eps: 0.100000\n",
      " 1065962/2000000: episode: 1485, duration: 24.720s, episode steps: 1111, steps per second:  45, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.019671, mae: 2.627459, mean_q: 3.172779, mean_eps: 0.100000\n",
      " 1066888/2000000: episode: 1486, duration: 20.412s, episode steps: 926, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.887 [0.000, 5.000],  loss: 0.021426, mae: 2.645870, mean_q: 3.195501, mean_eps: 0.100000\n",
      " 1067803/2000000: episode: 1487, duration: 20.360s, episode steps: 915, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.022617, mae: 2.674727, mean_q: 3.229895, mean_eps: 0.100000\n",
      " 1068413/2000000: episode: 1488, duration: 13.419s, episode steps: 610, steps per second:  45, episode reward:  5.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.020428, mae: 2.626581, mean_q: 3.173158, mean_eps: 0.100000\n",
      " 1069042/2000000: episode: 1489, duration: 14.353s, episode steps: 629, steps per second:  44, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.019788, mae: 2.645947, mean_q: 3.198253, mean_eps: 0.100000\n",
      " 1069785/2000000: episode: 1490, duration: 16.339s, episode steps: 743, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.946 [0.000, 5.000],  loss: 0.021215, mae: 2.654099, mean_q: 3.205982, mean_eps: 0.100000\n",
      " 1070687/2000000: episode: 1491, duration: 19.663s, episode steps: 902, steps per second:  46, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.019633, mae: 2.665870, mean_q: 3.222728, mean_eps: 0.100000\n",
      " 1071320/2000000: episode: 1492, duration: 14.212s, episode steps: 633, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.019408, mae: 2.680733, mean_q: 3.240185, mean_eps: 0.100000\n",
      " 1071943/2000000: episode: 1493, duration: 13.853s, episode steps: 623, steps per second:  45, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.017451, mae: 2.661065, mean_q: 3.214509, mean_eps: 0.100000\n",
      " 1072713/2000000: episode: 1494, duration: 16.926s, episode steps: 770, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.068 [0.000, 5.000],  loss: 0.020571, mae: 2.685112, mean_q: 3.239250, mean_eps: 0.100000\n",
      " 1073720/2000000: episode: 1495, duration: 22.009s, episode steps: 1007, steps per second:  46, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.698 [0.000, 5.000],  loss: 0.021724, mae: 2.677058, mean_q: 3.232094, mean_eps: 0.100000\n",
      " 1074738/2000000: episode: 1496, duration: 22.622s, episode steps: 1018, steps per second:  45, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.937 [0.000, 5.000],  loss: 0.020860, mae: 2.682115, mean_q: 3.238824, mean_eps: 0.100000\n",
      " 1075682/2000000: episode: 1497, duration: 20.833s, episode steps: 944, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.019922, mae: 2.688722, mean_q: 3.246469, mean_eps: 0.100000\n",
      " 1076069/2000000: episode: 1498, duration: 8.257s, episode steps: 387, steps per second:  47, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 3.664 [0.000, 5.000],  loss: 0.022890, mae: 2.656805, mean_q: 3.208466, mean_eps: 0.100000\n",
      " 1077120/2000000: episode: 1499, duration: 23.373s, episode steps: 1051, steps per second:  45, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.019574, mae: 2.668753, mean_q: 3.223151, mean_eps: 0.100000\n",
      " 1078059/2000000: episode: 1500, duration: 20.799s, episode steps: 939, steps per second:  45, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.021875, mae: 2.683245, mean_q: 3.240921, mean_eps: 0.100000\n",
      " 1078865/2000000: episode: 1501, duration: 17.501s, episode steps: 806, steps per second:  46, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.020090, mae: 2.663757, mean_q: 3.216745, mean_eps: 0.100000\n",
      " 1079755/2000000: episode: 1502, duration: 19.648s, episode steps: 890, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.021256, mae: 2.674817, mean_q: 3.228112, mean_eps: 0.100000\n",
      " 1080384/2000000: episode: 1503, duration: 13.729s, episode steps: 629, steps per second:  46, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.870 [0.000, 5.000],  loss: 0.018391, mae: 2.691385, mean_q: 3.247873, mean_eps: 0.100000\n",
      " 1081252/2000000: episode: 1504, duration: 19.695s, episode steps: 868, steps per second:  44, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.116 [0.000, 5.000],  loss: 0.021855, mae: 2.711426, mean_q: 3.274368, mean_eps: 0.100000\n",
      " 1081875/2000000: episode: 1505, duration: 13.784s, episode steps: 623, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.671 [0.000, 5.000],  loss: 0.018786, mae: 2.707518, mean_q: 3.272883, mean_eps: 0.100000\n",
      " 1082746/2000000: episode: 1506, duration: 19.533s, episode steps: 871, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.019838, mae: 2.697487, mean_q: 3.259551, mean_eps: 0.100000\n",
      " 1083322/2000000: episode: 1507, duration: 12.516s, episode steps: 576, steps per second:  46, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.021493, mae: 2.697350, mean_q: 3.257292, mean_eps: 0.100000\n",
      " 1084193/2000000: episode: 1508, duration: 19.154s, episode steps: 871, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.023666, mae: 2.689110, mean_q: 3.250263, mean_eps: 0.100000\n",
      " 1085015/2000000: episode: 1509, duration: 18.245s, episode steps: 822, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.019486, mae: 2.690964, mean_q: 3.251023, mean_eps: 0.100000\n",
      " 1085868/2000000: episode: 1510, duration: 18.733s, episode steps: 853, steps per second:  46, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.999 [0.000, 5.000],  loss: 0.019351, mae: 2.699537, mean_q: 3.258065, mean_eps: 0.100000\n",
      " 1086875/2000000: episode: 1511, duration: 22.017s, episode steps: 1007, steps per second:  46, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.217 [0.000, 5.000],  loss: 0.021019, mae: 2.688357, mean_q: 3.244066, mean_eps: 0.100000\n",
      " 1087687/2000000: episode: 1512, duration: 17.957s, episode steps: 812, steps per second:  45, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.021347, mae: 2.703913, mean_q: 3.263394, mean_eps: 0.100000\n",
      " 1088456/2000000: episode: 1513, duration: 16.610s, episode steps: 769, steps per second:  46, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.020134, mae: 2.707545, mean_q: 3.269130, mean_eps: 0.100000\n",
      " 1089068/2000000: episode: 1514, duration: 13.725s, episode steps: 612, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.020447, mae: 2.689972, mean_q: 3.249263, mean_eps: 0.100000\n",
      " 1089669/2000000: episode: 1515, duration: 13.318s, episode steps: 601, steps per second:  45, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.248 [0.000, 5.000],  loss: 0.023000, mae: 2.683834, mean_q: 3.243339, mean_eps: 0.100000\n",
      " 1090461/2000000: episode: 1516, duration: 17.408s, episode steps: 792, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.301 [0.000, 5.000],  loss: 0.020304, mae: 2.728084, mean_q: 3.294214, mean_eps: 0.100000\n",
      " 1091225/2000000: episode: 1517, duration: 16.543s, episode steps: 764, steps per second:  46, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.019994, mae: 2.732779, mean_q: 3.300812, mean_eps: 0.100000\n",
      " 1091902/2000000: episode: 1518, duration: 15.016s, episode steps: 677, steps per second:  45, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.019182, mae: 2.725653, mean_q: 3.291391, mean_eps: 0.100000\n",
      " 1093084/2000000: episode: 1519, duration: 25.856s, episode steps: 1182, steps per second:  46, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.020832, mae: 2.731867, mean_q: 3.298055, mean_eps: 0.100000\n",
      " 1093742/2000000: episode: 1520, duration: 14.358s, episode steps: 658, steps per second:  46, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.020244, mae: 2.737126, mean_q: 3.304088, mean_eps: 0.100000\n",
      " 1094097/2000000: episode: 1521, duration: 7.649s, episode steps: 355, steps per second:  46, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.022498, mae: 2.756246, mean_q: 3.326363, mean_eps: 0.100000\n",
      " 1094740/2000000: episode: 1522, duration: 14.340s, episode steps: 643, steps per second:  45, episode reward: 20.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.862 [0.000, 5.000],  loss: 0.021653, mae: 2.720801, mean_q: 3.283841, mean_eps: 0.100000\n",
      " 1095374/2000000: episode: 1523, duration: 13.760s, episode steps: 634, steps per second:  46, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.131 [0.000, 5.000],  loss: 0.021573, mae: 2.726268, mean_q: 3.290998, mean_eps: 0.100000\n",
      " 1096354/2000000: episode: 1524, duration: 21.602s, episode steps: 980, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.021770, mae: 2.732361, mean_q: 3.298941, mean_eps: 0.100000\n",
      " 1097271/2000000: episode: 1525, duration: 20.095s, episode steps: 917, steps per second:  46, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.997 [0.000, 5.000],  loss: 0.021153, mae: 2.730416, mean_q: 3.297144, mean_eps: 0.100000\n",
      " 1098324/2000000: episode: 1526, duration: 23.002s, episode steps: 1053, steps per second:  46, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.711 [0.000, 5.000],  loss: 0.023374, mae: 2.753493, mean_q: 3.322352, mean_eps: 0.100000\n",
      " 1099099/2000000: episode: 1527, duration: 16.971s, episode steps: 775, steps per second:  46, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.022183, mae: 2.755277, mean_q: 3.323078, mean_eps: 0.100000\n",
      " 1099742/2000000: episode: 1528, duration: 14.121s, episode steps: 643, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.675 [0.000, 5.000],  loss: 0.022081, mae: 2.747738, mean_q: 3.315482, mean_eps: 0.100000\n",
      " 1100636/2000000: episode: 1529, duration: 19.839s, episode steps: 894, steps per second:  45, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.017704, mae: 2.732682, mean_q: 3.301001, mean_eps: 0.100000\n",
      " 1101465/2000000: episode: 1530, duration: 18.183s, episode steps: 829, steps per second:  46, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.020334, mae: 2.741523, mean_q: 3.313650, mean_eps: 0.100000\n",
      " 1102014/2000000: episode: 1531, duration: 12.267s, episode steps: 549, steps per second:  45, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.023825, mae: 2.743926, mean_q: 3.311424, mean_eps: 0.100000\n",
      " 1103068/2000000: episode: 1532, duration: 23.076s, episode steps: 1054, steps per second:  46, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.021545, mae: 2.723156, mean_q: 3.289673, mean_eps: 0.100000\n",
      " 1103823/2000000: episode: 1533, duration: 16.593s, episode steps: 755, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.019860, mae: 2.733365, mean_q: 3.299851, mean_eps: 0.100000\n",
      " 1104358/2000000: episode: 1534, duration: 11.675s, episode steps: 535, steps per second:  46, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.021472, mae: 2.729202, mean_q: 3.297266, mean_eps: 0.100000\n",
      " 1105344/2000000: episode: 1535, duration: 22.023s, episode steps: 986, steps per second:  45, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.019275, mae: 2.725860, mean_q: 3.292462, mean_eps: 0.100000\n",
      " 1106256/2000000: episode: 1536, duration: 19.771s, episode steps: 912, steps per second:  46, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.022759, mae: 2.737552, mean_q: 3.308536, mean_eps: 0.100000\n",
      " 1106923/2000000: episode: 1537, duration: 14.823s, episode steps: 667, steps per second:  45, episode reward:  6.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.020410, mae: 2.732948, mean_q: 3.300915, mean_eps: 0.100000\n",
      " 1107634/2000000: episode: 1538, duration: 15.766s, episode steps: 711, steps per second:  45, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.932 [0.000, 5.000],  loss: 0.020783, mae: 2.747946, mean_q: 3.319519, mean_eps: 0.100000\n",
      " 1108679/2000000: episode: 1539, duration: 23.110s, episode steps: 1045, steps per second:  45, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.019352, mae: 2.727793, mean_q: 3.294879, mean_eps: 0.100000\n",
      " 1109333/2000000: episode: 1540, duration: 14.878s, episode steps: 654, steps per second:  44, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.735 [0.000, 5.000],  loss: 0.020311, mae: 2.746393, mean_q: 3.318471, mean_eps: 0.100000\n",
      " 1110422/2000000: episode: 1541, duration: 24.028s, episode steps: 1089, steps per second:  45, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.020796, mae: 2.754048, mean_q: 3.326768, mean_eps: 0.100000\n",
      " 1111533/2000000: episode: 1542, duration: 24.634s, episode steps: 1111, steps per second:  45, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.842 [0.000, 5.000],  loss: 0.018166, mae: 2.745147, mean_q: 3.316332, mean_eps: 0.100000\n",
      " 1112460/2000000: episode: 1543, duration: 20.440s, episode steps: 927, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.019131, mae: 2.769967, mean_q: 3.346241, mean_eps: 0.100000\n",
      " 1112894/2000000: episode: 1544, duration: 10.047s, episode steps: 434, steps per second:  43, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.023269, mae: 2.746581, mean_q: 3.315223, mean_eps: 0.100000\n",
      " 1113751/2000000: episode: 1545, duration: 19.219s, episode steps: 857, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.755 [0.000, 5.000],  loss: 0.020025, mae: 2.764649, mean_q: 3.338643, mean_eps: 0.100000\n",
      " 1114692/2000000: episode: 1546, duration: 21.204s, episode steps: 941, steps per second:  44, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.776 [0.000, 5.000],  loss: 0.020274, mae: 2.753661, mean_q: 3.325614, mean_eps: 0.100000\n",
      " 1115558/2000000: episode: 1547, duration: 19.026s, episode steps: 866, steps per second:  46, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.129 [0.000, 5.000],  loss: 0.020907, mae: 2.752897, mean_q: 3.323523, mean_eps: 0.100000\n",
      " 1116153/2000000: episode: 1548, duration: 13.234s, episode steps: 595, steps per second:  45, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.914 [0.000, 5.000],  loss: 0.021336, mae: 2.747005, mean_q: 3.317642, mean_eps: 0.100000\n",
      " 1117069/2000000: episode: 1549, duration: 20.261s, episode steps: 916, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.018537, mae: 2.756205, mean_q: 3.328843, mean_eps: 0.100000\n",
      " 1117908/2000000: episode: 1550, duration: 18.653s, episode steps: 839, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.794 [0.000, 5.000],  loss: 0.022303, mae: 2.763004, mean_q: 3.336783, mean_eps: 0.100000\n",
      " 1118315/2000000: episode: 1551, duration: 9.190s, episode steps: 407, steps per second:  44, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.020719, mae: 2.758472, mean_q: 3.331893, mean_eps: 0.100000\n",
      " 1119101/2000000: episode: 1552, duration: 17.405s, episode steps: 786, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.642 [0.000, 5.000],  loss: 0.022810, mae: 2.757936, mean_q: 3.327898, mean_eps: 0.100000\n",
      " 1119834/2000000: episode: 1553, duration: 16.160s, episode steps: 733, steps per second:  45, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.020573, mae: 2.739476, mean_q: 3.306902, mean_eps: 0.100000\n",
      " 1120498/2000000: episode: 1554, duration: 14.741s, episode steps: 664, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.021055, mae: 2.777102, mean_q: 3.352207, mean_eps: 0.100000\n",
      " 1120859/2000000: episode: 1555, duration: 7.805s, episode steps: 361, steps per second:  46, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.024054, mae: 2.799001, mean_q: 3.376927, mean_eps: 0.100000\n",
      " 1121606/2000000: episode: 1556, duration: 16.475s, episode steps: 747, steps per second:  45, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.018631, mae: 2.784780, mean_q: 3.363630, mean_eps: 0.100000\n",
      " 1122855/2000000: episode: 1557, duration: 27.672s, episode steps: 1249, steps per second:  45, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.725 [0.000, 5.000],  loss: 0.020184, mae: 2.787145, mean_q: 3.364409, mean_eps: 0.100000\n",
      " 1124119/2000000: episode: 1558, duration: 27.903s, episode steps: 1264, steps per second:  45, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.460 [0.000, 5.000],  loss: 0.021682, mae: 2.780546, mean_q: 3.356023, mean_eps: 0.100000\n",
      " 1124562/2000000: episode: 1559, duration: 9.648s, episode steps: 443, steps per second:  46, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.372 [0.000, 5.000],  loss: 0.017780, mae: 2.823562, mean_q: 3.406714, mean_eps: 0.100000\n",
      " 1125613/2000000: episode: 1560, duration: 23.464s, episode steps: 1051, steps per second:  45, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.509 [0.000, 5.000],  loss: 0.020474, mae: 2.796575, mean_q: 3.375753, mean_eps: 0.100000\n",
      " 1126518/2000000: episode: 1561, duration: 19.885s, episode steps: 905, steps per second:  46, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.245 [0.000, 5.000],  loss: 0.020006, mae: 2.806105, mean_q: 3.388828, mean_eps: 0.100000\n",
      " 1127074/2000000: episode: 1562, duration: 12.395s, episode steps: 556, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.025738, mae: 2.827299, mean_q: 3.414045, mean_eps: 0.100000\n",
      " 1127651/2000000: episode: 1563, duration: 12.428s, episode steps: 577, steps per second:  46, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.019512, mae: 2.814892, mean_q: 3.398434, mean_eps: 0.100000\n",
      " 1128379/2000000: episode: 1564, duration: 16.444s, episode steps: 728, steps per second:  44, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.743 [0.000, 5.000],  loss: 0.022435, mae: 2.793774, mean_q: 3.372024, mean_eps: 0.100000\n",
      " 1128837/2000000: episode: 1565, duration: 10.015s, episode steps: 458, steps per second:  46, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.020407, mae: 2.774284, mean_q: 3.345606, mean_eps: 0.100000\n",
      " 1129209/2000000: episode: 1566, duration: 8.474s, episode steps: 372, steps per second:  44, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.801 [0.000, 5.000],  loss: 0.019627, mae: 2.785781, mean_q: 3.361018, mean_eps: 0.100000\n",
      " 1130465/2000000: episode: 1567, duration: 27.362s, episode steps: 1256, steps per second:  46, episode reward: 24.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.859 [0.000, 5.000],  loss: 0.021027, mae: 2.791941, mean_q: 3.369091, mean_eps: 0.100000\n",
      " 1131135/2000000: episode: 1568, duration: 14.943s, episode steps: 670, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.697 [0.000, 5.000],  loss: 0.019145, mae: 2.808439, mean_q: 3.391337, mean_eps: 0.100000\n",
      " 1131675/2000000: episode: 1569, duration: 12.354s, episode steps: 540, steps per second:  44, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.017479, mae: 2.834161, mean_q: 3.420141, mean_eps: 0.100000\n",
      " 1132693/2000000: episode: 1570, duration: 22.592s, episode steps: 1018, steps per second:  45, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.018938, mae: 2.808614, mean_q: 3.390240, mean_eps: 0.100000\n",
      " 1134048/2000000: episode: 1571, duration: 29.794s, episode steps: 1355, steps per second:  45, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.020758, mae: 2.828106, mean_q: 3.411958, mean_eps: 0.100000\n",
      " 1134505/2000000: episode: 1572, duration: 10.398s, episode steps: 457, steps per second:  44, episode reward:  5.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.019310, mae: 2.828990, mean_q: 3.413942, mean_eps: 0.100000\n",
      " 1135148/2000000: episode: 1573, duration: 14.376s, episode steps: 643, steps per second:  45, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.896 [0.000, 5.000],  loss: 0.020996, mae: 2.824909, mean_q: 3.408708, mean_eps: 0.100000\n",
      " 1136227/2000000: episode: 1574, duration: 23.597s, episode steps: 1079, steps per second:  46, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.019029, mae: 2.820519, mean_q: 3.406286, mean_eps: 0.100000\n",
      " 1136922/2000000: episode: 1575, duration: 15.362s, episode steps: 695, steps per second:  45, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.012 [0.000, 5.000],  loss: 0.018839, mae: 2.834070, mean_q: 3.420958, mean_eps: 0.100000\n",
      " 1138148/2000000: episode: 1576, duration: 27.126s, episode steps: 1226, steps per second:  45, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.021008, mae: 2.806101, mean_q: 3.386626, mean_eps: 0.100000\n",
      " 1138766/2000000: episode: 1577, duration: 13.707s, episode steps: 618, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.022674, mae: 2.808541, mean_q: 3.391003, mean_eps: 0.100000\n",
      " 1139472/2000000: episode: 1578, duration: 15.730s, episode steps: 706, steps per second:  45, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.042 [0.000, 5.000],  loss: 0.020667, mae: 2.827960, mean_q: 3.414528, mean_eps: 0.100000\n",
      " 1140100/2000000: episode: 1579, duration: 14.154s, episode steps: 628, steps per second:  44, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.161 [0.000, 5.000],  loss: 0.023442, mae: 2.831837, mean_q: 3.414267, mean_eps: 0.100000\n",
      " 1140906/2000000: episode: 1580, duration: 18.071s, episode steps: 806, steps per second:  45, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.022711, mae: 2.839841, mean_q: 3.427579, mean_eps: 0.100000\n",
      " 1141362/2000000: episode: 1581, duration: 9.975s, episode steps: 456, steps per second:  46, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.515 [0.000, 5.000],  loss: 0.021168, mae: 2.851005, mean_q: 3.438967, mean_eps: 0.100000\n",
      " 1141751/2000000: episode: 1582, duration: 8.834s, episode steps: 389, steps per second:  44, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.519 [0.000, 5.000],  loss: 0.022468, mae: 2.869054, mean_q: 3.459916, mean_eps: 0.100000\n",
      " 1142789/2000000: episode: 1583, duration: 22.477s, episode steps: 1038, steps per second:  46, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.020492, mae: 2.832862, mean_q: 3.415655, mean_eps: 0.100000\n",
      " 1143516/2000000: episode: 1584, duration: 16.325s, episode steps: 727, steps per second:  45, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.337 [0.000, 5.000],  loss: 0.021625, mae: 2.855362, mean_q: 3.444739, mean_eps: 0.100000\n",
      " 1144253/2000000: episode: 1585, duration: 16.148s, episode steps: 737, steps per second:  46, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.410 [0.000, 5.000],  loss: 0.021267, mae: 2.822120, mean_q: 3.405884, mean_eps: 0.100000\n",
      " 1145404/2000000: episode: 1586, duration: 25.327s, episode steps: 1151, steps per second:  45, episode reward: 15.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.020150, mae: 2.822725, mean_q: 3.405405, mean_eps: 0.100000\n",
      " 1145910/2000000: episode: 1587, duration: 11.489s, episode steps: 506, steps per second:  44, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.021215, mae: 2.842372, mean_q: 3.429342, mean_eps: 0.100000\n",
      " 1146444/2000000: episode: 1588, duration: 11.917s, episode steps: 534, steps per second:  45, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.324 [0.000, 5.000],  loss: 0.022783, mae: 2.830178, mean_q: 3.411626, mean_eps: 0.100000\n",
      " 1147829/2000000: episode: 1589, duration: 30.655s, episode steps: 1385, steps per second:  45, episode reward: 24.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.020386, mae: 2.832313, mean_q: 3.415065, mean_eps: 0.100000\n",
      " 1148458/2000000: episode: 1590, duration: 13.997s, episode steps: 629, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.023748, mae: 2.829504, mean_q: 3.411674, mean_eps: 0.100000\n",
      " 1149071/2000000: episode: 1591, duration: 13.445s, episode steps: 613, steps per second:  46, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.759 [0.000, 5.000],  loss: 0.023078, mae: 2.848031, mean_q: 3.432929, mean_eps: 0.100000\n",
      " 1150135/2000000: episode: 1592, duration: 23.877s, episode steps: 1064, steps per second:  45, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.142 [0.000, 5.000],  loss: 0.022350, mae: 2.834203, mean_q: 3.420025, mean_eps: 0.100000\n",
      " 1150703/2000000: episode: 1593, duration: 12.190s, episode steps: 568, steps per second:  47, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.188 [0.000, 5.000],  loss: 0.021151, mae: 2.838673, mean_q: 3.422821, mean_eps: 0.100000\n",
      " 1151363/2000000: episode: 1594, duration: 14.820s, episode steps: 660, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.689 [0.000, 5.000],  loss: 0.018427, mae: 2.827115, mean_q: 3.411211, mean_eps: 0.100000\n",
      " 1152139/2000000: episode: 1595, duration: 16.807s, episode steps: 776, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.582 [0.000, 5.000],  loss: 0.019906, mae: 2.838218, mean_q: 3.424130, mean_eps: 0.100000\n",
      " 1153167/2000000: episode: 1596, duration: 23.035s, episode steps: 1028, steps per second:  45, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.019419, mae: 2.821501, mean_q: 3.401977, mean_eps: 0.100000\n",
      " 1153890/2000000: episode: 1597, duration: 16.025s, episode steps: 723, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.459 [0.000, 5.000],  loss: 0.019066, mae: 2.850187, mean_q: 3.436368, mean_eps: 0.100000\n",
      " 1154339/2000000: episode: 1598, duration: 9.789s, episode steps: 449, steps per second:  46, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.746 [0.000, 5.000],  loss: 0.022360, mae: 2.815312, mean_q: 3.396719, mean_eps: 0.100000\n",
      " 1155189/2000000: episode: 1599, duration: 19.340s, episode steps: 850, steps per second:  44, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.085 [0.000, 5.000],  loss: 0.020876, mae: 2.856186, mean_q: 3.443858, mean_eps: 0.100000\n",
      " 1155610/2000000: episode: 1600, duration: 9.370s, episode steps: 421, steps per second:  45, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.023535, mae: 2.834682, mean_q: 3.415388, mean_eps: 0.100000\n",
      " 1156382/2000000: episode: 1601, duration: 17.105s, episode steps: 772, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.022958, mae: 2.841763, mean_q: 3.430643, mean_eps: 0.100000\n",
      " 1157000/2000000: episode: 1602, duration: 13.931s, episode steps: 618, steps per second:  44, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.020017, mae: 2.827835, mean_q: 3.409868, mean_eps: 0.100000\n",
      " 1158156/2000000: episode: 1603, duration: 25.711s, episode steps: 1156, steps per second:  45, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.822 [0.000, 5.000],  loss: 0.021556, mae: 2.825219, mean_q: 3.405845, mean_eps: 0.100000\n",
      " 1159206/2000000: episode: 1604, duration: 23.154s, episode steps: 1050, steps per second:  45, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.018724, mae: 2.825117, mean_q: 3.405326, mean_eps: 0.100000\n",
      " 1159665/2000000: episode: 1605, duration: 10.430s, episode steps: 459, steps per second:  44, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.016786, mae: 2.834364, mean_q: 3.419568, mean_eps: 0.100000\n",
      " 1160185/2000000: episode: 1606, duration: 11.436s, episode steps: 520, steps per second:  45, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.973 [0.000, 5.000],  loss: 0.020230, mae: 2.847794, mean_q: 3.433720, mean_eps: 0.100000\n",
      " 1161311/2000000: episode: 1607, duration: 24.658s, episode steps: 1126, steps per second:  46, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.019948, mae: 2.858585, mean_q: 3.446994, mean_eps: 0.100000\n",
      " 1162457/2000000: episode: 1608, duration: 26.007s, episode steps: 1146, steps per second:  44, episode reward: 23.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.019804, mae: 2.883298, mean_q: 3.478617, mean_eps: 0.100000\n",
      " 1163208/2000000: episode: 1609, duration: 16.874s, episode steps: 751, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.020570, mae: 2.873244, mean_q: 3.463719, mean_eps: 0.100000\n",
      " 1163837/2000000: episode: 1610, duration: 13.946s, episode steps: 629, steps per second:  45, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.008 [0.000, 5.000],  loss: 0.022075, mae: 2.873795, mean_q: 3.466679, mean_eps: 0.100000\n",
      " 1164701/2000000: episode: 1611, duration: 19.206s, episode steps: 864, steps per second:  45, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.019126, mae: 2.893670, mean_q: 3.491224, mean_eps: 0.100000\n",
      " 1165810/2000000: episode: 1612, duration: 24.421s, episode steps: 1109, steps per second:  45, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.789 [0.000, 5.000],  loss: 0.019690, mae: 2.866509, mean_q: 3.457534, mean_eps: 0.100000\n",
      " 1166743/2000000: episode: 1613, duration: 20.998s, episode steps: 933, steps per second:  44, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.019269, mae: 2.865931, mean_q: 3.455890, mean_eps: 0.100000\n",
      " 1167859/2000000: episode: 1614, duration: 25.240s, episode steps: 1116, steps per second:  44, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: 0.019581, mae: 2.867884, mean_q: 3.460408, mean_eps: 0.100000\n",
      " 1168722/2000000: episode: 1615, duration: 19.442s, episode steps: 863, steps per second:  44, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.024411, mae: 2.876108, mean_q: 3.468722, mean_eps: 0.100000\n",
      " 1169672/2000000: episode: 1616, duration: 21.358s, episode steps: 950, steps per second:  44, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.020047, mae: 2.876864, mean_q: 3.472476, mean_eps: 0.100000\n",
      " 1170161/2000000: episode: 1617, duration: 11.042s, episode steps: 489, steps per second:  44, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.019407, mae: 2.885486, mean_q: 3.482706, mean_eps: 0.100000\n",
      " 1170903/2000000: episode: 1618, duration: 16.560s, episode steps: 742, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.972 [0.000, 5.000],  loss: 0.019556, mae: 2.898287, mean_q: 3.497212, mean_eps: 0.100000\n",
      " 1171872/2000000: episode: 1619, duration: 21.710s, episode steps: 969, steps per second:  45, episode reward:  9.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.019167, mae: 2.886688, mean_q: 3.484256, mean_eps: 0.100000\n",
      " 1172974/2000000: episode: 1620, duration: 24.447s, episode steps: 1102, steps per second:  45, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.018464, mae: 2.876200, mean_q: 3.470188, mean_eps: 0.100000\n",
      " 1173762/2000000: episode: 1621, duration: 17.554s, episode steps: 788, steps per second:  45, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.021359, mae: 2.898159, mean_q: 3.496552, mean_eps: 0.100000\n",
      " 1174187/2000000: episode: 1622, duration: 9.467s, episode steps: 425, steps per second:  45, episode reward:  4.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.885 [0.000, 5.000],  loss: 0.024352, mae: 2.886329, mean_q: 3.481557, mean_eps: 0.100000\n",
      " 1175086/2000000: episode: 1623, duration: 19.827s, episode steps: 899, steps per second:  45, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.021068, mae: 2.898796, mean_q: 3.496127, mean_eps: 0.100000\n",
      " 1175765/2000000: episode: 1624, duration: 15.297s, episode steps: 679, steps per second:  44, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.022082, mae: 2.896540, mean_q: 3.491719, mean_eps: 0.100000\n",
      " 1176327/2000000: episode: 1625, duration: 12.678s, episode steps: 562, steps per second:  44, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.024252, mae: 2.903148, mean_q: 3.503372, mean_eps: 0.100000\n",
      " 1177117/2000000: episode: 1626, duration: 18.297s, episode steps: 790, steps per second:  43, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.018938, mae: 2.882034, mean_q: 3.475692, mean_eps: 0.100000\n",
      " 1178281/2000000: episode: 1627, duration: 26.596s, episode steps: 1164, steps per second:  44, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.934 [0.000, 5.000],  loss: 0.020750, mae: 2.886153, mean_q: 3.480129, mean_eps: 0.100000\n",
      " 1178946/2000000: episode: 1628, duration: 14.718s, episode steps: 665, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.263 [0.000, 5.000],  loss: 0.024337, mae: 2.858534, mean_q: 3.446546, mean_eps: 0.100000\n",
      " 1179832/2000000: episode: 1629, duration: 19.634s, episode steps: 886, steps per second:  45, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.702 [0.000, 5.000],  loss: 0.019958, mae: 2.877164, mean_q: 3.469337, mean_eps: 0.100000\n",
      " 1180920/2000000: episode: 1630, duration: 24.104s, episode steps: 1088, steps per second:  45, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.676 [0.000, 5.000],  loss: 0.020145, mae: 2.913799, mean_q: 3.516493, mean_eps: 0.100000\n",
      " 1181619/2000000: episode: 1631, duration: 15.344s, episode steps: 699, steps per second:  46, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.418 [0.000, 5.000],  loss: 0.018949, mae: 2.909488, mean_q: 3.505915, mean_eps: 0.100000\n",
      " 1182079/2000000: episode: 1632, duration: 10.132s, episode steps: 460, steps per second:  45, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.018236, mae: 2.906680, mean_q: 3.502643, mean_eps: 0.100000\n",
      " 1182725/2000000: episode: 1633, duration: 14.302s, episode steps: 646, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.689 [0.000, 5.000],  loss: 0.022474, mae: 2.905318, mean_q: 3.504616, mean_eps: 0.100000\n",
      " 1183813/2000000: episode: 1634, duration: 24.112s, episode steps: 1088, steps per second:  45, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.882 [0.000, 5.000],  loss: 0.018760, mae: 2.910692, mean_q: 3.511978, mean_eps: 0.100000\n",
      " 1184851/2000000: episode: 1635, duration: 22.518s, episode steps: 1038, steps per second:  46, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.020246, mae: 2.917954, mean_q: 3.519847, mean_eps: 0.100000\n",
      " 1185454/2000000: episode: 1636, duration: 13.340s, episode steps: 603, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.021751, mae: 2.895154, mean_q: 3.495162, mean_eps: 0.100000\n",
      " 1186840/2000000: episode: 1637, duration: 30.319s, episode steps: 1386, steps per second:  46, episode reward: 33.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.488 [0.000, 5.000],  loss: 0.020373, mae: 2.906746, mean_q: 3.506398, mean_eps: 0.100000\n",
      " 1187524/2000000: episode: 1638, duration: 15.132s, episode steps: 684, steps per second:  45, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.751 [0.000, 5.000],  loss: 0.019154, mae: 2.928804, mean_q: 3.533205, mean_eps: 0.100000\n",
      " 1188282/2000000: episode: 1639, duration: 16.802s, episode steps: 758, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.265 [0.000, 5.000],  loss: 0.018133, mae: 2.888391, mean_q: 3.483644, mean_eps: 0.100000\n",
      " 1189369/2000000: episode: 1640, duration: 23.941s, episode steps: 1087, steps per second:  45, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.987 [0.000, 5.000],  loss: 0.020535, mae: 2.902174, mean_q: 3.502764, mean_eps: 0.100000\n",
      " 1190002/2000000: episode: 1641, duration: 13.830s, episode steps: 633, steps per second:  46, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.602 [0.000, 5.000],  loss: 0.021864, mae: 2.909925, mean_q: 3.507681, mean_eps: 0.100000\n",
      " 1190947/2000000: episode: 1642, duration: 20.893s, episode steps: 945, steps per second:  45, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.549 [0.000, 5.000],  loss: 0.019494, mae: 2.900228, mean_q: 3.499265, mean_eps: 0.100000\n",
      " 1191575/2000000: episode: 1643, duration: 13.886s, episode steps: 628, steps per second:  45, episode reward: 18.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.016610, mae: 2.879955, mean_q: 3.474820, mean_eps: 0.100000\n",
      " 1192253/2000000: episode: 1644, duration: 14.778s, episode steps: 678, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.056 [0.000, 5.000],  loss: 0.019147, mae: 2.889123, mean_q: 3.484669, mean_eps: 0.100000\n",
      " 1192632/2000000: episode: 1645, duration: 8.391s, episode steps: 379, steps per second:  45, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.022657, mae: 2.887849, mean_q: 3.487198, mean_eps: 0.100000\n",
      " 1193329/2000000: episode: 1646, duration: 15.597s, episode steps: 697, steps per second:  45, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.154 [0.000, 5.000],  loss: 0.020945, mae: 2.870127, mean_q: 3.465276, mean_eps: 0.100000\n",
      " 1194000/2000000: episode: 1647, duration: 14.971s, episode steps: 671, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.073 [0.000, 5.000],  loss: 0.020046, mae: 2.902418, mean_q: 3.500700, mean_eps: 0.100000\n",
      " 1194884/2000000: episode: 1648, duration: 19.674s, episode steps: 884, steps per second:  45, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.650 [0.000, 5.000],  loss: 0.021077, mae: 2.911078, mean_q: 3.510395, mean_eps: 0.100000\n",
      " 1195508/2000000: episode: 1649, duration: 13.930s, episode steps: 624, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.546 [0.000, 5.000],  loss: 0.020777, mae: 2.873509, mean_q: 3.466303, mean_eps: 0.100000\n",
      " 1196019/2000000: episode: 1650, duration: 11.430s, episode steps: 511, steps per second:  45, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.841 [0.000, 5.000],  loss: 0.021356, mae: 2.891317, mean_q: 3.490860, mean_eps: 0.100000\n",
      " 1197175/2000000: episode: 1651, duration: 25.622s, episode steps: 1156, steps per second:  45, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.020474, mae: 2.904243, mean_q: 3.503739, mean_eps: 0.100000\n",
      " 1198272/2000000: episode: 1652, duration: 23.996s, episode steps: 1097, steps per second:  46, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.022649, mae: 2.905457, mean_q: 3.506232, mean_eps: 0.100000\n",
      " 1198621/2000000: episode: 1653, duration: 7.768s, episode steps: 349, steps per second:  45, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.021287, mae: 2.911647, mean_q: 3.512650, mean_eps: 0.100000\n",
      " 1199403/2000000: episode: 1654, duration: 17.067s, episode steps: 782, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.010 [0.000, 5.000],  loss: 0.020713, mae: 2.893457, mean_q: 3.489706, mean_eps: 0.100000\n",
      " 1200611/2000000: episode: 1655, duration: 26.700s, episode steps: 1208, steps per second:  45, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.020562, mae: 2.908221, mean_q: 3.508895, mean_eps: 0.100000\n",
      " 1201241/2000000: episode: 1656, duration: 14.081s, episode steps: 630, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.019259, mae: 2.927218, mean_q: 3.534325, mean_eps: 0.100000\n",
      " 1201866/2000000: episode: 1657, duration: 13.825s, episode steps: 625, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.954 [0.000, 5.000],  loss: 0.019872, mae: 2.929045, mean_q: 3.534625, mean_eps: 0.100000\n",
      " 1202236/2000000: episode: 1658, duration: 8.010s, episode steps: 370, steps per second:  46, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.408 [0.000, 5.000],  loss: 0.020172, mae: 2.892729, mean_q: 3.488790, mean_eps: 0.100000\n",
      " 1203187/2000000: episode: 1659, duration: 20.942s, episode steps: 951, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.020816, mae: 2.914991, mean_q: 3.516684, mean_eps: 0.100000\n",
      " 1204379/2000000: episode: 1660, duration: 26.125s, episode steps: 1192, steps per second:  46, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.954 [0.000, 5.000],  loss: 0.020305, mae: 2.902443, mean_q: 3.500606, mean_eps: 0.100000\n",
      " 1204967/2000000: episode: 1661, duration: 12.935s, episode steps: 588, steps per second:  45, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.912 [0.000, 5.000],  loss: 0.019665, mae: 2.909846, mean_q: 3.513067, mean_eps: 0.100000\n",
      " 1206013/2000000: episode: 1662, duration: 23.239s, episode steps: 1046, steps per second:  45, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.854 [0.000, 5.000],  loss: 0.019298, mae: 2.892285, mean_q: 3.487195, mean_eps: 0.100000\n",
      " 1206552/2000000: episode: 1663, duration: 12.006s, episode steps: 539, steps per second:  45, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.018263, mae: 2.927587, mean_q: 3.532913, mean_eps: 0.100000\n",
      " 1207166/2000000: episode: 1664, duration: 13.431s, episode steps: 614, steps per second:  46, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.023062, mae: 2.912519, mean_q: 3.514409, mean_eps: 0.100000\n",
      " 1207703/2000000: episode: 1665, duration: 11.723s, episode steps: 537, steps per second:  46, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.020636, mae: 2.901240, mean_q: 3.501096, mean_eps: 0.100000\n",
      " 1208903/2000000: episode: 1666, duration: 26.357s, episode steps: 1200, steps per second:  46, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.012 [0.000, 5.000],  loss: 0.020057, mae: 2.912274, mean_q: 3.513331, mean_eps: 0.100000\n",
      " 1209847/2000000: episode: 1667, duration: 21.036s, episode steps: 944, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.020201, mae: 2.918432, mean_q: 3.519328, mean_eps: 0.100000\n",
      " 1210340/2000000: episode: 1668, duration: 10.701s, episode steps: 493, steps per second:  46, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.022529, mae: 2.907573, mean_q: 3.507388, mean_eps: 0.100000\n",
      " 1211426/2000000: episode: 1669, duration: 23.996s, episode steps: 1086, steps per second:  45, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.283 [0.000, 5.000],  loss: 0.018664, mae: 2.927225, mean_q: 3.531815, mean_eps: 0.100000\n",
      " 1212264/2000000: episode: 1670, duration: 18.496s, episode steps: 838, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.662 [0.000, 5.000],  loss: 0.018623, mae: 2.916560, mean_q: 3.517893, mean_eps: 0.100000\n",
      " 1213352/2000000: episode: 1671, duration: 24.257s, episode steps: 1088, steps per second:  45, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.020472, mae: 2.932655, mean_q: 3.536695, mean_eps: 0.100000\n",
      " 1214402/2000000: episode: 1672, duration: 23.103s, episode steps: 1050, steps per second:  45, episode reward: 20.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.207 [0.000, 5.000],  loss: 0.019978, mae: 2.926585, mean_q: 3.527646, mean_eps: 0.100000\n",
      " 1215108/2000000: episode: 1673, duration: 15.985s, episode steps: 706, steps per second:  44, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.019927, mae: 2.944932, mean_q: 3.551919, mean_eps: 0.100000\n",
      " 1215882/2000000: episode: 1674, duration: 16.852s, episode steps: 774, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.305 [0.000, 5.000],  loss: 0.019127, mae: 2.925090, mean_q: 3.528131, mean_eps: 0.100000\n",
      " 1216457/2000000: episode: 1675, duration: 12.895s, episode steps: 575, steps per second:  45, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.896 [0.000, 5.000],  loss: 0.019492, mae: 2.909838, mean_q: 3.507844, mean_eps: 0.100000\n",
      " 1217690/2000000: episode: 1676, duration: 27.398s, episode steps: 1233, steps per second:  45, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.998 [0.000, 5.000],  loss: 0.020162, mae: 2.923220, mean_q: 3.525122, mean_eps: 0.100000\n",
      " 1218823/2000000: episode: 1677, duration: 25.076s, episode steps: 1133, steps per second:  45, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.021103, mae: 2.944178, mean_q: 3.551231, mean_eps: 0.100000\n",
      " 1219751/2000000: episode: 1678, duration: 20.405s, episode steps: 928, steps per second:  45, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.022038, mae: 2.916543, mean_q: 3.519353, mean_eps: 0.100000\n",
      " 1220522/2000000: episode: 1679, duration: 17.137s, episode steps: 771, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.021576, mae: 2.938690, mean_q: 3.545018, mean_eps: 0.100000\n",
      " 1221917/2000000: episode: 1680, duration: 31.302s, episode steps: 1395, steps per second:  45, episode reward: 28.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.020417, mae: 2.970211, mean_q: 3.583476, mean_eps: 0.100000\n",
      " 1222791/2000000: episode: 1681, duration: 19.291s, episode steps: 874, steps per second:  45, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.907 [0.000, 5.000],  loss: 0.021892, mae: 2.969294, mean_q: 3.583642, mean_eps: 0.100000\n",
      " 1223259/2000000: episode: 1682, duration: 10.327s, episode steps: 468, steps per second:  45, episode reward: 11.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.036 [0.000, 5.000],  loss: 0.023904, mae: 2.975307, mean_q: 3.591963, mean_eps: 0.100000\n",
      " 1224108/2000000: episode: 1683, duration: 18.707s, episode steps: 849, steps per second:  45, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.994 [0.000, 5.000],  loss: 0.020739, mae: 2.994148, mean_q: 3.613271, mean_eps: 0.100000\n",
      " 1224634/2000000: episode: 1684, duration: 11.690s, episode steps: 526, steps per second:  45, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.521 [0.000, 5.000],  loss: 0.017666, mae: 2.961267, mean_q: 3.572793, mean_eps: 0.100000\n",
      " 1225774/2000000: episode: 1685, duration: 25.178s, episode steps: 1140, steps per second:  45, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.020292, mae: 2.964483, mean_q: 3.574913, mean_eps: 0.100000\n",
      " 1226842/2000000: episode: 1686, duration: 23.378s, episode steps: 1068, steps per second:  46, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.021109, mae: 2.967630, mean_q: 3.581555, mean_eps: 0.100000\n",
      " 1227384/2000000: episode: 1687, duration: 12.421s, episode steps: 542, steps per second:  44, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.009 [0.000, 5.000],  loss: 0.019632, mae: 2.957203, mean_q: 3.566729, mean_eps: 0.100000\n",
      " 1228419/2000000: episode: 1688, duration: 22.974s, episode steps: 1035, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.020692, mae: 2.966053, mean_q: 3.579858, mean_eps: 0.100000\n",
      " 1229247/2000000: episode: 1689, duration: 18.259s, episode steps: 828, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.019114, mae: 2.971472, mean_q: 3.584435, mean_eps: 0.100000\n",
      " 1229630/2000000: episode: 1690, duration: 8.773s, episode steps: 383, steps per second:  44, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.232 [0.000, 5.000],  loss: 0.020169, mae: 2.974013, mean_q: 3.585364, mean_eps: 0.100000\n",
      " 1230488/2000000: episode: 1691, duration: 18.722s, episode steps: 858, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.580 [0.000, 5.000],  loss: 0.019715, mae: 2.998918, mean_q: 3.616062, mean_eps: 0.100000\n",
      " 1231460/2000000: episode: 1692, duration: 21.811s, episode steps: 972, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.264 [0.000, 5.000],  loss: 0.020552, mae: 3.005541, mean_q: 3.623456, mean_eps: 0.100000\n",
      " 1232131/2000000: episode: 1693, duration: 14.839s, episode steps: 671, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.019574, mae: 2.998283, mean_q: 3.615303, mean_eps: 0.100000\n",
      " 1232528/2000000: episode: 1694, duration: 9.339s, episode steps: 397, steps per second:  43, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.242 [0.000, 5.000],  loss: 0.021406, mae: 2.979378, mean_q: 3.592258, mean_eps: 0.100000\n",
      " 1233344/2000000: episode: 1695, duration: 18.505s, episode steps: 816, steps per second:  44, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.023666, mae: 2.988279, mean_q: 3.603263, mean_eps: 0.100000\n",
      " 1233953/2000000: episode: 1696, duration: 13.862s, episode steps: 609, steps per second:  44, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.153 [0.000, 5.000],  loss: 0.019876, mae: 3.003933, mean_q: 3.622952, mean_eps: 0.100000\n",
      " 1234337/2000000: episode: 1697, duration: 8.597s, episode steps: 384, steps per second:  45, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.016011, mae: 2.997786, mean_q: 3.616679, mean_eps: 0.100000\n",
      " 1235358/2000000: episode: 1698, duration: 22.825s, episode steps: 1021, steps per second:  45, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.019428, mae: 2.988867, mean_q: 3.603404, mean_eps: 0.100000\n",
      " 1236059/2000000: episode: 1699, duration: 15.288s, episode steps: 701, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.889 [0.000, 5.000],  loss: 0.021078, mae: 3.005112, mean_q: 3.624591, mean_eps: 0.100000\n",
      " 1237012/2000000: episode: 1700, duration: 21.366s, episode steps: 953, steps per second:  45, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.703 [0.000, 5.000],  loss: 0.020258, mae: 2.976675, mean_q: 3.590022, mean_eps: 0.100000\n",
      " 1237892/2000000: episode: 1701, duration: 19.779s, episode steps: 880, steps per second:  44, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.020392, mae: 3.006931, mean_q: 3.625468, mean_eps: 0.100000\n",
      " 1239544/2000000: episode: 1702, duration: 37.223s, episode steps: 1652, steps per second:  44, episode reward: 33.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.022109, mae: 2.997923, mean_q: 3.614164, mean_eps: 0.100000\n",
      " 1240114/2000000: episode: 1703, duration: 12.945s, episode steps: 570, steps per second:  44, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.940 [0.000, 5.000],  loss: 0.022200, mae: 2.995765, mean_q: 3.611262, mean_eps: 0.100000\n",
      " 1240903/2000000: episode: 1704, duration: 17.634s, episode steps: 789, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.017678, mae: 2.987161, mean_q: 3.604240, mean_eps: 0.100000\n",
      " 1241842/2000000: episode: 1705, duration: 20.771s, episode steps: 939, steps per second:  45, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.370 [0.000, 5.000],  loss: 0.018291, mae: 2.991083, mean_q: 3.607965, mean_eps: 0.100000\n",
      " 1242805/2000000: episode: 1706, duration: 21.044s, episode steps: 963, steps per second:  46, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.226 [0.000, 5.000],  loss: 0.020630, mae: 3.002331, mean_q: 3.622590, mean_eps: 0.100000\n",
      " 1243410/2000000: episode: 1707, duration: 13.832s, episode steps: 605, steps per second:  44, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.425 [0.000, 5.000],  loss: 0.020098, mae: 2.998321, mean_q: 3.614345, mean_eps: 0.100000\n",
      " 1244190/2000000: episode: 1708, duration: 17.402s, episode steps: 780, steps per second:  45, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.169 [0.000, 5.000],  loss: 0.022550, mae: 2.998676, mean_q: 3.615409, mean_eps: 0.100000\n",
      " 1245057/2000000: episode: 1709, duration: 19.323s, episode steps: 867, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.098 [0.000, 5.000],  loss: 0.020260, mae: 2.996763, mean_q: 3.614592, mean_eps: 0.100000\n",
      " 1245539/2000000: episode: 1710, duration: 10.530s, episode steps: 482, steps per second:  46, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.988 [0.000, 5.000],  loss: 0.020198, mae: 2.986803, mean_q: 3.599847, mean_eps: 0.100000\n",
      " 1246167/2000000: episode: 1711, duration: 14.061s, episode steps: 628, steps per second:  45, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.019083, mae: 2.969246, mean_q: 3.581361, mean_eps: 0.100000\n",
      " 1246967/2000000: episode: 1712, duration: 17.735s, episode steps: 800, steps per second:  45, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.020683, mae: 2.996339, mean_q: 3.612358, mean_eps: 0.100000\n",
      " 1247595/2000000: episode: 1713, duration: 13.926s, episode steps: 628, steps per second:  45, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.279 [0.000, 5.000],  loss: 0.019883, mae: 2.991214, mean_q: 3.608066, mean_eps: 0.100000\n",
      " 1248191/2000000: episode: 1714, duration: 12.949s, episode steps: 596, steps per second:  46, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.727 [0.000, 5.000],  loss: 0.022030, mae: 2.986263, mean_q: 3.600036, mean_eps: 0.100000\n",
      " 1249111/2000000: episode: 1715, duration: 20.342s, episode steps: 920, steps per second:  45, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.020768, mae: 3.001443, mean_q: 3.618868, mean_eps: 0.100000\n",
      " 1250332/2000000: episode: 1716, duration: 27.477s, episode steps: 1221, steps per second:  44, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.020773, mae: 3.010409, mean_q: 3.629470, mean_eps: 0.100000\n",
      " 1250877/2000000: episode: 1717, duration: 12.065s, episode steps: 545, steps per second:  45, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.750 [0.000, 5.000],  loss: 0.018941, mae: 2.997626, mean_q: 3.614744, mean_eps: 0.100000\n",
      " 1251463/2000000: episode: 1718, duration: 12.888s, episode steps: 586, steps per second:  45, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.020074, mae: 2.993030, mean_q: 3.610730, mean_eps: 0.100000\n",
      " 1252272/2000000: episode: 1719, duration: 17.727s, episode steps: 809, steps per second:  46, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.020562, mae: 2.998442, mean_q: 3.614580, mean_eps: 0.100000\n",
      " 1253106/2000000: episode: 1720, duration: 19.686s, episode steps: 834, steps per second:  42, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.020236, mae: 2.999529, mean_q: 3.620069, mean_eps: 0.100000\n",
      " 1253873/2000000: episode: 1721, duration: 17.282s, episode steps: 767, steps per second:  44, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.288 [0.000, 5.000],  loss: 0.020834, mae: 3.017237, mean_q: 3.640948, mean_eps: 0.100000\n",
      " 1254897/2000000: episode: 1722, duration: 22.468s, episode steps: 1024, steps per second:  46, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.286 [0.000, 5.000],  loss: 0.022678, mae: 3.016634, mean_q: 3.639447, mean_eps: 0.100000\n",
      " 1255664/2000000: episode: 1723, duration: 17.091s, episode steps: 767, steps per second:  45, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.587 [0.000, 5.000],  loss: 0.023332, mae: 3.008258, mean_q: 3.628283, mean_eps: 0.100000\n",
      " 1256392/2000000: episode: 1724, duration: 16.301s, episode steps: 728, steps per second:  45, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.566 [0.000, 5.000],  loss: 0.021568, mae: 3.001295, mean_q: 3.618063, mean_eps: 0.100000\n",
      " 1257338/2000000: episode: 1725, duration: 20.916s, episode steps: 946, steps per second:  45, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.344 [0.000, 5.000],  loss: 0.019903, mae: 3.027981, mean_q: 3.651368, mean_eps: 0.100000\n",
      " 1258316/2000000: episode: 1726, duration: 21.640s, episode steps: 978, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.944 [0.000, 5.000],  loss: 0.020768, mae: 2.990677, mean_q: 3.606560, mean_eps: 0.100000\n",
      " 1259081/2000000: episode: 1727, duration: 17.117s, episode steps: 765, steps per second:  45, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.935 [0.000, 5.000],  loss: 0.019174, mae: 2.992439, mean_q: 3.608327, mean_eps: 0.100000\n",
      " 1260006/2000000: episode: 1728, duration: 20.520s, episode steps: 925, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.681 [0.000, 5.000],  loss: 0.021467, mae: 3.013130, mean_q: 3.633317, mean_eps: 0.100000\n",
      " 1260685/2000000: episode: 1729, duration: 14.981s, episode steps: 679, steps per second:  45, episode reward: 19.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.695 [0.000, 5.000],  loss: 0.020835, mae: 3.013686, mean_q: 3.635629, mean_eps: 0.100000\n",
      " 1262004/2000000: episode: 1730, duration: 29.126s, episode steps: 1319, steps per second:  45, episode reward: 26.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.191 [0.000, 5.000],  loss: 0.020166, mae: 3.030197, mean_q: 3.654817, mean_eps: 0.100000\n",
      " 1263116/2000000: episode: 1731, duration: 24.503s, episode steps: 1112, steps per second:  45, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.019900, mae: 3.022281, mean_q: 3.646232, mean_eps: 0.100000\n",
      " 1264072/2000000: episode: 1732, duration: 21.141s, episode steps: 956, steps per second:  45, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.022186, mae: 3.038435, mean_q: 3.665535, mean_eps: 0.100000\n",
      " 1264743/2000000: episode: 1733, duration: 14.755s, episode steps: 671, steps per second:  45, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.019177, mae: 3.009487, mean_q: 3.630625, mean_eps: 0.100000\n",
      " 1265258/2000000: episode: 1734, duration: 11.656s, episode steps: 515, steps per second:  44, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.332 [0.000, 5.000],  loss: 0.022368, mae: 3.008232, mean_q: 3.627126, mean_eps: 0.100000\n",
      " 1266145/2000000: episode: 1735, duration: 19.620s, episode steps: 887, steps per second:  45, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.021220, mae: 3.013197, mean_q: 3.634371, mean_eps: 0.100000\n",
      " 1266615/2000000: episode: 1736, duration: 10.243s, episode steps: 470, steps per second:  46, episode reward:  8.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.696 [0.000, 5.000],  loss: 0.019426, mae: 2.998131, mean_q: 3.615827, mean_eps: 0.100000\n",
      " 1267281/2000000: episode: 1737, duration: 14.763s, episode steps: 666, steps per second:  45, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.020234, mae: 3.012361, mean_q: 3.632531, mean_eps: 0.100000\n",
      " 1268070/2000000: episode: 1738, duration: 17.358s, episode steps: 789, steps per second:  45, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.022544, mae: 3.024266, mean_q: 3.649077, mean_eps: 0.100000\n",
      " 1268646/2000000: episode: 1739, duration: 12.825s, episode steps: 576, steps per second:  45, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.022170, mae: 3.016707, mean_q: 3.639568, mean_eps: 0.100000\n",
      " 1269263/2000000: episode: 1740, duration: 13.444s, episode steps: 617, steps per second:  46, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.023576, mae: 2.999250, mean_q: 3.617335, mean_eps: 0.100000\n",
      " 1270037/2000000: episode: 1741, duration: 17.155s, episode steps: 774, steps per second:  45, episode reward: 17.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.282 [0.000, 5.000],  loss: 0.016340, mae: 3.012690, mean_q: 3.633327, mean_eps: 0.100000\n",
      " 1271198/2000000: episode: 1742, duration: 25.909s, episode steps: 1161, steps per second:  45, episode reward: 28.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.018868, mae: 3.051174, mean_q: 3.682098, mean_eps: 0.100000\n",
      " 1271926/2000000: episode: 1743, duration: 16.034s, episode steps: 728, steps per second:  45, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.135 [0.000, 5.000],  loss: 0.020814, mae: 3.019350, mean_q: 3.644845, mean_eps: 0.100000\n",
      " 1272631/2000000: episode: 1744, duration: 15.461s, episode steps: 705, steps per second:  46, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.021589, mae: 3.034768, mean_q: 3.659735, mean_eps: 0.100000\n",
      " 1273656/2000000: episode: 1745, duration: 22.536s, episode steps: 1025, steps per second:  45, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.736 [0.000, 5.000],  loss: 0.021386, mae: 3.048373, mean_q: 3.678133, mean_eps: 0.100000\n",
      " 1274656/2000000: episode: 1746, duration: 22.322s, episode steps: 1000, steps per second:  45, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.019013, mae: 3.045778, mean_q: 3.675435, mean_eps: 0.100000\n",
      " 1275432/2000000: episode: 1747, duration: 17.096s, episode steps: 776, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.022061, mae: 3.055300, mean_q: 3.684664, mean_eps: 0.100000\n",
      " 1276379/2000000: episode: 1748, duration: 21.020s, episode steps: 947, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.699 [0.000, 5.000],  loss: 0.022791, mae: 3.045656, mean_q: 3.672739, mean_eps: 0.100000\n",
      " 1277267/2000000: episode: 1749, duration: 19.659s, episode steps: 888, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.021595, mae: 3.033517, mean_q: 3.656403, mean_eps: 0.100000\n",
      " 1277765/2000000: episode: 1750, duration: 11.026s, episode steps: 498, steps per second:  45, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.472 [0.000, 5.000],  loss: 0.020422, mae: 3.038003, mean_q: 3.661729, mean_eps: 0.100000\n",
      " 1278849/2000000: episode: 1751, duration: 23.839s, episode steps: 1084, steps per second:  45, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.639 [0.000, 5.000],  loss: 0.021489, mae: 3.060632, mean_q: 3.689387, mean_eps: 0.100000\n",
      " 1279865/2000000: episode: 1752, duration: 22.511s, episode steps: 1016, steps per second:  45, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.022143, mae: 3.040908, mean_q: 3.668391, mean_eps: 0.100000\n",
      " 1280781/2000000: episode: 1753, duration: 20.057s, episode steps: 916, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.453 [0.000, 5.000],  loss: 0.020615, mae: 3.050760, mean_q: 3.679685, mean_eps: 0.100000\n",
      " 1281977/2000000: episode: 1754, duration: 26.469s, episode steps: 1196, steps per second:  45, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.561 [0.000, 5.000],  loss: 0.022029, mae: 3.053095, mean_q: 3.680548, mean_eps: 0.100000\n",
      " 1282662/2000000: episode: 1755, duration: 15.132s, episode steps: 685, steps per second:  45, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.019331, mae: 3.047666, mean_q: 3.676009, mean_eps: 0.100000\n",
      " 1283299/2000000: episode: 1756, duration: 13.870s, episode steps: 637, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.268 [0.000, 5.000],  loss: 0.020610, mae: 3.029415, mean_q: 3.652965, mean_eps: 0.100000\n",
      " 1284225/2000000: episode: 1757, duration: 20.521s, episode steps: 926, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.021933, mae: 3.045000, mean_q: 3.670821, mean_eps: 0.100000\n",
      " 1285066/2000000: episode: 1758, duration: 18.346s, episode steps: 841, steps per second:  46, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.018310, mae: 3.038812, mean_q: 3.665589, mean_eps: 0.100000\n",
      " 1285705/2000000: episode: 1759, duration: 14.017s, episode steps: 639, steps per second:  46, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.018810, mae: 3.043474, mean_q: 3.668697, mean_eps: 0.100000\n",
      " 1286359/2000000: episode: 1760, duration: 14.561s, episode steps: 654, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.901 [0.000, 5.000],  loss: 0.022625, mae: 3.049449, mean_q: 3.676242, mean_eps: 0.100000\n",
      " 1286927/2000000: episode: 1761, duration: 12.394s, episode steps: 568, steps per second:  46, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.931 [0.000, 5.000],  loss: 0.023629, mae: 3.047982, mean_q: 3.675035, mean_eps: 0.100000\n",
      " 1287823/2000000: episode: 1762, duration: 20.263s, episode steps: 896, steps per second:  44, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.021418, mae: 3.043984, mean_q: 3.670167, mean_eps: 0.100000\n",
      " 1288889/2000000: episode: 1763, duration: 23.559s, episode steps: 1066, steps per second:  45, episode reward: 19.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.021501, mae: 3.032748, mean_q: 3.655611, mean_eps: 0.100000\n",
      " 1289670/2000000: episode: 1764, duration: 17.042s, episode steps: 781, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.021372, mae: 3.058582, mean_q: 3.688098, mean_eps: 0.100000\n",
      " 1290312/2000000: episode: 1765, duration: 14.171s, episode steps: 642, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.586 [0.000, 5.000],  loss: 0.020275, mae: 3.067967, mean_q: 3.699238, mean_eps: 0.100000\n",
      " 1291333/2000000: episode: 1766, duration: 22.587s, episode steps: 1021, steps per second:  45, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.019482, mae: 3.063252, mean_q: 3.694316, mean_eps: 0.100000\n",
      " 1291839/2000000: episode: 1767, duration: 11.148s, episode steps: 506, steps per second:  45, episode reward: 11.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.921 [0.000, 5.000],  loss: 0.019424, mae: 3.070119, mean_q: 3.704220, mean_eps: 0.100000\n",
      " 1292761/2000000: episode: 1768, duration: 20.600s, episode steps: 922, steps per second:  45, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.020262, mae: 3.073339, mean_q: 3.706209, mean_eps: 0.100000\n",
      " 1293135/2000000: episode: 1769, duration: 8.542s, episode steps: 374, steps per second:  44, episode reward:  7.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.019817, mae: 3.061976, mean_q: 3.692310, mean_eps: 0.100000\n",
      " 1294348/2000000: episode: 1770, duration: 27.411s, episode steps: 1213, steps per second:  44, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.019590, mae: 3.045646, mean_q: 3.672067, mean_eps: 0.100000\n",
      " 1294911/2000000: episode: 1771, duration: 12.716s, episode steps: 563, steps per second:  44, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.020176, mae: 3.083587, mean_q: 3.719234, mean_eps: 0.100000\n",
      " 1295586/2000000: episode: 1772, duration: 14.801s, episode steps: 675, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.819 [0.000, 5.000],  loss: 0.019222, mae: 3.048591, mean_q: 3.675371, mean_eps: 0.100000\n",
      " 1296565/2000000: episode: 1773, duration: 21.891s, episode steps: 979, steps per second:  45, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.758 [0.000, 5.000],  loss: 0.021149, mae: 3.069138, mean_q: 3.703153, mean_eps: 0.100000\n",
      " 1297633/2000000: episode: 1774, duration: 23.586s, episode steps: 1068, steps per second:  45, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.960 [0.000, 5.000],  loss: 0.019602, mae: 3.071731, mean_q: 3.704376, mean_eps: 0.100000\n",
      " 1298163/2000000: episode: 1775, duration: 11.934s, episode steps: 530, steps per second:  44, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.024067, mae: 3.078639, mean_q: 3.708832, mean_eps: 0.100000\n",
      " 1299212/2000000: episode: 1776, duration: 23.509s, episode steps: 1049, steps per second:  45, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.022237, mae: 3.092584, mean_q: 3.727528, mean_eps: 0.100000\n",
      " 1299701/2000000: episode: 1777, duration: 10.790s, episode steps: 489, steps per second:  45, episode reward:  9.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.677 [0.000, 5.000],  loss: 0.018962, mae: 3.089904, mean_q: 3.723432, mean_eps: 0.100000\n",
      " 1300916/2000000: episode: 1778, duration: 26.707s, episode steps: 1215, steps per second:  45, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.017486, mae: 3.054931, mean_q: 3.684402, mean_eps: 0.100000\n",
      " 1301601/2000000: episode: 1779, duration: 15.177s, episode steps: 685, steps per second:  45, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.021365, mae: 3.077375, mean_q: 3.711889, mean_eps: 0.100000\n",
      " 1302154/2000000: episode: 1780, duration: 12.010s, episode steps: 553, steps per second:  46, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.019423, mae: 3.076608, mean_q: 3.709747, mean_eps: 0.100000\n",
      " 1303554/2000000: episode: 1781, duration: 30.703s, episode steps: 1400, steps per second:  46, episode reward: 34.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.019516, mae: 3.076754, mean_q: 3.708531, mean_eps: 0.100000\n",
      " 1304343/2000000: episode: 1782, duration: 17.718s, episode steps: 789, steps per second:  45, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.021182, mae: 3.087160, mean_q: 3.721306, mean_eps: 0.100000\n",
      " 1305569/2000000: episode: 1783, duration: 26.843s, episode steps: 1226, steps per second:  46, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.022362, mae: 3.097567, mean_q: 3.734263, mean_eps: 0.100000\n",
      " 1306333/2000000: episode: 1784, duration: 16.603s, episode steps: 764, steps per second:  46, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.021924, mae: 3.090246, mean_q: 3.724552, mean_eps: 0.100000\n",
      " 1307227/2000000: episode: 1785, duration: 19.816s, episode steps: 894, steps per second:  45, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.020502, mae: 3.082706, mean_q: 3.716567, mean_eps: 0.100000\n",
      " 1308352/2000000: episode: 1786, duration: 24.636s, episode steps: 1125, steps per second:  46, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.291 [0.000, 5.000],  loss: 0.020005, mae: 3.086428, mean_q: 3.718750, mean_eps: 0.100000\n",
      " 1308955/2000000: episode: 1787, duration: 13.293s, episode steps: 603, steps per second:  45, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.947 [0.000, 5.000],  loss: 0.017839, mae: 3.101312, mean_q: 3.739661, mean_eps: 0.100000\n",
      " 1309477/2000000: episode: 1788, duration: 11.792s, episode steps: 522, steps per second:  44, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.155 [0.000, 5.000],  loss: 0.019150, mae: 3.092763, mean_q: 3.729374, mean_eps: 0.100000\n",
      " 1310449/2000000: episode: 1789, duration: 21.371s, episode steps: 972, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.017368, mae: 3.107056, mean_q: 3.747390, mean_eps: 0.100000\n",
      " 1311226/2000000: episode: 1790, duration: 17.006s, episode steps: 777, steps per second:  46, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.395 [0.000, 5.000],  loss: 0.019235, mae: 3.115628, mean_q: 3.757849, mean_eps: 0.100000\n",
      " 1311547/2000000: episode: 1791, duration: 7.126s, episode steps: 321, steps per second:  45, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.087 [0.000, 5.000],  loss: 0.024347, mae: 3.122210, mean_q: 3.761766, mean_eps: 0.100000\n",
      " 1312313/2000000: episode: 1792, duration: 17.003s, episode steps: 766, steps per second:  45, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.019765, mae: 3.140486, mean_q: 3.789194, mean_eps: 0.100000\n",
      " 1312698/2000000: episode: 1793, duration: 8.556s, episode steps: 385, steps per second:  45, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.016 [0.000, 5.000],  loss: 0.023899, mae: 3.121737, mean_q: 3.762898, mean_eps: 0.100000\n",
      " 1313639/2000000: episode: 1794, duration: 20.637s, episode steps: 941, steps per second:  46, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.019991, mae: 3.106623, mean_q: 3.746832, mean_eps: 0.100000\n",
      " 1314361/2000000: episode: 1795, duration: 15.642s, episode steps: 722, steps per second:  46, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.021818, mae: 3.133898, mean_q: 3.778481, mean_eps: 0.100000\n",
      " 1315974/2000000: episode: 1796, duration: 35.733s, episode steps: 1613, steps per second:  45, episode reward: 28.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.020516, mae: 3.125826, mean_q: 3.767661, mean_eps: 0.100000\n",
      " 1316917/2000000: episode: 1797, duration: 20.723s, episode steps: 943, steps per second:  46, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.018791, mae: 3.137160, mean_q: 3.783895, mean_eps: 0.100000\n",
      " 1317602/2000000: episode: 1798, duration: 15.113s, episode steps: 685, steps per second:  45, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.804 [0.000, 5.000],  loss: 0.022001, mae: 3.130299, mean_q: 3.772938, mean_eps: 0.100000\n",
      " 1318175/2000000: episode: 1799, duration: 12.753s, episode steps: 573, steps per second:  45, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.747 [0.000, 5.000],  loss: 0.023948, mae: 3.127068, mean_q: 3.767916, mean_eps: 0.100000\n",
      " 1318988/2000000: episode: 1800, duration: 18.002s, episode steps: 813, steps per second:  45, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.085 [0.000, 5.000],  loss: 0.023410, mae: 3.117689, mean_q: 3.756950, mean_eps: 0.100000\n",
      " 1319915/2000000: episode: 1801, duration: 20.582s, episode steps: 927, steps per second:  45, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.020514, mae: 3.127693, mean_q: 3.773978, mean_eps: 0.100000\n",
      " 1320313/2000000: episode: 1802, duration: 8.945s, episode steps: 398, steps per second:  44, episode reward:  8.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.030 [0.000, 5.000],  loss: 0.019837, mae: 3.125808, mean_q: 3.769806, mean_eps: 0.100000\n",
      " 1320848/2000000: episode: 1803, duration: 12.760s, episode steps: 535, steps per second:  42, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.641 [0.000, 5.000],  loss: 0.020834, mae: 3.134014, mean_q: 3.777722, mean_eps: 0.100000\n",
      " 1321375/2000000: episode: 1804, duration: 12.165s, episode steps: 527, steps per second:  43, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.575 [0.000, 5.000],  loss: 0.023036, mae: 3.148127, mean_q: 3.797055, mean_eps: 0.100000\n",
      " 1322134/2000000: episode: 1805, duration: 16.876s, episode steps: 759, steps per second:  45, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.022565, mae: 3.138913, mean_q: 3.785502, mean_eps: 0.100000\n",
      " 1322697/2000000: episode: 1806, duration: 12.401s, episode steps: 563, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.540 [0.000, 5.000],  loss: 0.022944, mae: 3.119347, mean_q: 3.759463, mean_eps: 0.100000\n",
      " 1323205/2000000: episode: 1807, duration: 11.429s, episode steps: 508, steps per second:  44, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.022904, mae: 3.148098, mean_q: 3.793772, mean_eps: 0.100000\n",
      " 1323909/2000000: episode: 1808, duration: 15.460s, episode steps: 704, steps per second:  46, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.018 [0.000, 5.000],  loss: 0.020799, mae: 3.146666, mean_q: 3.794447, mean_eps: 0.100000\n",
      " 1324279/2000000: episode: 1809, duration: 8.093s, episode steps: 370, steps per second:  46, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.023057, mae: 3.130337, mean_q: 3.773585, mean_eps: 0.100000\n",
      " 1325171/2000000: episode: 1810, duration: 19.472s, episode steps: 892, steps per second:  46, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.022716, mae: 3.139013, mean_q: 3.783460, mean_eps: 0.100000\n",
      " 1325940/2000000: episode: 1811, duration: 17.359s, episode steps: 769, steps per second:  44, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.020403, mae: 3.108687, mean_q: 3.745295, mean_eps: 0.100000\n",
      " 1326699/2000000: episode: 1812, duration: 16.562s, episode steps: 759, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.928 [0.000, 5.000],  loss: 0.021008, mae: 3.141136, mean_q: 3.787649, mean_eps: 0.100000\n",
      " 1327682/2000000: episode: 1813, duration: 21.794s, episode steps: 983, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.004 [0.000, 5.000],  loss: 0.018636, mae: 3.136704, mean_q: 3.781305, mean_eps: 0.100000\n",
      " 1328613/2000000: episode: 1814, duration: 20.338s, episode steps: 931, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.021077, mae: 3.135261, mean_q: 3.780040, mean_eps: 0.100000\n",
      " 1329243/2000000: episode: 1815, duration: 13.704s, episode steps: 630, steps per second:  46, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.211 [0.000, 5.000],  loss: 0.019702, mae: 3.130305, mean_q: 3.773141, mean_eps: 0.100000\n",
      " 1329789/2000000: episode: 1816, duration: 12.131s, episode steps: 546, steps per second:  45, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.987 [0.000, 5.000],  loss: 0.020269, mae: 3.127225, mean_q: 3.769189, mean_eps: 0.100000\n",
      " 1330312/2000000: episode: 1817, duration: 11.460s, episode steps: 523, steps per second:  46, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.018408, mae: 3.141587, mean_q: 3.789489, mean_eps: 0.100000\n",
      " 1330680/2000000: episode: 1818, duration: 8.151s, episode steps: 368, steps per second:  45, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.019288, mae: 3.167973, mean_q: 3.819653, mean_eps: 0.100000\n",
      " 1331459/2000000: episode: 1819, duration: 17.407s, episode steps: 779, steps per second:  45, episode reward: 13.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.403 [0.000, 5.000],  loss: 0.022198, mae: 3.147115, mean_q: 3.796457, mean_eps: 0.100000\n",
      " 1332556/2000000: episode: 1820, duration: 24.329s, episode steps: 1097, steps per second:  45, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.020863, mae: 3.167173, mean_q: 3.821195, mean_eps: 0.100000\n",
      " 1333775/2000000: episode: 1821, duration: 31.309s, episode steps: 1219, steps per second:  39, episode reward: 31.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.267 [0.000, 5.000],  loss: 0.024735, mae: 3.146662, mean_q: 3.794008, mean_eps: 0.100000\n",
      " 1334612/2000000: episode: 1822, duration: 20.096s, episode steps: 837, steps per second:  42, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.886 [0.000, 5.000],  loss: 0.024110, mae: 3.160205, mean_q: 3.808669, mean_eps: 0.100000\n",
      " 1335444/2000000: episode: 1823, duration: 20.104s, episode steps: 832, steps per second:  41, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.023290, mae: 3.173761, mean_q: 3.826439, mean_eps: 0.100000\n",
      " 1336103/2000000: episode: 1824, duration: 16.067s, episode steps: 659, steps per second:  41, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.021790, mae: 3.178170, mean_q: 3.830453, mean_eps: 0.100000\n",
      " 1336665/2000000: episode: 1825, duration: 13.843s, episode steps: 562, steps per second:  41, episode reward: 16.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.893 [0.000, 5.000],  loss: 0.017466, mae: 3.176197, mean_q: 3.830292, mean_eps: 0.100000\n",
      " 1337416/2000000: episode: 1826, duration: 18.233s, episode steps: 751, steps per second:  41, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.084 [0.000, 5.000],  loss: 0.021328, mae: 3.147107, mean_q: 3.795385, mean_eps: 0.100000\n",
      " 1337769/2000000: episode: 1827, duration: 8.698s, episode steps: 353, steps per second:  41, episode reward:  7.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.022758, mae: 3.157850, mean_q: 3.810520, mean_eps: 0.100000\n",
      " 1338963/2000000: episode: 1828, duration: 28.671s, episode steps: 1194, steps per second:  42, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.209 [0.000, 5.000],  loss: 0.019846, mae: 3.168999, mean_q: 3.820004, mean_eps: 0.100000\n",
      " 1339662/2000000: episode: 1829, duration: 16.226s, episode steps: 699, steps per second:  43, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.022663, mae: 3.142144, mean_q: 3.786740, mean_eps: 0.100000\n",
      " 1340436/2000000: episode: 1830, duration: 18.590s, episode steps: 774, steps per second:  42, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.030 [0.000, 5.000],  loss: 0.020889, mae: 3.164610, mean_q: 3.816398, mean_eps: 0.100000\n",
      " 1341105/2000000: episode: 1831, duration: 16.032s, episode steps: 669, steps per second:  42, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.021366, mae: 3.156882, mean_q: 3.806394, mean_eps: 0.100000\n",
      " 1341989/2000000: episode: 1832, duration: 20.673s, episode steps: 884, steps per second:  43, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.933 [0.000, 5.000],  loss: 0.020872, mae: 3.150363, mean_q: 3.796930, mean_eps: 0.100000\n",
      " 1342888/2000000: episode: 1833, duration: 19.768s, episode steps: 899, steps per second:  45, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.083 [0.000, 5.000],  loss: 0.020949, mae: 3.164022, mean_q: 3.814147, mean_eps: 0.100000\n",
      " 1343558/2000000: episode: 1834, duration: 16.684s, episode steps: 670, steps per second:  40, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.912 [0.000, 5.000],  loss: 0.022578, mae: 3.161469, mean_q: 3.808590, mean_eps: 0.100000\n",
      " 1344315/2000000: episode: 1835, duration: 19.187s, episode steps: 757, steps per second:  39, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.476 [0.000, 5.000],  loss: 0.018825, mae: 3.159440, mean_q: 3.812002, mean_eps: 0.100000\n",
      " 1344944/2000000: episode: 1836, duration: 15.682s, episode steps: 629, steps per second:  40, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.094 [0.000, 5.000],  loss: 0.022389, mae: 3.162626, mean_q: 3.813821, mean_eps: 0.100000\n",
      " 1345776/2000000: episode: 1837, duration: 20.910s, episode steps: 832, steps per second:  40, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.375 [0.000, 5.000],  loss: 0.021620, mae: 3.152649, mean_q: 3.801858, mean_eps: 0.100000\n",
      " 1347320/2000000: episode: 1838, duration: 41.397s, episode steps: 1544, steps per second:  37, episode reward: 30.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.021754, mae: 3.176979, mean_q: 3.829062, mean_eps: 0.100000\n",
      " 1347811/2000000: episode: 1839, duration: 12.847s, episode steps: 491, steps per second:  38, episode reward:  2.000, mean reward:  0.004 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.022699, mae: 3.172309, mean_q: 3.823177, mean_eps: 0.100000\n",
      " 1348412/2000000: episode: 1840, duration: 16.175s, episode steps: 601, steps per second:  37, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.629 [0.000, 5.000],  loss: 0.022243, mae: 3.174690, mean_q: 3.828003, mean_eps: 0.100000\n",
      " 1349530/2000000: episode: 1841, duration: 28.619s, episode steps: 1118, steps per second:  39, episode reward: 18.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.022012, mae: 3.162691, mean_q: 3.814584, mean_eps: 0.100000\n",
      " 1350451/2000000: episode: 1842, duration: 22.432s, episode steps: 921, steps per second:  41, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.780 [0.000, 5.000],  loss: 0.021506, mae: 3.170137, mean_q: 3.822730, mean_eps: 0.100000\n",
      " 1350982/2000000: episode: 1843, duration: 12.944s, episode steps: 531, steps per second:  41, episode reward: 12.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.019862, mae: 3.155123, mean_q: 3.805636, mean_eps: 0.100000\n",
      " 1351490/2000000: episode: 1844, duration: 12.211s, episode steps: 508, steps per second:  42, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.945 [0.000, 5.000],  loss: 0.018802, mae: 3.162382, mean_q: 3.813866, mean_eps: 0.100000\n",
      " 1352152/2000000: episode: 1845, duration: 16.082s, episode steps: 662, steps per second:  41, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.950 [0.000, 5.000],  loss: 0.019485, mae: 3.163389, mean_q: 3.814063, mean_eps: 0.100000\n",
      " 1352884/2000000: episode: 1846, duration: 17.752s, episode steps: 732, steps per second:  41, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.568 [0.000, 5.000],  loss: 0.020617, mae: 3.145487, mean_q: 3.790359, mean_eps: 0.100000\n",
      " 1353684/2000000: episode: 1847, duration: 19.324s, episode steps: 800, steps per second:  41, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.777 [0.000, 5.000],  loss: 0.020594, mae: 3.150453, mean_q: 3.795787, mean_eps: 0.100000\n",
      " 1354898/2000000: episode: 1848, duration: 29.651s, episode steps: 1214, steps per second:  41, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.020362, mae: 3.146786, mean_q: 3.791760, mean_eps: 0.100000\n",
      " 1355674/2000000: episode: 1849, duration: 18.873s, episode steps: 776, steps per second:  41, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.017887, mae: 3.173655, mean_q: 3.824383, mean_eps: 0.100000\n",
      " 1356337/2000000: episode: 1850, duration: 15.834s, episode steps: 663, steps per second:  42, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.020928, mae: 3.182816, mean_q: 3.837902, mean_eps: 0.100000\n",
      " 1356714/2000000: episode: 1851, duration: 8.531s, episode steps: 377, steps per second:  44, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.883 [0.000, 5.000],  loss: 0.020585, mae: 3.180852, mean_q: 3.835410, mean_eps: 0.100000\n",
      " 1357726/2000000: episode: 1852, duration: 22.471s, episode steps: 1012, steps per second:  45, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.001 [0.000, 5.000],  loss: 0.020161, mae: 3.168171, mean_q: 3.816958, mean_eps: 0.100000\n",
      " 1358595/2000000: episode: 1853, duration: 19.503s, episode steps: 869, steps per second:  45, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.780 [0.000, 5.000],  loss: 0.021349, mae: 3.173462, mean_q: 3.824117, mean_eps: 0.100000\n",
      " 1359556/2000000: episode: 1854, duration: 21.205s, episode steps: 961, steps per second:  45, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.022057, mae: 3.165522, mean_q: 3.813596, mean_eps: 0.100000\n",
      " 1360427/2000000: episode: 1855, duration: 19.149s, episode steps: 871, steps per second:  45, episode reward: 15.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.020250, mae: 3.169387, mean_q: 3.821572, mean_eps: 0.100000\n",
      " 1361230/2000000: episode: 1856, duration: 18.031s, episode steps: 803, steps per second:  45, episode reward: 24.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.019137, mae: 3.176674, mean_q: 3.831666, mean_eps: 0.100000\n",
      " 1361989/2000000: episode: 1857, duration: 16.743s, episode steps: 759, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.019230, mae: 3.184523, mean_q: 3.838861, mean_eps: 0.100000\n",
      " 1362820/2000000: episode: 1858, duration: 18.627s, episode steps: 831, steps per second:  45, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.416 [0.000, 5.000],  loss: 0.020248, mae: 3.185108, mean_q: 3.840081, mean_eps: 0.100000\n",
      " 1363976/2000000: episode: 1859, duration: 26.055s, episode steps: 1156, steps per second:  44, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.021671, mae: 3.182154, mean_q: 3.834820, mean_eps: 0.100000\n",
      " 1364605/2000000: episode: 1860, duration: 13.720s, episode steps: 629, steps per second:  46, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.018408, mae: 3.191410, mean_q: 3.846953, mean_eps: 0.100000\n",
      " 1365456/2000000: episode: 1861, duration: 18.547s, episode steps: 851, steps per second:  46, episode reward: 27.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.022148, mae: 3.192216, mean_q: 3.847481, mean_eps: 0.100000\n",
      " 1365798/2000000: episode: 1862, duration: 7.291s, episode steps: 342, steps per second:  47, episode reward:  6.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.158 [0.000, 5.000],  loss: 0.021445, mae: 3.203226, mean_q: 3.858044, mean_eps: 0.100000\n",
      " 1366417/2000000: episode: 1863, duration: 13.960s, episode steps: 619, steps per second:  44, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.020953, mae: 3.197072, mean_q: 3.853039, mean_eps: 0.100000\n",
      " 1367240/2000000: episode: 1864, duration: 18.062s, episode steps: 823, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.020419, mae: 3.189624, mean_q: 3.843029, mean_eps: 0.100000\n",
      " 1367616/2000000: episode: 1865, duration: 8.545s, episode steps: 376, steps per second:  44, episode reward:  9.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.721 [0.000, 5.000],  loss: 0.021316, mae: 3.183709, mean_q: 3.841225, mean_eps: 0.100000\n",
      " 1368263/2000000: episode: 1866, duration: 14.294s, episode steps: 647, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.019925, mae: 3.203241, mean_q: 3.863257, mean_eps: 0.100000\n",
      " 1369274/2000000: episode: 1867, duration: 21.920s, episode steps: 1011, steps per second:  46, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.106 [0.000, 5.000],  loss: 0.021296, mae: 3.194687, mean_q: 3.850057, mean_eps: 0.100000\n",
      " 1370164/2000000: episode: 1868, duration: 19.449s, episode steps: 890, steps per second:  46, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.022783, mae: 3.190102, mean_q: 3.844250, mean_eps: 0.100000\n",
      " 1370738/2000000: episode: 1869, duration: 12.937s, episode steps: 574, steps per second:  44, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.179 [0.000, 5.000],  loss: 0.020016, mae: 3.197780, mean_q: 3.855945, mean_eps: 0.100000\n",
      " 1371349/2000000: episode: 1870, duration: 13.588s, episode steps: 611, steps per second:  45, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.890 [0.000, 5.000],  loss: 0.027785, mae: 3.192027, mean_q: 3.846668, mean_eps: 0.100000\n",
      " 1372415/2000000: episode: 1871, duration: 23.930s, episode steps: 1066, steps per second:  45, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.018 [0.000, 5.000],  loss: 0.021118, mae: 3.195677, mean_q: 3.852214, mean_eps: 0.100000\n",
      " 1373325/2000000: episode: 1872, duration: 20.417s, episode steps: 910, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.021391, mae: 3.166935, mean_q: 3.818335, mean_eps: 0.100000\n",
      " 1373840/2000000: episode: 1873, duration: 11.236s, episode steps: 515, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.295 [0.000, 5.000],  loss: 0.020189, mae: 3.193120, mean_q: 3.845653, mean_eps: 0.100000\n",
      " 1374671/2000000: episode: 1874, duration: 18.127s, episode steps: 831, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.905 [0.000, 5.000],  loss: 0.019909, mae: 3.202230, mean_q: 3.859460, mean_eps: 0.100000\n",
      " 1375041/2000000: episode: 1875, duration: 7.983s, episode steps: 370, steps per second:  46, episode reward:  5.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.684 [0.000, 5.000],  loss: 0.022442, mae: 3.200430, mean_q: 3.857698, mean_eps: 0.100000\n",
      " 1375390/2000000: episode: 1876, duration: 7.535s, episode steps: 349, steps per second:  46, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.894 [0.000, 5.000],  loss: 0.021643, mae: 3.210150, mean_q: 3.868584, mean_eps: 0.100000\n",
      " 1377057/2000000: episode: 1877, duration: 36.487s, episode steps: 1667, steps per second:  46, episode reward: 33.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.020190, mae: 3.181254, mean_q: 3.833650, mean_eps: 0.100000\n",
      " 1378280/2000000: episode: 1878, duration: 27.287s, episode steps: 1223, steps per second:  45, episode reward: 20.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.020235, mae: 3.197107, mean_q: 3.850694, mean_eps: 0.100000\n",
      " 1379451/2000000: episode: 1879, duration: 25.957s, episode steps: 1171, steps per second:  45, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.020829, mae: 3.188874, mean_q: 3.843736, mean_eps: 0.100000\n",
      " 1380403/2000000: episode: 1880, duration: 21.103s, episode steps: 952, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.022886, mae: 3.185093, mean_q: 3.837788, mean_eps: 0.100000\n",
      " 1381306/2000000: episode: 1881, duration: 19.808s, episode steps: 903, steps per second:  46, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.329 [0.000, 5.000],  loss: 0.020887, mae: 3.231337, mean_q: 3.893559, mean_eps: 0.100000\n",
      " 1382084/2000000: episode: 1882, duration: 17.248s, episode steps: 778, steps per second:  45, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.021133, mae: 3.222437, mean_q: 3.883120, mean_eps: 0.100000\n",
      " 1383152/2000000: episode: 1883, duration: 23.813s, episode steps: 1068, steps per second:  45, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.303 [0.000, 5.000],  loss: 0.020546, mae: 3.219801, mean_q: 3.881715, mean_eps: 0.100000\n",
      " 1383801/2000000: episode: 1884, duration: 14.414s, episode steps: 649, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.661 [0.000, 5.000],  loss: 0.021744, mae: 3.219351, mean_q: 3.882817, mean_eps: 0.100000\n",
      " 1384772/2000000: episode: 1885, duration: 21.744s, episode steps: 971, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.021108, mae: 3.227269, mean_q: 3.888546, mean_eps: 0.100000\n",
      " 1385686/2000000: episode: 1886, duration: 19.869s, episode steps: 914, steps per second:  46, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.918 [0.000, 5.000],  loss: 0.021159, mae: 3.216108, mean_q: 3.873352, mean_eps: 0.100000\n",
      " 1386542/2000000: episode: 1887, duration: 18.425s, episode steps: 856, steps per second:  46, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.022551, mae: 3.232425, mean_q: 3.896201, mean_eps: 0.100000\n",
      " 1387507/2000000: episode: 1888, duration: 21.289s, episode steps: 965, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.021157, mae: 3.224673, mean_q: 3.887109, mean_eps: 0.100000\n",
      " 1388610/2000000: episode: 1889, duration: 24.936s, episode steps: 1103, steps per second:  44, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.137 [0.000, 5.000],  loss: 0.022299, mae: 3.210816, mean_q: 3.870117, mean_eps: 0.100000\n",
      " 1389490/2000000: episode: 1890, duration: 19.580s, episode steps: 880, steps per second:  45, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.019850, mae: 3.211801, mean_q: 3.870749, mean_eps: 0.100000\n",
      " 1390445/2000000: episode: 1891, duration: 21.017s, episode steps: 955, steps per second:  45, episode reward: 29.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 3.090 [0.000, 5.000],  loss: 0.019290, mae: 3.228397, mean_q: 3.890752, mean_eps: 0.100000\n",
      " 1391292/2000000: episode: 1892, duration: 18.840s, episode steps: 847, steps per second:  45, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.277 [0.000, 5.000],  loss: 0.020538, mae: 3.207391, mean_q: 3.863962, mean_eps: 0.100000\n",
      " 1392236/2000000: episode: 1893, duration: 20.621s, episode steps: 944, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.174 [0.000, 5.000],  loss: 0.020280, mae: 3.229194, mean_q: 3.890185, mean_eps: 0.100000\n",
      " 1393005/2000000: episode: 1894, duration: 16.832s, episode steps: 769, steps per second:  46, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.811 [0.000, 5.000],  loss: 0.022069, mae: 3.259015, mean_q: 3.928537, mean_eps: 0.100000\n",
      " 1393650/2000000: episode: 1895, duration: 14.333s, episode steps: 645, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.219 [0.000, 5.000],  loss: 0.023067, mae: 3.240309, mean_q: 3.905960, mean_eps: 0.100000\n",
      " 1394794/2000000: episode: 1896, duration: 25.142s, episode steps: 1144, steps per second:  46, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.022310, mae: 3.233661, mean_q: 3.896513, mean_eps: 0.100000\n",
      " 1395466/2000000: episode: 1897, duration: 14.796s, episode steps: 672, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.949 [0.000, 5.000],  loss: 0.022633, mae: 3.223718, mean_q: 3.885892, mean_eps: 0.100000\n",
      " 1396167/2000000: episode: 1898, duration: 15.475s, episode steps: 701, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.020765, mae: 3.254732, mean_q: 3.919059, mean_eps: 0.100000\n",
      " 1396677/2000000: episode: 1899, duration: 11.308s, episode steps: 510, steps per second:  45, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.018567, mae: 3.202307, mean_q: 3.858893, mean_eps: 0.100000\n",
      " 1397686/2000000: episode: 1900, duration: 22.099s, episode steps: 1009, steps per second:  46, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.020841, mae: 3.239367, mean_q: 3.904870, mean_eps: 0.100000\n",
      " 1398824/2000000: episode: 1901, duration: 24.666s, episode steps: 1138, steps per second:  46, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.020899, mae: 3.232175, mean_q: 3.894379, mean_eps: 0.100000\n",
      " 1399996/2000000: episode: 1902, duration: 26.138s, episode steps: 1172, steps per second:  45, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.022171, mae: 3.223929, mean_q: 3.883665, mean_eps: 0.100000\n",
      " 1400845/2000000: episode: 1903, duration: 18.767s, episode steps: 849, steps per second:  45, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.801 [0.000, 5.000],  loss: 0.020370, mae: 3.265721, mean_q: 3.937291, mean_eps: 0.100000\n",
      " 1401533/2000000: episode: 1904, duration: 15.051s, episode steps: 688, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.012 [0.000, 5.000],  loss: 0.018265, mae: 3.280922, mean_q: 3.957562, mean_eps: 0.100000\n",
      " 1402506/2000000: episode: 1905, duration: 21.068s, episode steps: 973, steps per second:  46, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.824 [0.000, 5.000],  loss: 0.019296, mae: 3.276410, mean_q: 3.951945, mean_eps: 0.100000\n",
      " 1403055/2000000: episode: 1906, duration: 12.038s, episode steps: 549, steps per second:  46, episode reward: 15.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.022309, mae: 3.287687, mean_q: 3.962230, mean_eps: 0.100000\n",
      " 1404121/2000000: episode: 1907, duration: 23.501s, episode steps: 1066, steps per second:  45, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.014 [0.000, 5.000],  loss: 0.019071, mae: 3.277477, mean_q: 3.950231, mean_eps: 0.100000\n",
      " 1405214/2000000: episode: 1908, duration: 24.092s, episode steps: 1093, steps per second:  45, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.023386, mae: 3.292662, mean_q: 3.968457, mean_eps: 0.100000\n",
      " 1405830/2000000: episode: 1909, duration: 13.584s, episode steps: 616, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.020414, mae: 3.293487, mean_q: 3.969054, mean_eps: 0.100000\n",
      " 1407452/2000000: episode: 1910, duration: 35.435s, episode steps: 1622, steps per second:  46, episode reward: 46.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.589 [0.000, 5.000],  loss: 0.022919, mae: 3.290587, mean_q: 3.963747, mean_eps: 0.100000\n",
      " 1408373/2000000: episode: 1911, duration: 19.810s, episode steps: 921, steps per second:  46, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.511 [0.000, 5.000],  loss: 0.022220, mae: 3.292850, mean_q: 3.967518, mean_eps: 0.100000\n",
      " 1409029/2000000: episode: 1912, duration: 14.377s, episode steps: 656, steps per second:  46, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.825 [0.000, 5.000],  loss: 0.020980, mae: 3.281389, mean_q: 3.951828, mean_eps: 0.100000\n",
      " 1409891/2000000: episode: 1913, duration: 18.471s, episode steps: 862, steps per second:  47, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.022477, mae: 3.299012, mean_q: 3.975279, mean_eps: 0.100000\n",
      " 1410384/2000000: episode: 1914, duration: 11.015s, episode steps: 493, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.280 [0.000, 5.000],  loss: 0.020306, mae: 3.302066, mean_q: 3.978123, mean_eps: 0.100000\n",
      " 1411450/2000000: episode: 1915, duration: 23.385s, episode steps: 1066, steps per second:  46, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.021279, mae: 3.318787, mean_q: 3.999507, mean_eps: 0.100000\n",
      " 1412668/2000000: episode: 1916, duration: 27.124s, episode steps: 1218, steps per second:  45, episode reward: 26.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.436 [0.000, 5.000],  loss: 0.022241, mae: 3.307134, mean_q: 3.985949, mean_eps: 0.100000\n",
      " 1413442/2000000: episode: 1917, duration: 16.966s, episode steps: 774, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.020630, mae: 3.332965, mean_q: 4.018449, mean_eps: 0.100000\n",
      " 1414687/2000000: episode: 1918, duration: 27.144s, episode steps: 1245, steps per second:  46, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.022287, mae: 3.315106, mean_q: 3.994820, mean_eps: 0.100000\n",
      " 1415665/2000000: episode: 1919, duration: 21.355s, episode steps: 978, steps per second:  46, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.021288, mae: 3.301080, mean_q: 3.977654, mean_eps: 0.100000\n",
      " 1416315/2000000: episode: 1920, duration: 14.237s, episode steps: 650, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.988 [0.000, 5.000],  loss: 0.018200, mae: 3.316582, mean_q: 3.995335, mean_eps: 0.100000\n",
      " 1416861/2000000: episode: 1921, duration: 12.014s, episode steps: 546, steps per second:  45, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.025037, mae: 3.324053, mean_q: 4.006909, mean_eps: 0.100000\n",
      " 1417668/2000000: episode: 1922, duration: 17.607s, episode steps: 807, steps per second:  46, episode reward:  8.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.870 [0.000, 5.000],  loss: 0.017992, mae: 3.312889, mean_q: 3.994362, mean_eps: 0.100000\n",
      " 1418339/2000000: episode: 1923, duration: 14.573s, episode steps: 671, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.669 [0.000, 5.000],  loss: 0.020836, mae: 3.310091, mean_q: 3.986756, mean_eps: 0.100000\n",
      " 1419308/2000000: episode: 1924, duration: 20.850s, episode steps: 969, steps per second:  46, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.021615, mae: 3.299528, mean_q: 3.973594, mean_eps: 0.100000\n",
      " 1419934/2000000: episode: 1925, duration: 13.814s, episode steps: 626, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.944 [0.000, 5.000],  loss: 0.021917, mae: 3.331165, mean_q: 4.013643, mean_eps: 0.100000\n",
      " 1421291/2000000: episode: 1926, duration: 29.964s, episode steps: 1357, steps per second:  45, episode reward: 29.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.186 [0.000, 5.000],  loss: 0.022371, mae: 3.343526, mean_q: 4.032732, mean_eps: 0.100000\n",
      " 1422147/2000000: episode: 1927, duration: 18.743s, episode steps: 856, steps per second:  46, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.905 [0.000, 5.000],  loss: 0.020012, mae: 3.339513, mean_q: 4.028791, mean_eps: 0.100000\n",
      " 1422957/2000000: episode: 1928, duration: 18.141s, episode steps: 810, steps per second:  45, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.723 [0.000, 5.000],  loss: 0.020543, mae: 3.366209, mean_q: 4.058754, mean_eps: 0.100000\n",
      " 1423903/2000000: episode: 1929, duration: 20.755s, episode steps: 946, steps per second:  46, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.021508, mae: 3.358190, mean_q: 4.049410, mean_eps: 0.100000\n",
      " 1424479/2000000: episode: 1930, duration: 12.502s, episode steps: 576, steps per second:  46, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.859 [0.000, 5.000],  loss: 0.022062, mae: 3.356917, mean_q: 4.048064, mean_eps: 0.100000\n",
      " 1425171/2000000: episode: 1931, duration: 14.993s, episode steps: 692, steps per second:  46, episode reward: 23.000, mean reward:  0.033 [ 0.000,  1.000], mean action: 2.412 [0.000, 5.000],  loss: 0.021836, mae: 3.354415, mean_q: 4.043591, mean_eps: 0.100000\n",
      " 1425659/2000000: episode: 1932, duration: 10.573s, episode steps: 488, steps per second:  46, episode reward:  6.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.021143, mae: 3.349176, mean_q: 4.035868, mean_eps: 0.100000\n",
      " 1426820/2000000: episode: 1933, duration: 25.457s, episode steps: 1161, steps per second:  46, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.967 [0.000, 5.000],  loss: 0.021150, mae: 3.354138, mean_q: 4.042187, mean_eps: 0.100000\n",
      " 1428336/2000000: episode: 1934, duration: 33.260s, episode steps: 1516, steps per second:  46, episode reward: 32.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.956 [0.000, 5.000],  loss: 0.021719, mae: 3.350750, mean_q: 4.039319, mean_eps: 0.100000\n",
      " 1429003/2000000: episode: 1935, duration: 14.819s, episode steps: 667, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.021623, mae: 3.355901, mean_q: 4.046958, mean_eps: 0.100000\n",
      " 1429559/2000000: episode: 1936, duration: 12.170s, episode steps: 556, steps per second:  46, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.018441, mae: 3.345512, mean_q: 4.032768, mean_eps: 0.100000\n",
      " 1430193/2000000: episode: 1937, duration: 14.034s, episode steps: 634, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.019401, mae: 3.367520, mean_q: 4.059957, mean_eps: 0.100000\n",
      " 1431306/2000000: episode: 1938, duration: 24.103s, episode steps: 1113, steps per second:  46, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.020475, mae: 3.356607, mean_q: 4.045246, mean_eps: 0.100000\n",
      " 1432011/2000000: episode: 1939, duration: 15.434s, episode steps: 705, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.200 [0.000, 5.000],  loss: 0.020694, mae: 3.373197, mean_q: 4.068256, mean_eps: 0.100000\n",
      " 1432699/2000000: episode: 1940, duration: 15.553s, episode steps: 688, steps per second:  44, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.994 [0.000, 5.000],  loss: 0.022019, mae: 3.364031, mean_q: 4.052414, mean_eps: 0.100000\n",
      " 1433068/2000000: episode: 1941, duration: 8.489s, episode steps: 369, steps per second:  43, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.710 [0.000, 5.000],  loss: 0.020504, mae: 3.356367, mean_q: 4.042959, mean_eps: 0.100000\n",
      " 1433780/2000000: episode: 1942, duration: 15.799s, episode steps: 712, steps per second:  45, episode reward: 10.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.020336, mae: 3.363638, mean_q: 4.052199, mean_eps: 0.100000\n",
      " 1434905/2000000: episode: 1943, duration: 24.480s, episode steps: 1125, steps per second:  46, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.701 [0.000, 5.000],  loss: 0.020899, mae: 3.372927, mean_q: 4.064693, mean_eps: 0.100000\n",
      " 1435648/2000000: episode: 1944, duration: 16.165s, episode steps: 743, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.984 [0.000, 5.000],  loss: 0.020135, mae: 3.399583, mean_q: 4.096816, mean_eps: 0.100000\n",
      " 1436576/2000000: episode: 1945, duration: 21.154s, episode steps: 928, steps per second:  44, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.911 [0.000, 5.000],  loss: 0.022094, mae: 3.368613, mean_q: 4.058001, mean_eps: 0.100000\n",
      " 1437565/2000000: episode: 1946, duration: 21.821s, episode steps: 989, steps per second:  45, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.147 [0.000, 5.000],  loss: 0.020380, mae: 3.389155, mean_q: 4.083953, mean_eps: 0.100000\n",
      " 1438286/2000000: episode: 1947, duration: 16.058s, episode steps: 721, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.020730, mae: 3.375422, mean_q: 4.069715, mean_eps: 0.100000\n",
      " 1439308/2000000: episode: 1948, duration: 22.704s, episode steps: 1022, steps per second:  45, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.475 [0.000, 5.000],  loss: 0.023702, mae: 3.371103, mean_q: 4.064378, mean_eps: 0.100000\n",
      " 1439824/2000000: episode: 1949, duration: 11.366s, episode steps: 516, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.527 [0.000, 5.000],  loss: 0.018294, mae: 3.368936, mean_q: 4.063041, mean_eps: 0.100000\n",
      " 1440933/2000000: episode: 1950, duration: 24.183s, episode steps: 1109, steps per second:  46, episode reward: 19.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.021944, mae: 3.387120, mean_q: 4.081126, mean_eps: 0.100000\n",
      " 1441973/2000000: episode: 1951, duration: 22.576s, episode steps: 1040, steps per second:  46, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.021142, mae: 3.367431, mean_q: 4.058031, mean_eps: 0.100000\n",
      " 1442870/2000000: episode: 1952, duration: 19.506s, episode steps: 897, steps per second:  46, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.958 [0.000, 5.000],  loss: 0.021829, mae: 3.404270, mean_q: 4.099934, mean_eps: 0.100000\n",
      " 1443682/2000000: episode: 1953, duration: 18.307s, episode steps: 812, steps per second:  44, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.936 [0.000, 5.000],  loss: 0.021286, mae: 3.377918, mean_q: 4.070163, mean_eps: 0.100000\n",
      " 1444301/2000000: episode: 1954, duration: 13.626s, episode steps: 619, steps per second:  45, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.021585, mae: 3.367958, mean_q: 4.058602, mean_eps: 0.100000\n",
      " 1444955/2000000: episode: 1955, duration: 14.612s, episode steps: 654, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.781 [0.000, 5.000],  loss: 0.019897, mae: 3.362327, mean_q: 4.050248, mean_eps: 0.100000\n",
      " 1445822/2000000: episode: 1956, duration: 19.171s, episode steps: 867, steps per second:  45, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.691 [0.000, 5.000],  loss: 0.022664, mae: 3.388416, mean_q: 4.083442, mean_eps: 0.100000\n",
      " 1446656/2000000: episode: 1957, duration: 18.401s, episode steps: 834, steps per second:  45, episode reward: 10.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.021160, mae: 3.379732, mean_q: 4.074921, mean_eps: 0.100000\n",
      " 1448021/2000000: episode: 1958, duration: 29.696s, episode steps: 1365, steps per second:  46, episode reward: 24.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.424 [0.000, 5.000],  loss: 0.020079, mae: 3.384559, mean_q: 4.077450, mean_eps: 0.100000\n",
      " 1448450/2000000: episode: 1959, duration: 9.459s, episode steps: 429, steps per second:  45, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.811 [0.000, 5.000],  loss: 0.024405, mae: 3.370758, mean_q: 4.058130, mean_eps: 0.100000\n",
      " 1449558/2000000: episode: 1960, duration: 24.461s, episode steps: 1108, steps per second:  45, episode reward: 14.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.755 [0.000, 5.000],  loss: 0.022533, mae: 3.388372, mean_q: 4.082265, mean_eps: 0.100000\n",
      " 1450371/2000000: episode: 1961, duration: 17.661s, episode steps: 813, steps per second:  46, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.135 [0.000, 5.000],  loss: 0.022252, mae: 3.402557, mean_q: 4.101425, mean_eps: 0.100000\n",
      " 1451075/2000000: episode: 1962, duration: 15.480s, episode steps: 704, steps per second:  45, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.023032, mae: 3.419840, mean_q: 4.119088, mean_eps: 0.100000\n",
      " 1451815/2000000: episode: 1963, duration: 16.016s, episode steps: 740, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.021130, mae: 3.409356, mean_q: 4.107748, mean_eps: 0.100000\n",
      " 1452897/2000000: episode: 1964, duration: 23.545s, episode steps: 1082, steps per second:  46, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.123 [0.000, 5.000],  loss: 0.020084, mae: 3.412076, mean_q: 4.110974, mean_eps: 0.100000\n",
      " 1453834/2000000: episode: 1965, duration: 21.188s, episode steps: 937, steps per second:  44, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.494 [0.000, 5.000],  loss: 0.023941, mae: 3.397056, mean_q: 4.093244, mean_eps: 0.100000\n",
      " 1454563/2000000: episode: 1966, duration: 16.304s, episode steps: 729, steps per second:  45, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.646 [0.000, 5.000],  loss: 0.020691, mae: 3.400718, mean_q: 4.098290, mean_eps: 0.100000\n",
      " 1455678/2000000: episode: 1967, duration: 24.781s, episode steps: 1115, steps per second:  45, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.837 [0.000, 5.000],  loss: 0.021188, mae: 3.416876, mean_q: 4.117528, mean_eps: 0.100000\n",
      " 1456466/2000000: episode: 1968, duration: 18.048s, episode steps: 788, steps per second:  44, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.898 [0.000, 5.000],  loss: 0.019516, mae: 3.401657, mean_q: 4.100365, mean_eps: 0.100000\n",
      " 1456958/2000000: episode: 1969, duration: 11.425s, episode steps: 492, steps per second:  43, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.386 [0.000, 5.000],  loss: 0.021745, mae: 3.384321, mean_q: 4.080429, mean_eps: 0.100000\n",
      " 1457315/2000000: episode: 1970, duration: 7.602s, episode steps: 357, steps per second:  47, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.026754, mae: 3.409619, mean_q: 4.106496, mean_eps: 0.100000\n",
      " 1458219/2000000: episode: 1971, duration: 19.793s, episode steps: 904, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.299 [0.000, 5.000],  loss: 0.021840, mae: 3.390375, mean_q: 4.084750, mean_eps: 0.100000\n",
      " 1459415/2000000: episode: 1972, duration: 25.906s, episode steps: 1196, steps per second:  46, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.671 [0.000, 5.000],  loss: 0.019895, mae: 3.405517, mean_q: 4.102355, mean_eps: 0.100000\n",
      " 1460405/2000000: episode: 1973, duration: 22.374s, episode steps: 990, steps per second:  44, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.020523, mae: 3.419722, mean_q: 4.120279, mean_eps: 0.100000\n",
      " 1461052/2000000: episode: 1974, duration: 14.328s, episode steps: 647, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.018776, mae: 3.409691, mean_q: 4.112462, mean_eps: 0.100000\n",
      " 1462189/2000000: episode: 1975, duration: 25.010s, episode steps: 1137, steps per second:  45, episode reward: 20.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.711 [0.000, 5.000],  loss: 0.020785, mae: 3.419770, mean_q: 4.118596, mean_eps: 0.100000\n",
      " 1462902/2000000: episode: 1976, duration: 15.772s, episode steps: 713, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.930 [0.000, 5.000],  loss: 0.020391, mae: 3.408672, mean_q: 4.105528, mean_eps: 0.100000\n",
      " 1463440/2000000: episode: 1977, duration: 11.933s, episode steps: 538, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.701 [0.000, 5.000],  loss: 0.021757, mae: 3.429655, mean_q: 4.135633, mean_eps: 0.100000\n",
      " 1464560/2000000: episode: 1978, duration: 24.269s, episode steps: 1120, steps per second:  46, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.823 [0.000, 5.000],  loss: 0.021592, mae: 3.402994, mean_q: 4.099375, mean_eps: 0.100000\n",
      " 1465505/2000000: episode: 1979, duration: 20.957s, episode steps: 945, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.590 [0.000, 5.000],  loss: 0.020411, mae: 3.439940, mean_q: 4.145215, mean_eps: 0.100000\n",
      " 1466184/2000000: episode: 1980, duration: 15.019s, episode steps: 679, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.019 [0.000, 5.000],  loss: 0.020686, mae: 3.409934, mean_q: 4.107977, mean_eps: 0.100000\n",
      " 1467152/2000000: episode: 1981, duration: 21.245s, episode steps: 968, steps per second:  46, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.018337, mae: 3.416620, mean_q: 4.115765, mean_eps: 0.100000\n",
      " 1468116/2000000: episode: 1982, duration: 20.948s, episode steps: 964, steps per second:  46, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.450 [0.000, 5.000],  loss: 0.021209, mae: 3.425917, mean_q: 4.130984, mean_eps: 0.100000\n",
      " 1469036/2000000: episode: 1983, duration: 20.218s, episode steps: 920, steps per second:  46, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.041 [0.000, 5.000],  loss: 0.021655, mae: 3.399568, mean_q: 4.095491, mean_eps: 0.100000\n",
      " 1469600/2000000: episode: 1984, duration: 12.585s, episode steps: 564, steps per second:  45, episode reward: 10.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.020090, mae: 3.414510, mean_q: 4.113037, mean_eps: 0.100000\n",
      " 1470586/2000000: episode: 1985, duration: 22.089s, episode steps: 986, steps per second:  45, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: 0.017630, mae: 3.417569, mean_q: 4.118043, mean_eps: 0.100000\n",
      " 1471048/2000000: episode: 1986, duration: 10.041s, episode steps: 462, steps per second:  46, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.898 [0.000, 5.000],  loss: 0.018274, mae: 3.405046, mean_q: 4.105512, mean_eps: 0.100000\n",
      " 1471451/2000000: episode: 1987, duration: 9.003s, episode steps: 403, steps per second:  45, episode reward:  9.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.020 [0.000, 5.000],  loss: 0.020863, mae: 3.430960, mean_q: 4.136729, mean_eps: 0.100000\n",
      " 1472363/2000000: episode: 1988, duration: 20.282s, episode steps: 912, steps per second:  45, episode reward: 15.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.018897, mae: 3.393984, mean_q: 4.088847, mean_eps: 0.100000\n",
      " 1473532/2000000: episode: 1989, duration: 25.889s, episode steps: 1169, steps per second:  45, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.022421, mae: 3.401279, mean_q: 4.097665, mean_eps: 0.100000\n",
      " 1474899/2000000: episode: 1990, duration: 29.602s, episode steps: 1367, steps per second:  46, episode reward: 28.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.497 [0.000, 5.000],  loss: 0.021262, mae: 3.420708, mean_q: 4.122103, mean_eps: 0.100000\n",
      " 1475766/2000000: episode: 1991, duration: 18.853s, episode steps: 867, steps per second:  46, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.028 [0.000, 5.000],  loss: 0.021680, mae: 3.423567, mean_q: 4.121929, mean_eps: 0.100000\n",
      " 1476389/2000000: episode: 1992, duration: 13.778s, episode steps: 623, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.547 [0.000, 5.000],  loss: 0.022268, mae: 3.414922, mean_q: 4.116588, mean_eps: 0.100000\n",
      " 1477016/2000000: episode: 1993, duration: 13.654s, episode steps: 627, steps per second:  46, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.020949, mae: 3.418650, mean_q: 4.118678, mean_eps: 0.100000\n",
      " 1477683/2000000: episode: 1994, duration: 14.801s, episode steps: 667, steps per second:  45, episode reward:  7.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.754 [0.000, 5.000],  loss: 0.022982, mae: 3.428336, mean_q: 4.134363, mean_eps: 0.100000\n",
      " 1478711/2000000: episode: 1995, duration: 24.286s, episode steps: 1028, steps per second:  42, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.336 [0.000, 5.000],  loss: 0.020165, mae: 3.428596, mean_q: 4.131397, mean_eps: 0.100000\n",
      " 1479414/2000000: episode: 1996, duration: 15.467s, episode steps: 703, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.812 [0.000, 5.000],  loss: 0.020181, mae: 3.386354, mean_q: 4.080954, mean_eps: 0.100000\n",
      " 1480489/2000000: episode: 1997, duration: 23.272s, episode steps: 1075, steps per second:  46, episode reward: 26.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.047 [0.000, 5.000],  loss: 0.020437, mae: 3.411486, mean_q: 4.111945, mean_eps: 0.100000\n",
      " 1481210/2000000: episode: 1998, duration: 15.768s, episode steps: 721, steps per second:  46, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.019188, mae: 3.388920, mean_q: 4.084664, mean_eps: 0.100000\n",
      " 1481871/2000000: episode: 1999, duration: 14.728s, episode steps: 661, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.115 [0.000, 5.000],  loss: 0.019422, mae: 3.397327, mean_q: 4.094605, mean_eps: 0.100000\n",
      " 1482752/2000000: episode: 2000, duration: 19.286s, episode steps: 881, steps per second:  46, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.149 [0.000, 5.000],  loss: 0.023821, mae: 3.394882, mean_q: 4.092972, mean_eps: 0.100000\n",
      " 1484121/2000000: episode: 2001, duration: 29.895s, episode steps: 1369, steps per second:  46, episode reward: 31.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.022966, mae: 3.392122, mean_q: 4.088339, mean_eps: 0.100000\n",
      " 1484803/2000000: episode: 2002, duration: 15.129s, episode steps: 682, steps per second:  45, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.019888, mae: 3.406199, mean_q: 4.105529, mean_eps: 0.100000\n",
      " 1485700/2000000: episode: 2003, duration: 19.727s, episode steps: 897, steps per second:  45, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.760 [0.000, 5.000],  loss: 0.019760, mae: 3.406050, mean_q: 4.103933, mean_eps: 0.100000\n",
      " 1486152/2000000: episode: 2004, duration: 10.018s, episode steps: 452, steps per second:  45, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.024298, mae: 3.403747, mean_q: 4.101421, mean_eps: 0.100000\n",
      " 1486675/2000000: episode: 2005, duration: 11.608s, episode steps: 523, steps per second:  45, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.983 [0.000, 5.000],  loss: 0.019373, mae: 3.406288, mean_q: 4.107293, mean_eps: 0.100000\n",
      " 1487343/2000000: episode: 2006, duration: 15.253s, episode steps: 668, steps per second:  44, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.907 [0.000, 5.000],  loss: 0.021326, mae: 3.398269, mean_q: 4.093579, mean_eps: 0.100000\n",
      " 1487854/2000000: episode: 2007, duration: 11.704s, episode steps: 511, steps per second:  44, episode reward:  4.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.646 [0.000, 5.000],  loss: 0.020003, mae: 3.409730, mean_q: 4.107299, mean_eps: 0.100000\n",
      " 1488431/2000000: episode: 2008, duration: 12.677s, episode steps: 577, steps per second:  46, episode reward: 11.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.496 [0.000, 5.000],  loss: 0.021602, mae: 3.397691, mean_q: 4.094875, mean_eps: 0.100000\n",
      " 1489233/2000000: episode: 2009, duration: 17.750s, episode steps: 802, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.859 [0.000, 5.000],  loss: 0.020380, mae: 3.400786, mean_q: 4.098690, mean_eps: 0.100000\n",
      " 1489931/2000000: episode: 2010, duration: 15.727s, episode steps: 698, steps per second:  44, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.020191, mae: 3.401631, mean_q: 4.096341, mean_eps: 0.100000\n",
      " 1490758/2000000: episode: 2011, duration: 18.344s, episode steps: 827, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.769 [0.000, 5.000],  loss: 0.021323, mae: 3.394603, mean_q: 4.093351, mean_eps: 0.100000\n",
      " 1491235/2000000: episode: 2012, duration: 10.513s, episode steps: 477, steps per second:  45, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.713 [0.000, 5.000],  loss: 0.019616, mae: 3.374683, mean_q: 4.065294, mean_eps: 0.100000\n",
      " 1491751/2000000: episode: 2013, duration: 11.529s, episode steps: 516, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.021241, mae: 3.389540, mean_q: 4.084432, mean_eps: 0.100000\n",
      " 1492508/2000000: episode: 2014, duration: 17.235s, episode steps: 757, steps per second:  44, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.022859, mae: 3.402962, mean_q: 4.100924, mean_eps: 0.100000\n",
      " 1493445/2000000: episode: 2015, duration: 20.819s, episode steps: 937, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.022870, mae: 3.399773, mean_q: 4.095141, mean_eps: 0.100000\n",
      " 1494186/2000000: episode: 2016, duration: 16.607s, episode steps: 741, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.019639, mae: 3.394040, mean_q: 4.090026, mean_eps: 0.100000\n",
      " 1494992/2000000: episode: 2017, duration: 18.036s, episode steps: 806, steps per second:  45, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.020949, mae: 3.422664, mean_q: 4.124991, mean_eps: 0.100000\n",
      " 1495631/2000000: episode: 2018, duration: 14.353s, episode steps: 639, steps per second:  45, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.426 [0.000, 5.000],  loss: 0.021843, mae: 3.425875, mean_q: 4.128334, mean_eps: 0.100000\n",
      " 1496290/2000000: episode: 2019, duration: 14.604s, episode steps: 659, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.127 [0.000, 5.000],  loss: 0.019810, mae: 3.409562, mean_q: 4.107812, mean_eps: 0.100000\n",
      " 1497054/2000000: episode: 2020, duration: 16.994s, episode steps: 764, steps per second:  45, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.808 [0.000, 5.000],  loss: 0.020452, mae: 3.390568, mean_q: 4.084720, mean_eps: 0.100000\n",
      " 1497713/2000000: episode: 2021, duration: 14.539s, episode steps: 659, steps per second:  45, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.338 [0.000, 5.000],  loss: 0.020282, mae: 3.391639, mean_q: 4.086629, mean_eps: 0.100000\n",
      " 1498356/2000000: episode: 2022, duration: 14.745s, episode steps: 643, steps per second:  44, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.871 [0.000, 5.000],  loss: 0.020654, mae: 3.420965, mean_q: 4.123207, mean_eps: 0.100000\n",
      " 1498924/2000000: episode: 2023, duration: 12.762s, episode steps: 568, steps per second:  45, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.963 [0.000, 5.000],  loss: 0.017814, mae: 3.383627, mean_q: 4.078111, mean_eps: 0.100000\n",
      " 1499565/2000000: episode: 2024, duration: 14.367s, episode steps: 641, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.022140, mae: 3.389048, mean_q: 4.081746, mean_eps: 0.100000\n",
      " 1499983/2000000: episode: 2025, duration: 9.020s, episode steps: 418, steps per second:  46, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.018428, mae: 3.405148, mean_q: 4.104072, mean_eps: 0.100000\n",
      " 1500616/2000000: episode: 2026, duration: 14.217s, episode steps: 633, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.716 [0.000, 5.000],  loss: 0.019852, mae: 3.441807, mean_q: 4.148820, mean_eps: 0.100000\n",
      " 1501795/2000000: episode: 2027, duration: 26.371s, episode steps: 1179, steps per second:  45, episode reward: 21.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.019184, mae: 3.425119, mean_q: 4.126497, mean_eps: 0.100000\n",
      " 1502654/2000000: episode: 2028, duration: 18.883s, episode steps: 859, steps per second:  45, episode reward: 27.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.019227, mae: 3.424147, mean_q: 4.125744, mean_eps: 0.100000\n",
      " 1503594/2000000: episode: 2029, duration: 20.948s, episode steps: 940, steps per second:  45, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.020826, mae: 3.435612, mean_q: 4.138758, mean_eps: 0.100000\n",
      " 1504013/2000000: episode: 2030, duration: 9.288s, episode steps: 419, steps per second:  45, episode reward:  3.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.023167, mae: 3.424599, mean_q: 4.122746, mean_eps: 0.100000\n",
      " 1504477/2000000: episode: 2031, duration: 10.350s, episode steps: 464, steps per second:  45, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.121 [0.000, 5.000],  loss: 0.022672, mae: 3.436848, mean_q: 4.143053, mean_eps: 0.100000\n",
      " 1505112/2000000: episode: 2032, duration: 14.011s, episode steps: 635, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.024394, mae: 3.433844, mean_q: 4.135162, mean_eps: 0.100000\n",
      " 1506280/2000000: episode: 2033, duration: 25.370s, episode steps: 1168, steps per second:  46, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.507 [0.000, 5.000],  loss: 0.023656, mae: 3.425104, mean_q: 4.125374, mean_eps: 0.100000\n",
      " 1506924/2000000: episode: 2034, duration: 14.020s, episode steps: 644, steps per second:  46, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.651 [0.000, 5.000],  loss: 0.017437, mae: 3.434623, mean_q: 4.137684, mean_eps: 0.100000\n",
      " 1507534/2000000: episode: 2035, duration: 13.354s, episode steps: 610, steps per second:  46, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.333 [0.000, 5.000],  loss: 0.020641, mae: 3.418930, mean_q: 4.117894, mean_eps: 0.100000\n",
      " 1508010/2000000: episode: 2036, duration: 10.494s, episode steps: 476, steps per second:  45, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.405 [0.000, 5.000],  loss: 0.019972, mae: 3.413638, mean_q: 4.110749, mean_eps: 0.100000\n",
      " 1508749/2000000: episode: 2037, duration: 16.012s, episode steps: 739, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.021153, mae: 3.442602, mean_q: 4.147236, mean_eps: 0.100000\n",
      " 1509211/2000000: episode: 2038, duration: 10.210s, episode steps: 462, steps per second:  45, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.955 [0.000, 5.000],  loss: 0.019752, mae: 3.431117, mean_q: 4.130687, mean_eps: 0.100000\n",
      " 1510261/2000000: episode: 2039, duration: 23.287s, episode steps: 1050, steps per second:  45, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.145 [0.000, 5.000],  loss: 0.023054, mae: 3.427967, mean_q: 4.128588, mean_eps: 0.100000\n",
      " 1511476/2000000: episode: 2040, duration: 26.775s, episode steps: 1215, steps per second:  45, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.022229, mae: 3.443458, mean_q: 4.150898, mean_eps: 0.100000\n",
      " 1511797/2000000: episode: 2041, duration: 7.237s, episode steps: 321, steps per second:  44, episode reward:  2.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 2.704 [0.000, 5.000],  loss: 0.021013, mae: 3.460511, mean_q: 4.169036, mean_eps: 0.100000\n",
      " 1512588/2000000: episode: 2042, duration: 17.416s, episode steps: 791, steps per second:  45, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.219 [0.000, 5.000],  loss: 0.019594, mae: 3.442923, mean_q: 4.148713, mean_eps: 0.100000\n",
      " 1513156/2000000: episode: 2043, duration: 12.599s, episode steps: 568, steps per second:  45, episode reward:  6.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.020284, mae: 3.462272, mean_q: 4.173234, mean_eps: 0.100000\n",
      " 1514068/2000000: episode: 2044, duration: 19.877s, episode steps: 912, steps per second:  46, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.322 [0.000, 5.000],  loss: 0.021532, mae: 3.439319, mean_q: 4.142738, mean_eps: 0.100000\n",
      " 1514979/2000000: episode: 2045, duration: 20.327s, episode steps: 911, steps per second:  45, episode reward: 27.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.022169, mae: 3.417919, mean_q: 4.116509, mean_eps: 0.100000\n",
      " 1515848/2000000: episode: 2046, duration: 19.351s, episode steps: 869, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.724 [0.000, 5.000],  loss: 0.023603, mae: 3.452190, mean_q: 4.159441, mean_eps: 0.100000\n",
      " 1516472/2000000: episode: 2047, duration: 14.040s, episode steps: 624, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.548 [0.000, 5.000],  loss: 0.022627, mae: 3.405286, mean_q: 4.105276, mean_eps: 0.100000\n",
      " 1517168/2000000: episode: 2048, duration: 15.348s, episode steps: 696, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.022537, mae: 3.415332, mean_q: 4.113633, mean_eps: 0.100000\n",
      " 1518412/2000000: episode: 2049, duration: 27.162s, episode steps: 1244, steps per second:  46, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.506 [0.000, 5.000],  loss: 0.023523, mae: 3.458790, mean_q: 4.166951, mean_eps: 0.100000\n",
      " 1519081/2000000: episode: 2050, duration: 14.826s, episode steps: 669, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.903 [0.000, 5.000],  loss: 0.023473, mae: 3.467181, mean_q: 4.173622, mean_eps: 0.100000\n",
      " 1519759/2000000: episode: 2051, duration: 15.048s, episode steps: 678, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.022079, mae: 3.457467, mean_q: 4.162556, mean_eps: 0.100000\n",
      " 1520288/2000000: episode: 2052, duration: 11.753s, episode steps: 529, steps per second:  45, episode reward:  3.000, mean reward:  0.006 [ 0.000,  1.000], mean action: 1.985 [0.000, 5.000],  loss: 0.022348, mae: 3.479597, mean_q: 4.192600, mean_eps: 0.100000\n",
      " 1521219/2000000: episode: 2053, duration: 20.882s, episode steps: 931, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.361 [0.000, 5.000],  loss: 0.022411, mae: 3.456762, mean_q: 4.164921, mean_eps: 0.100000\n",
      " 1521904/2000000: episode: 2054, duration: 15.023s, episode steps: 685, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.166 [0.000, 5.000],  loss: 0.022168, mae: 3.451233, mean_q: 4.161327, mean_eps: 0.100000\n",
      " 1522733/2000000: episode: 2055, duration: 18.293s, episode steps: 829, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.417 [0.000, 5.000],  loss: 0.019238, mae: 3.465007, mean_q: 4.175377, mean_eps: 0.100000\n",
      " 1523682/2000000: episode: 2056, duration: 20.459s, episode steps: 949, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.082 [0.000, 5.000],  loss: 0.021510, mae: 3.464701, mean_q: 4.173064, mean_eps: 0.100000\n",
      " 1524745/2000000: episode: 2057, duration: 23.187s, episode steps: 1063, steps per second:  46, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.019744, mae: 3.486527, mean_q: 4.199670, mean_eps: 0.100000\n",
      " 1525380/2000000: episode: 2058, duration: 13.980s, episode steps: 635, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.021622, mae: 3.459869, mean_q: 4.169321, mean_eps: 0.100000\n",
      " 1526285/2000000: episode: 2059, duration: 19.918s, episode steps: 905, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.895 [0.000, 5.000],  loss: 0.022639, mae: 3.494069, mean_q: 4.210786, mean_eps: 0.100000\n",
      " 1526914/2000000: episode: 2060, duration: 13.926s, episode steps: 629, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.076 [0.000, 5.000],  loss: 0.021390, mae: 3.477407, mean_q: 4.187207, mean_eps: 0.100000\n",
      " 1527465/2000000: episode: 2061, duration: 12.022s, episode steps: 551, steps per second:  46, episode reward:  9.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.023681, mae: 3.491815, mean_q: 4.206096, mean_eps: 0.100000\n",
      " 1528438/2000000: episode: 2062, duration: 21.494s, episode steps: 973, steps per second:  45, episode reward: 20.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 0.021372, mae: 3.471922, mean_q: 4.181535, mean_eps: 0.100000\n",
      " 1528895/2000000: episode: 2063, duration: 9.870s, episode steps: 457, steps per second:  46, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.853 [0.000, 5.000],  loss: 0.026219, mae: 3.463982, mean_q: 4.175660, mean_eps: 0.100000\n",
      " 1530052/2000000: episode: 2064, duration: 25.357s, episode steps: 1157, steps per second:  46, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.022015, mae: 3.478136, mean_q: 4.191150, mean_eps: 0.100000\n",
      " 1530747/2000000: episode: 2065, duration: 15.264s, episode steps: 695, steps per second:  46, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.377 [0.000, 5.000],  loss: 0.019457, mae: 3.485680, mean_q: 4.200932, mean_eps: 0.100000\n",
      " 1531297/2000000: episode: 2066, duration: 12.379s, episode steps: 550, steps per second:  44, episode reward: 14.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.962 [0.000, 5.000],  loss: 0.020790, mae: 3.465285, mean_q: 4.177816, mean_eps: 0.100000\n",
      " 1531856/2000000: episode: 2067, duration: 12.248s, episode steps: 559, steps per second:  46, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.020736, mae: 3.481327, mean_q: 4.193079, mean_eps: 0.100000\n",
      " 1532441/2000000: episode: 2068, duration: 13.082s, episode steps: 585, steps per second:  45, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.638 [0.000, 5.000],  loss: 0.022450, mae: 3.447934, mean_q: 4.151246, mean_eps: 0.100000\n",
      " 1533193/2000000: episode: 2069, duration: 16.328s, episode steps: 752, steps per second:  46, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.664 [0.000, 5.000],  loss: 0.021165, mae: 3.474766, mean_q: 4.189261, mean_eps: 0.100000\n",
      " 1533768/2000000: episode: 2070, duration: 12.595s, episode steps: 575, steps per second:  46, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.871 [0.000, 5.000],  loss: 0.019671, mae: 3.458409, mean_q: 4.166769, mean_eps: 0.100000\n",
      " 1534480/2000000: episode: 2071, duration: 15.686s, episode steps: 712, steps per second:  45, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.020515, mae: 3.475131, mean_q: 4.188234, mean_eps: 0.100000\n",
      " 1535339/2000000: episode: 2072, duration: 18.777s, episode steps: 859, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.019474, mae: 3.446875, mean_q: 4.153733, mean_eps: 0.100000\n",
      " 1535957/2000000: episode: 2073, duration: 13.596s, episode steps: 618, steps per second:  45, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.672 [0.000, 5.000],  loss: 0.019659, mae: 3.475408, mean_q: 4.191991, mean_eps: 0.100000\n",
      " 1536957/2000000: episode: 2074, duration: 22.128s, episode steps: 1000, steps per second:  45, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.128 [0.000, 5.000],  loss: 0.020967, mae: 3.464454, mean_q: 4.174989, mean_eps: 0.100000\n",
      " 1537608/2000000: episode: 2075, duration: 14.651s, episode steps: 651, steps per second:  44, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.745 [0.000, 5.000],  loss: 0.020695, mae: 3.460558, mean_q: 4.170946, mean_eps: 0.100000\n",
      " 1538592/2000000: episode: 2076, duration: 21.666s, episode steps: 984, steps per second:  45, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.019718, mae: 3.474025, mean_q: 4.186978, mean_eps: 0.100000\n",
      " 1539232/2000000: episode: 2077, duration: 14.146s, episode steps: 640, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.022371, mae: 3.451545, mean_q: 4.155484, mean_eps: 0.100000\n",
      " 1539852/2000000: episode: 2078, duration: 13.468s, episode steps: 620, steps per second:  46, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.944 [0.000, 5.000],  loss: 0.019849, mae: 3.482275, mean_q: 4.200144, mean_eps: 0.100000\n",
      " 1540458/2000000: episode: 2079, duration: 13.494s, episode steps: 606, steps per second:  45, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.019507, mae: 3.489196, mean_q: 4.203967, mean_eps: 0.100000\n",
      " 1541410/2000000: episode: 2080, duration: 20.688s, episode steps: 952, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.556 [0.000, 5.000],  loss: 0.020002, mae: 3.516560, mean_q: 4.240198, mean_eps: 0.100000\n",
      " 1542064/2000000: episode: 2081, duration: 14.424s, episode steps: 654, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.022029, mae: 3.492638, mean_q: 4.209816, mean_eps: 0.100000\n",
      " 1542654/2000000: episode: 2082, duration: 12.913s, episode steps: 590, steps per second:  46, episode reward:  6.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.018077, mae: 3.492002, mean_q: 4.209905, mean_eps: 0.100000\n",
      " 1543331/2000000: episode: 2083, duration: 15.147s, episode steps: 677, steps per second:  45, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.026067, mae: 3.513204, mean_q: 4.232113, mean_eps: 0.100000\n",
      " 1544179/2000000: episode: 2084, duration: 18.653s, episode steps: 848, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.557 [0.000, 5.000],  loss: 0.021076, mae: 3.490213, mean_q: 4.204886, mean_eps: 0.100000\n",
      " 1545092/2000000: episode: 2085, duration: 20.304s, episode steps: 913, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.327 [0.000, 5.000],  loss: 0.024011, mae: 3.502509, mean_q: 4.220704, mean_eps: 0.100000\n",
      " 1545752/2000000: episode: 2086, duration: 14.156s, episode steps: 660, steps per second:  47, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.691 [0.000, 5.000],  loss: 0.017791, mae: 3.496689, mean_q: 4.212057, mean_eps: 0.100000\n",
      " 1546452/2000000: episode: 2087, duration: 15.497s, episode steps: 700, steps per second:  45, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.120 [0.000, 5.000],  loss: 0.018505, mae: 3.490144, mean_q: 4.203067, mean_eps: 0.100000\n",
      " 1547608/2000000: episode: 2088, duration: 25.416s, episode steps: 1156, steps per second:  45, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.467 [0.000, 5.000],  loss: 0.021056, mae: 3.496155, mean_q: 4.214060, mean_eps: 0.100000\n",
      " 1548605/2000000: episode: 2089, duration: 21.915s, episode steps: 997, steps per second:  45, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.559 [0.000, 5.000],  loss: 0.022311, mae: 3.519708, mean_q: 4.239453, mean_eps: 0.100000\n",
      " 1549835/2000000: episode: 2090, duration: 26.945s, episode steps: 1230, steps per second:  46, episode reward: 30.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.483 [0.000, 5.000],  loss: 0.022256, mae: 3.494966, mean_q: 4.210253, mean_eps: 0.100000\n",
      " 1550501/2000000: episode: 2091, duration: 14.447s, episode steps: 666, steps per second:  46, episode reward: 17.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.770 [0.000, 5.000],  loss: 0.021052, mae: 3.503590, mean_q: 4.220125, mean_eps: 0.100000\n",
      " 1551236/2000000: episode: 2092, duration: 16.173s, episode steps: 735, steps per second:  45, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.052 [0.000, 5.000],  loss: 0.022176, mae: 3.509013, mean_q: 4.227559, mean_eps: 0.100000\n",
      " 1552102/2000000: episode: 2093, duration: 19.194s, episode steps: 866, steps per second:  45, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.019491, mae: 3.517240, mean_q: 4.236412, mean_eps: 0.100000\n",
      " 1552598/2000000: episode: 2094, duration: 10.982s, episode steps: 496, steps per second:  45, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.022009, mae: 3.520460, mean_q: 4.240582, mean_eps: 0.100000\n",
      " 1553541/2000000: episode: 2095, duration: 20.790s, episode steps: 943, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.617 [0.000, 5.000],  loss: 0.026245, mae: 3.500762, mean_q: 4.216806, mean_eps: 0.100000\n",
      " 1554351/2000000: episode: 2096, duration: 17.884s, episode steps: 810, steps per second:  45, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.195 [0.000, 5.000],  loss: 0.021181, mae: 3.499313, mean_q: 4.214643, mean_eps: 0.100000\n",
      " 1555359/2000000: episode: 2097, duration: 22.358s, episode steps: 1008, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.019910, mae: 3.490199, mean_q: 4.205404, mean_eps: 0.100000\n",
      " 1555744/2000000: episode: 2098, duration: 8.384s, episode steps: 385, steps per second:  46, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.899 [0.000, 5.000],  loss: 0.018944, mae: 3.504817, mean_q: 4.224745, mean_eps: 0.100000\n",
      " 1556978/2000000: episode: 2099, duration: 27.006s, episode steps: 1234, steps per second:  46, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.241 [0.000, 5.000],  loss: 0.023597, mae: 3.497230, mean_q: 4.215041, mean_eps: 0.100000\n",
      " 1557998/2000000: episode: 2100, duration: 22.001s, episode steps: 1020, steps per second:  46, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.172 [0.000, 5.000],  loss: 0.020375, mae: 3.533804, mean_q: 4.256538, mean_eps: 0.100000\n",
      " 1558880/2000000: episode: 2101, duration: 20.027s, episode steps: 882, steps per second:  44, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.957 [0.000, 5.000],  loss: 0.021199, mae: 3.500246, mean_q: 4.216743, mean_eps: 0.100000\n",
      " 1559384/2000000: episode: 2102, duration: 11.403s, episode steps: 504, steps per second:  44, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.546 [0.000, 5.000],  loss: 0.022792, mae: 3.502495, mean_q: 4.218241, mean_eps: 0.100000\n",
      " 1559874/2000000: episode: 2103, duration: 10.917s, episode steps: 490, steps per second:  45, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.020615, mae: 3.528011, mean_q: 4.250044, mean_eps: 0.100000\n",
      " 1560646/2000000: episode: 2104, duration: 16.916s, episode steps: 772, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.199 [0.000, 5.000],  loss: 0.020008, mae: 3.538736, mean_q: 4.263135, mean_eps: 0.100000\n",
      " 1561236/2000000: episode: 2105, duration: 13.161s, episode steps: 590, steps per second:  45, episode reward: 14.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.653 [0.000, 5.000],  loss: 0.020648, mae: 3.524962, mean_q: 4.248831, mean_eps: 0.100000\n",
      " 1561915/2000000: episode: 2106, duration: 14.979s, episode steps: 679, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.018279, mae: 3.504659, mean_q: 4.223904, mean_eps: 0.100000\n",
      " 1563486/2000000: episode: 2107, duration: 34.374s, episode steps: 1571, steps per second:  46, episode reward: 30.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.987 [0.000, 5.000],  loss: 0.022134, mae: 3.513789, mean_q: 4.235388, mean_eps: 0.100000\n",
      " 1563934/2000000: episode: 2108, duration: 9.949s, episode steps: 448, steps per second:  45, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.806 [0.000, 5.000],  loss: 0.025803, mae: 3.517256, mean_q: 4.243116, mean_eps: 0.100000\n",
      " 1564964/2000000: episode: 2109, duration: 22.496s, episode steps: 1030, steps per second:  46, episode reward: 26.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.380 [0.000, 5.000],  loss: 0.023630, mae: 3.538561, mean_q: 4.263616, mean_eps: 0.100000\n",
      " 1566286/2000000: episode: 2110, duration: 28.995s, episode steps: 1322, steps per second:  46, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.693 [0.000, 5.000],  loss: 0.022363, mae: 3.508197, mean_q: 4.229023, mean_eps: 0.100000\n",
      " 1567461/2000000: episode: 2111, duration: 25.523s, episode steps: 1175, steps per second:  46, episode reward: 29.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.946 [0.000, 5.000],  loss: 0.019976, mae: 3.530970, mean_q: 4.254874, mean_eps: 0.100000\n",
      " 1568110/2000000: episode: 2112, duration: 14.325s, episode steps: 649, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.020690, mae: 3.522775, mean_q: 4.243058, mean_eps: 0.100000\n",
      " 1569168/2000000: episode: 2113, duration: 23.474s, episode steps: 1058, steps per second:  45, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.817 [0.000, 5.000],  loss: 0.023600, mae: 3.535330, mean_q: 4.259542, mean_eps: 0.100000\n",
      " 1569758/2000000: episode: 2114, duration: 12.927s, episode steps: 590, steps per second:  46, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.054 [0.000, 5.000],  loss: 0.020042, mae: 3.525251, mean_q: 4.248686, mean_eps: 0.100000\n",
      " 1570267/2000000: episode: 2115, duration: 11.253s, episode steps: 509, steps per second:  45, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.088 [0.000, 5.000],  loss: 0.018655, mae: 3.533780, mean_q: 4.259669, mean_eps: 0.100000\n",
      " 1571853/2000000: episode: 2116, duration: 34.834s, episode steps: 1586, steps per second:  46, episode reward: 39.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.021083, mae: 3.534991, mean_q: 4.259873, mean_eps: 0.100000\n",
      " 1572615/2000000: episode: 2117, duration: 16.305s, episode steps: 762, steps per second:  47, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.453 [0.000, 5.000],  loss: 0.022774, mae: 3.506267, mean_q: 4.224930, mean_eps: 0.100000\n",
      " 1572997/2000000: episode: 2118, duration: 8.443s, episode steps: 382, steps per second:  45, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.016408, mae: 3.519389, mean_q: 4.244007, mean_eps: 0.100000\n",
      " 1573923/2000000: episode: 2119, duration: 20.015s, episode steps: 926, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.025681, mae: 3.542265, mean_q: 4.266562, mean_eps: 0.100000\n",
      " 1574623/2000000: episode: 2120, duration: 15.332s, episode steps: 700, steps per second:  46, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.901 [0.000, 5.000],  loss: 0.019812, mae: 3.536850, mean_q: 4.261851, mean_eps: 0.100000\n",
      " 1575315/2000000: episode: 2121, duration: 15.416s, episode steps: 692, steps per second:  45, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.058 [0.000, 5.000],  loss: 0.019676, mae: 3.532751, mean_q: 4.260283, mean_eps: 0.100000\n",
      " 1576185/2000000: episode: 2122, duration: 19.501s, episode steps: 870, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.544 [0.000, 5.000],  loss: 0.021547, mae: 3.528455, mean_q: 4.252321, mean_eps: 0.100000\n",
      " 1576726/2000000: episode: 2123, duration: 11.897s, episode steps: 541, steps per second:  45, episode reward:  4.000, mean reward:  0.007 [ 0.000,  1.000], mean action: 1.834 [0.000, 5.000],  loss: 0.021498, mae: 3.538025, mean_q: 4.260433, mean_eps: 0.100000\n",
      " 1577365/2000000: episode: 2124, duration: 14.200s, episode steps: 639, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.495 [0.000, 5.000],  loss: 0.021928, mae: 3.546670, mean_q: 4.273411, mean_eps: 0.100000\n",
      " 1578033/2000000: episode: 2125, duration: 14.543s, episode steps: 668, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.473 [0.000, 5.000],  loss: 0.023201, mae: 3.543308, mean_q: 4.271444, mean_eps: 0.100000\n",
      " 1578775/2000000: episode: 2126, duration: 16.118s, episode steps: 742, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.022657, mae: 3.523622, mean_q: 4.244362, mean_eps: 0.100000\n",
      " 1579526/2000000: episode: 2127, duration: 16.596s, episode steps: 751, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.216 [0.000, 5.000],  loss: 0.021434, mae: 3.520223, mean_q: 4.240610, mean_eps: 0.100000\n",
      " 1580166/2000000: episode: 2128, duration: 14.041s, episode steps: 640, steps per second:  46, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.022075, mae: 3.532130, mean_q: 4.253365, mean_eps: 0.100000\n",
      " 1580907/2000000: episode: 2129, duration: 16.349s, episode steps: 741, steps per second:  45, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.021208, mae: 3.547819, mean_q: 4.276953, mean_eps: 0.100000\n",
      " 1582020/2000000: episode: 2130, duration: 24.496s, episode steps: 1113, steps per second:  45, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.111 [0.000, 5.000],  loss: 0.021087, mae: 3.563642, mean_q: 4.296221, mean_eps: 0.100000\n",
      " 1582945/2000000: episode: 2131, duration: 20.340s, episode steps: 925, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.601 [0.000, 5.000],  loss: 0.021769, mae: 3.557301, mean_q: 4.284633, mean_eps: 0.100000\n",
      " 1583450/2000000: episode: 2132, duration: 10.768s, episode steps: 505, steps per second:  47, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.023359, mae: 3.572897, mean_q: 4.306801, mean_eps: 0.100000\n",
      " 1584163/2000000: episode: 2133, duration: 15.570s, episode steps: 713, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.717 [0.000, 5.000],  loss: 0.022106, mae: 3.553135, mean_q: 4.280808, mean_eps: 0.100000\n",
      " 1585106/2000000: episode: 2134, duration: 20.867s, episode steps: 943, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.513 [0.000, 5.000],  loss: 0.021408, mae: 3.528931, mean_q: 4.253956, mean_eps: 0.100000\n",
      " 1585692/2000000: episode: 2135, duration: 12.959s, episode steps: 586, steps per second:  45, episode reward: 16.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.208 [0.000, 5.000],  loss: 0.023989, mae: 3.566536, mean_q: 4.295954, mean_eps: 0.100000\n",
      " 1586597/2000000: episode: 2136, duration: 19.984s, episode steps: 905, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.818 [0.000, 5.000],  loss: 0.020147, mae: 3.545735, mean_q: 4.270357, mean_eps: 0.100000\n",
      " 1587163/2000000: episode: 2137, duration: 12.167s, episode steps: 566, steps per second:  47, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.024941, mae: 3.544636, mean_q: 4.272342, mean_eps: 0.100000\n",
      " 1587797/2000000: episode: 2138, duration: 14.046s, episode steps: 634, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.021472, mae: 3.552113, mean_q: 4.281438, mean_eps: 0.100000\n",
      " 1588527/2000000: episode: 2139, duration: 16.020s, episode steps: 730, steps per second:  46, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.021435, mae: 3.573329, mean_q: 4.307682, mean_eps: 0.100000\n",
      " 1589101/2000000: episode: 2140, duration: 12.557s, episode steps: 574, steps per second:  46, episode reward: 13.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.843 [0.000, 5.000],  loss: 0.020642, mae: 3.550885, mean_q: 4.277661, mean_eps: 0.100000\n",
      " 1589623/2000000: episode: 2141, duration: 11.269s, episode steps: 522, steps per second:  46, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.389 [0.000, 5.000],  loss: 0.020553, mae: 3.560754, mean_q: 4.288576, mean_eps: 0.100000\n",
      " 1590261/2000000: episode: 2142, duration: 13.772s, episode steps: 638, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.066 [0.000, 5.000],  loss: 0.019892, mae: 3.572946, mean_q: 4.304622, mean_eps: 0.100000\n",
      " 1590896/2000000: episode: 2143, duration: 13.738s, episode steps: 635, steps per second:  46, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.934 [0.000, 5.000],  loss: 0.020627, mae: 3.582741, mean_q: 4.316040, mean_eps: 0.100000\n",
      " 1591457/2000000: episode: 2144, duration: 12.631s, episode steps: 561, steps per second:  44, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.019837, mae: 3.522217, mean_q: 4.245610, mean_eps: 0.100000\n",
      " 1592076/2000000: episode: 2145, duration: 13.614s, episode steps: 619, steps per second:  45, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.911 [0.000, 5.000],  loss: 0.019555, mae: 3.560181, mean_q: 4.292060, mean_eps: 0.100000\n",
      " 1592693/2000000: episode: 2146, duration: 13.788s, episode steps: 617, steps per second:  45, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.022965, mae: 3.559733, mean_q: 4.289698, mean_eps: 0.100000\n",
      " 1593749/2000000: episode: 2147, duration: 23.403s, episode steps: 1056, steps per second:  45, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.020164, mae: 3.552559, mean_q: 4.281397, mean_eps: 0.100000\n",
      " 1594429/2000000: episode: 2148, duration: 15.224s, episode steps: 680, steps per second:  45, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.019760, mae: 3.556760, mean_q: 4.284915, mean_eps: 0.100000\n",
      " 1595363/2000000: episode: 2149, duration: 21.998s, episode steps: 934, steps per second:  42, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.924 [0.000, 5.000],  loss: 0.020403, mae: 3.559288, mean_q: 4.287540, mean_eps: 0.100000\n",
      " 1596410/2000000: episode: 2150, duration: 24.370s, episode steps: 1047, steps per second:  43, episode reward: 22.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.973 [0.000, 5.000],  loss: 0.019411, mae: 3.554885, mean_q: 4.284129, mean_eps: 0.100000\n",
      " 1597308/2000000: episode: 2151, duration: 20.545s, episode steps: 898, steps per second:  44, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.504 [0.000, 5.000],  loss: 0.023863, mae: 3.549314, mean_q: 4.276784, mean_eps: 0.100000\n",
      " 1598206/2000000: episode: 2152, duration: 20.026s, episode steps: 898, steps per second:  45, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.533 [0.000, 5.000],  loss: 0.021646, mae: 3.547967, mean_q: 4.277210, mean_eps: 0.100000\n",
      " 1598828/2000000: episode: 2153, duration: 13.812s, episode steps: 622, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.669 [0.000, 5.000],  loss: 0.025189, mae: 3.559432, mean_q: 4.288432, mean_eps: 0.100000\n",
      " 1599445/2000000: episode: 2154, duration: 13.545s, episode steps: 617, steps per second:  46, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.318 [0.000, 5.000],  loss: 0.020716, mae: 3.536241, mean_q: 4.263625, mean_eps: 0.100000\n",
      " 1601144/2000000: episode: 2155, duration: 37.726s, episode steps: 1699, steps per second:  45, episode reward: 40.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.917 [0.000, 5.000],  loss: 0.021772, mae: 3.557057, mean_q: 4.285741, mean_eps: 0.100000\n",
      " 1602105/2000000: episode: 2156, duration: 21.413s, episode steps: 961, steps per second:  45, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.477 [0.000, 5.000],  loss: 0.020151, mae: 3.567292, mean_q: 4.298831, mean_eps: 0.100000\n",
      " 1602738/2000000: episode: 2157, duration: 14.102s, episode steps: 633, steps per second:  45, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.024102, mae: 3.567961, mean_q: 4.299751, mean_eps: 0.100000\n",
      " 1603476/2000000: episode: 2158, duration: 16.583s, episode steps: 738, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.640 [0.000, 5.000],  loss: 0.025244, mae: 3.564475, mean_q: 4.294855, mean_eps: 0.100000\n",
      " 1604274/2000000: episode: 2159, duration: 17.638s, episode steps: 798, steps per second:  45, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.029 [0.000, 5.000],  loss: 0.019643, mae: 3.559316, mean_q: 4.288031, mean_eps: 0.100000\n",
      " 1605580/2000000: episode: 2160, duration: 28.435s, episode steps: 1306, steps per second:  46, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.438 [0.000, 5.000],  loss: 0.021069, mae: 3.569428, mean_q: 4.299605, mean_eps: 0.100000\n",
      " 1606571/2000000: episode: 2161, duration: 21.533s, episode steps: 991, steps per second:  46, episode reward: 15.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.021751, mae: 3.562891, mean_q: 4.294510, mean_eps: 0.100000\n",
      " 1607072/2000000: episode: 2162, duration: 11.210s, episode steps: 501, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.902 [0.000, 5.000],  loss: 0.019482, mae: 3.581993, mean_q: 4.315455, mean_eps: 0.100000\n",
      " 1607790/2000000: episode: 2163, duration: 16.060s, episode steps: 718, steps per second:  45, episode reward: 14.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.018905, mae: 3.564103, mean_q: 4.293730, mean_eps: 0.100000\n",
      " 1608666/2000000: episode: 2164, duration: 19.546s, episode steps: 876, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.572 [0.000, 5.000],  loss: 0.018998, mae: 3.555716, mean_q: 4.281178, mean_eps: 0.100000\n",
      " 1609498/2000000: episode: 2165, duration: 18.592s, episode steps: 832, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.310 [0.000, 5.000],  loss: 0.019546, mae: 3.557172, mean_q: 4.285413, mean_eps: 0.100000\n",
      " 1610075/2000000: episode: 2166, duration: 12.840s, episode steps: 577, steps per second:  45, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.017940, mae: 3.556435, mean_q: 4.282651, mean_eps: 0.100000\n",
      " 1610785/2000000: episode: 2167, duration: 15.597s, episode steps: 710, steps per second:  46, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.776 [0.000, 5.000],  loss: 0.019528, mae: 3.562754, mean_q: 4.295129, mean_eps: 0.100000\n",
      " 1611655/2000000: episode: 2168, duration: 18.865s, episode steps: 870, steps per second:  46, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.857 [0.000, 5.000],  loss: 0.019430, mae: 3.567739, mean_q: 4.298167, mean_eps: 0.100000\n",
      " 1612565/2000000: episode: 2169, duration: 19.895s, episode steps: 910, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.114 [0.000, 5.000],  loss: 0.019722, mae: 3.567813, mean_q: 4.299491, mean_eps: 0.100000\n",
      " 1613370/2000000: episode: 2170, duration: 17.948s, episode steps: 805, steps per second:  45, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.347 [0.000, 5.000],  loss: 0.021867, mae: 3.544923, mean_q: 4.271271, mean_eps: 0.100000\n",
      " 1614193/2000000: episode: 2171, duration: 18.191s, episode steps: 823, steps per second:  45, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.620 [0.000, 5.000],  loss: 0.021009, mae: 3.555460, mean_q: 4.283144, mean_eps: 0.100000\n",
      " 1615282/2000000: episode: 2172, duration: 23.793s, episode steps: 1089, steps per second:  46, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.043 [0.000, 5.000],  loss: 0.021793, mae: 3.569090, mean_q: 4.299746, mean_eps: 0.100000\n",
      " 1616013/2000000: episode: 2173, duration: 15.765s, episode steps: 731, steps per second:  46, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.216 [0.000, 5.000],  loss: 0.018565, mae: 3.580473, mean_q: 4.313936, mean_eps: 0.100000\n",
      " 1616703/2000000: episode: 2174, duration: 15.132s, episode steps: 690, steps per second:  46, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.007 [0.000, 5.000],  loss: 0.025654, mae: 3.545258, mean_q: 4.270609, mean_eps: 0.100000\n",
      " 1617599/2000000: episode: 2175, duration: 19.923s, episode steps: 896, steps per second:  45, episode reward: 13.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.994 [0.000, 5.000],  loss: 0.022649, mae: 3.577299, mean_q: 4.310331, mean_eps: 0.100000\n",
      " 1618691/2000000: episode: 2176, duration: 24.240s, episode steps: 1092, steps per second:  45, episode reward: 30.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.220 [0.000, 5.000],  loss: 0.020552, mae: 3.571319, mean_q: 4.302699, mean_eps: 0.100000\n",
      " 1619457/2000000: episode: 2177, duration: 16.889s, episode steps: 766, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.195 [0.000, 5.000],  loss: 0.023209, mae: 3.574242, mean_q: 4.304971, mean_eps: 0.100000\n",
      " 1620106/2000000: episode: 2178, duration: 14.287s, episode steps: 649, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.798 [0.000, 5.000],  loss: 0.021363, mae: 3.589573, mean_q: 4.324279, mean_eps: 0.100000\n",
      " 1621200/2000000: episode: 2179, duration: 24.251s, episode steps: 1094, steps per second:  45, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.346 [0.000, 5.000],  loss: 0.023053, mae: 3.584480, mean_q: 4.319146, mean_eps: 0.100000\n",
      " 1622066/2000000: episode: 2180, duration: 18.966s, episode steps: 866, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.341 [0.000, 5.000],  loss: 0.019170, mae: 3.575592, mean_q: 4.305872, mean_eps: 0.100000\n",
      " 1622776/2000000: episode: 2181, duration: 15.577s, episode steps: 710, steps per second:  46, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.714 [0.000, 5.000],  loss: 0.020985, mae: 3.576485, mean_q: 4.308082, mean_eps: 0.100000\n",
      " 1623280/2000000: episode: 2182, duration: 10.872s, episode steps: 504, steps per second:  46, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.019630, mae: 3.589939, mean_q: 4.322294, mean_eps: 0.100000\n",
      " 1623878/2000000: episode: 2183, duration: 13.230s, episode steps: 598, steps per second:  45, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: 0.020473, mae: 3.563871, mean_q: 4.291954, mean_eps: 0.100000\n",
      " 1624546/2000000: episode: 2184, duration: 15.019s, episode steps: 668, steps per second:  44, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.503 [0.000, 5.000],  loss: 0.022452, mae: 3.564871, mean_q: 4.295212, mean_eps: 0.100000\n",
      " 1625101/2000000: episode: 2185, duration: 12.382s, episode steps: 555, steps per second:  45, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.777 [0.000, 5.000],  loss: 0.018306, mae: 3.562998, mean_q: 4.292390, mean_eps: 0.100000\n",
      " 1626037/2000000: episode: 2186, duration: 20.548s, episode steps: 936, steps per second:  46, episode reward: 28.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.353 [0.000, 5.000],  loss: 0.020969, mae: 3.569967, mean_q: 4.300003, mean_eps: 0.100000\n",
      " 1626671/2000000: episode: 2187, duration: 13.886s, episode steps: 634, steps per second:  46, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.465 [0.000, 5.000],  loss: 0.020799, mae: 3.551533, mean_q: 4.281322, mean_eps: 0.100000\n",
      " 1627533/2000000: episode: 2188, duration: 19.013s, episode steps: 862, steps per second:  45, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 1.702 [0.000, 5.000],  loss: 0.023157, mae: 3.581391, mean_q: 4.311942, mean_eps: 0.100000\n",
      " 1628163/2000000: episode: 2189, duration: 13.508s, episode steps: 630, steps per second:  47, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.022253, mae: 3.570714, mean_q: 4.301043, mean_eps: 0.100000\n",
      " 1628895/2000000: episode: 2190, duration: 15.976s, episode steps: 732, steps per second:  46, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.317 [0.000, 5.000],  loss: 0.022979, mae: 3.564588, mean_q: 4.289708, mean_eps: 0.100000\n",
      " 1629556/2000000: episode: 2191, duration: 14.640s, episode steps: 661, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.365 [0.000, 5.000],  loss: 0.020849, mae: 3.596596, mean_q: 4.333117, mean_eps: 0.100000\n",
      " 1630592/2000000: episode: 2192, duration: 22.987s, episode steps: 1036, steps per second:  45, episode reward: 24.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.918 [0.000, 5.000],  loss: 0.021283, mae: 3.570496, mean_q: 4.299711, mean_eps: 0.100000\n",
      " 1631618/2000000: episode: 2193, duration: 22.426s, episode steps: 1026, steps per second:  46, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.020342, mae: 3.573711, mean_q: 4.304366, mean_eps: 0.100000\n",
      " 1632823/2000000: episode: 2194, duration: 26.060s, episode steps: 1205, steps per second:  46, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.133 [0.000, 5.000],  loss: 0.022018, mae: 3.590305, mean_q: 4.326117, mean_eps: 0.100000\n",
      " 1633860/2000000: episode: 2195, duration: 22.983s, episode steps: 1037, steps per second:  45, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.021097, mae: 3.591260, mean_q: 4.327337, mean_eps: 0.100000\n",
      " 1634520/2000000: episode: 2196, duration: 14.493s, episode steps: 660, steps per second:  46, episode reward:  8.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.022398, mae: 3.574722, mean_q: 4.304787, mean_eps: 0.100000\n",
      " 1634975/2000000: episode: 2197, duration: 10.229s, episode steps: 455, steps per second:  44, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.470 [0.000, 5.000],  loss: 0.022058, mae: 3.596841, mean_q: 4.332545, mean_eps: 0.100000\n",
      " 1636034/2000000: episode: 2198, duration: 23.488s, episode steps: 1059, steps per second:  45, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.022038, mae: 3.588008, mean_q: 4.323424, mean_eps: 0.100000\n",
      " 1636566/2000000: episode: 2199, duration: 11.617s, episode steps: 532, steps per second:  46, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.006 [0.000, 5.000],  loss: 0.019957, mae: 3.605968, mean_q: 4.344105, mean_eps: 0.100000\n",
      " 1637640/2000000: episode: 2200, duration: 23.486s, episode steps: 1074, steps per second:  46, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.628 [0.000, 5.000],  loss: 0.020387, mae: 3.586698, mean_q: 4.320993, mean_eps: 0.100000\n",
      " 1638349/2000000: episode: 2201, duration: 15.440s, episode steps: 709, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.272 [0.000, 5.000],  loss: 0.021898, mae: 3.581234, mean_q: 4.315162, mean_eps: 0.100000\n",
      " 1638984/2000000: episode: 2202, duration: 14.068s, episode steps: 635, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.019694, mae: 3.564719, mean_q: 4.291821, mean_eps: 0.100000\n",
      " 1639469/2000000: episode: 2203, duration: 10.729s, episode steps: 485, steps per second:  45, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.080 [0.000, 5.000],  loss: 0.020911, mae: 3.578464, mean_q: 4.311154, mean_eps: 0.100000\n",
      " 1640640/2000000: episode: 2204, duration: 25.822s, episode steps: 1171, steps per second:  45, episode reward: 20.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.035 [0.000, 5.000],  loss: 0.020696, mae: 3.604204, mean_q: 4.342327, mean_eps: 0.100000\n",
      " 1641756/2000000: episode: 2205, duration: 25.067s, episode steps: 1116, steps per second:  45, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: 0.023630, mae: 3.614671, mean_q: 4.356283, mean_eps: 0.100000\n",
      " 1642791/2000000: episode: 2206, duration: 22.880s, episode steps: 1035, steps per second:  45, episode reward: 25.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.324 [0.000, 5.000],  loss: 0.021265, mae: 3.614782, mean_q: 4.354730, mean_eps: 0.100000\n",
      " 1643461/2000000: episode: 2207, duration: 14.769s, episode steps: 670, steps per second:  45, episode reward:  9.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 3.000 [0.000, 5.000],  loss: 0.021585, mae: 3.596806, mean_q: 4.333564, mean_eps: 0.100000\n",
      " 1644236/2000000: episode: 2208, duration: 17.037s, episode steps: 775, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 1.972 [0.000, 5.000],  loss: 0.022016, mae: 3.609451, mean_q: 4.350906, mean_eps: 0.100000\n",
      " 1645210/2000000: episode: 2209, duration: 21.404s, episode steps: 974, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.177 [0.000, 5.000],  loss: 0.018893, mae: 3.608048, mean_q: 4.346499, mean_eps: 0.100000\n",
      " 1646372/2000000: episode: 2210, duration: 25.447s, episode steps: 1162, steps per second:  46, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.022518, mae: 3.639349, mean_q: 4.384129, mean_eps: 0.100000\n",
      " 1647287/2000000: episode: 2211, duration: 20.071s, episode steps: 915, steps per second:  46, episode reward: 22.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.026 [0.000, 5.000],  loss: 0.020759, mae: 3.618243, mean_q: 4.357832, mean_eps: 0.100000\n",
      " 1647907/2000000: episode: 2212, duration: 13.941s, episode steps: 620, steps per second:  44, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.705 [0.000, 5.000],  loss: 0.023907, mae: 3.623689, mean_q: 4.363930, mean_eps: 0.100000\n",
      " 1648848/2000000: episode: 2213, duration: 20.539s, episode steps: 941, steps per second:  46, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.023506, mae: 3.626499, mean_q: 4.367391, mean_eps: 0.100000\n",
      " 1650035/2000000: episode: 2214, duration: 25.981s, episode steps: 1187, steps per second:  46, episode reward: 32.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.686 [0.000, 5.000],  loss: 0.023897, mae: 3.611532, mean_q: 4.348365, mean_eps: 0.100000\n",
      " 1650954/2000000: episode: 2215, duration: 20.354s, episode steps: 919, steps per second:  45, episode reward: 19.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.925 [0.000, 5.000],  loss: 0.022806, mae: 3.610637, mean_q: 4.351524, mean_eps: 0.100000\n",
      " 1651752/2000000: episode: 2216, duration: 17.881s, episode steps: 798, steps per second:  45, episode reward: 12.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.021363, mae: 3.637367, mean_q: 4.380125, mean_eps: 0.100000\n",
      " 1652687/2000000: episode: 2217, duration: 20.492s, episode steps: 935, steps per second:  46, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.616 [0.000, 5.000],  loss: 0.023422, mae: 3.613106, mean_q: 4.352162, mean_eps: 0.100000\n",
      " 1653311/2000000: episode: 2218, duration: 13.720s, episode steps: 624, steps per second:  45, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.774 [0.000, 5.000],  loss: 0.023147, mae: 3.636301, mean_q: 4.381746, mean_eps: 0.100000\n",
      " 1654434/2000000: episode: 2219, duration: 24.338s, episode steps: 1123, steps per second:  46, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.012 [0.000, 5.000],  loss: 0.022058, mae: 3.605015, mean_q: 4.343367, mean_eps: 0.100000\n",
      " 1655349/2000000: episode: 2220, duration: 20.131s, episode steps: 915, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.695 [0.000, 5.000],  loss: 0.021694, mae: 3.617314, mean_q: 4.358674, mean_eps: 0.100000\n",
      " 1655974/2000000: episode: 2221, duration: 13.326s, episode steps: 625, steps per second:  47, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.880 [0.000, 5.000],  loss: 0.023551, mae: 3.619895, mean_q: 4.359072, mean_eps: 0.100000\n",
      " 1657127/2000000: episode: 2222, duration: 25.556s, episode steps: 1153, steps per second:  45, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.957 [0.000, 5.000],  loss: 0.021740, mae: 3.629139, mean_q: 4.371358, mean_eps: 0.100000\n",
      " 1658119/2000000: episode: 2223, duration: 21.714s, episode steps: 992, steps per second:  46, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.935 [0.000, 5.000],  loss: 0.021155, mae: 3.617514, mean_q: 4.360739, mean_eps: 0.100000\n",
      " 1658774/2000000: episode: 2224, duration: 14.514s, episode steps: 655, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.101 [0.000, 5.000],  loss: 0.020345, mae: 3.627891, mean_q: 4.372395, mean_eps: 0.100000\n",
      " 1659515/2000000: episode: 2225, duration: 16.234s, episode steps: 741, steps per second:  46, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.560 [0.000, 5.000],  loss: 0.023513, mae: 3.601550, mean_q: 4.337226, mean_eps: 0.100000\n",
      " 1660467/2000000: episode: 2226, duration: 21.215s, episode steps: 952, steps per second:  45, episode reward: 28.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.791 [0.000, 5.000],  loss: 0.023573, mae: 3.619880, mean_q: 4.363906, mean_eps: 0.100000\n",
      " 1661433/2000000: episode: 2227, duration: 20.992s, episode steps: 966, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.712 [0.000, 5.000],  loss: 0.021527, mae: 3.626825, mean_q: 4.368924, mean_eps: 0.100000\n",
      " 1662078/2000000: episode: 2228, duration: 14.062s, episode steps: 645, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.997 [0.000, 5.000],  loss: 0.022555, mae: 3.626971, mean_q: 4.371320, mean_eps: 0.100000\n",
      " 1662973/2000000: episode: 2229, duration: 19.879s, episode steps: 895, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.020335, mae: 3.618851, mean_q: 4.359150, mean_eps: 0.100000\n",
      " 1663601/2000000: episode: 2230, duration: 13.772s, episode steps: 628, steps per second:  46, episode reward: 15.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.199 [0.000, 5.000],  loss: 0.020314, mae: 3.611869, mean_q: 4.353744, mean_eps: 0.100000\n",
      " 1664761/2000000: episode: 2231, duration: 25.402s, episode steps: 1160, steps per second:  46, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.023251, mae: 3.607676, mean_q: 4.345903, mean_eps: 0.100000\n",
      " 1666038/2000000: episode: 2232, duration: 27.707s, episode steps: 1277, steps per second:  46, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.269 [0.000, 5.000],  loss: 0.020713, mae: 3.605561, mean_q: 4.342315, mean_eps: 0.100000\n",
      " 1667108/2000000: episode: 2233, duration: 23.759s, episode steps: 1070, steps per second:  45, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.022797, mae: 3.606570, mean_q: 4.344901, mean_eps: 0.100000\n",
      " 1667934/2000000: episode: 2234, duration: 18.183s, episode steps: 826, steps per second:  45, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.596 [0.000, 5.000],  loss: 0.020852, mae: 3.616899, mean_q: 4.356627, mean_eps: 0.100000\n",
      " 1668593/2000000: episode: 2235, duration: 14.653s, episode steps: 659, steps per second:  45, episode reward: 21.000, mean reward:  0.032 [ 0.000,  1.000], mean action: 2.307 [0.000, 5.000],  loss: 0.022788, mae: 3.599055, mean_q: 4.333051, mean_eps: 0.100000\n",
      " 1669085/2000000: episode: 2236, duration: 10.789s, episode steps: 492, steps per second:  46, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.797 [0.000, 5.000],  loss: 0.021873, mae: 3.634709, mean_q: 4.377194, mean_eps: 0.100000\n",
      " 1669842/2000000: episode: 2237, duration: 16.752s, episode steps: 757, steps per second:  45, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.019999, mae: 3.624735, mean_q: 4.365855, mean_eps: 0.100000\n",
      " 1670800/2000000: episode: 2238, duration: 20.935s, episode steps: 958, steps per second:  46, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.818 [0.000, 5.000],  loss: 0.021713, mae: 3.644757, mean_q: 4.392136, mean_eps: 0.100000\n",
      " 1671483/2000000: episode: 2239, duration: 15.045s, episode steps: 683, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.022640, mae: 3.645077, mean_q: 4.392928, mean_eps: 0.100000\n",
      " 1672596/2000000: episode: 2240, duration: 24.203s, episode steps: 1113, steps per second:  46, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.500 [0.000, 5.000],  loss: 0.020441, mae: 3.619125, mean_q: 4.359806, mean_eps: 0.100000\n",
      " 1673454/2000000: episode: 2241, duration: 18.661s, episode steps: 858, steps per second:  46, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.134 [0.000, 5.000],  loss: 0.020278, mae: 3.637455, mean_q: 4.381974, mean_eps: 0.100000\n",
      " 1674155/2000000: episode: 2242, duration: 16.482s, episode steps: 701, steps per second:  43, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.025321, mae: 3.626481, mean_q: 4.366821, mean_eps: 0.100000\n",
      " 1674769/2000000: episode: 2243, duration: 14.374s, episode steps: 614, steps per second:  43, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.555 [0.000, 5.000],  loss: 0.022827, mae: 3.664464, mean_q: 4.415713, mean_eps: 0.100000\n",
      " 1675298/2000000: episode: 2244, duration: 11.841s, episode steps: 529, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.229 [0.000, 5.000],  loss: 0.023050, mae: 3.644412, mean_q: 4.392262, mean_eps: 0.100000\n",
      " 1675711/2000000: episode: 2245, duration: 9.145s, episode steps: 413, steps per second:  45, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.339 [0.000, 5.000],  loss: 0.025314, mae: 3.631322, mean_q: 4.372320, mean_eps: 0.100000\n",
      " 1676139/2000000: episode: 2246, duration: 9.419s, episode steps: 428, steps per second:  45, episode reward:  9.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.136 [0.000, 5.000],  loss: 0.020571, mae: 3.618170, mean_q: 4.360374, mean_eps: 0.100000\n",
      " 1677057/2000000: episode: 2247, duration: 22.218s, episode steps: 918, steps per second:  41, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.775 [0.000, 5.000],  loss: 0.022356, mae: 3.635876, mean_q: 4.381512, mean_eps: 0.100000\n",
      " 1677871/2000000: episode: 2248, duration: 19.792s, episode steps: 814, steps per second:  41, episode reward: 17.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.905 [0.000, 5.000],  loss: 0.022177, mae: 3.615966, mean_q: 4.356171, mean_eps: 0.100000\n",
      " 1679099/2000000: episode: 2249, duration: 29.950s, episode steps: 1228, steps per second:  41, episode reward: 27.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.138 [0.000, 5.000],  loss: 0.021439, mae: 3.630310, mean_q: 4.374472, mean_eps: 0.100000\n",
      " 1679660/2000000: episode: 2250, duration: 13.757s, episode steps: 561, steps per second:  41, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 2.234 [0.000, 5.000],  loss: 0.018583, mae: 3.630273, mean_q: 4.378788, mean_eps: 0.100000\n",
      " 1681414/2000000: episode: 2251, duration: 42.322s, episode steps: 1754, steps per second:  41, episode reward: 41.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.246 [0.000, 5.000],  loss: 0.021191, mae: 3.661520, mean_q: 4.412120, mean_eps: 0.100000\n",
      " 1682126/2000000: episode: 2252, duration: 17.149s, episode steps: 712, steps per second:  42, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.858 [0.000, 5.000],  loss: 0.022058, mae: 3.648653, mean_q: 4.397933, mean_eps: 0.100000\n",
      " 1682819/2000000: episode: 2253, duration: 16.837s, episode steps: 693, steps per second:  41, episode reward: 12.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.024981, mae: 3.661762, mean_q: 4.411524, mean_eps: 0.100000\n",
      " 1683703/2000000: episode: 2254, duration: 21.808s, episode steps: 884, steps per second:  41, episode reward: 21.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.025291, mae: 3.660057, mean_q: 4.407826, mean_eps: 0.100000\n",
      " 1684270/2000000: episode: 2255, duration: 13.760s, episode steps: 567, steps per second:  41, episode reward:  7.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.376 [0.000, 5.000],  loss: 0.023475, mae: 3.647539, mean_q: 4.393850, mean_eps: 0.100000\n",
      " 1685332/2000000: episode: 2256, duration: 23.407s, episode steps: 1062, steps per second:  45, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.863 [0.000, 5.000],  loss: 0.023510, mae: 3.659027, mean_q: 4.407119, mean_eps: 0.100000\n",
      " 1686264/2000000: episode: 2257, duration: 20.901s, episode steps: 932, steps per second:  45, episode reward: 26.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.465 [0.000, 5.000],  loss: 0.023539, mae: 3.660947, mean_q: 4.408060, mean_eps: 0.100000\n",
      " 1686907/2000000: episode: 2258, duration: 14.301s, episode steps: 643, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.020991, mae: 3.645969, mean_q: 4.390429, mean_eps: 0.100000\n",
      " 1687551/2000000: episode: 2259, duration: 14.025s, episode steps: 644, steps per second:  46, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.090 [0.000, 5.000],  loss: 0.020413, mae: 3.663017, mean_q: 4.412847, mean_eps: 0.100000\n",
      " 1688918/2000000: episode: 2260, duration: 30.268s, episode steps: 1367, steps per second:  45, episode reward: 32.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.022197, mae: 3.657746, mean_q: 4.406638, mean_eps: 0.100000\n",
      " 1689459/2000000: episode: 2261, duration: 12.010s, episode steps: 541, steps per second:  45, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.022092, mae: 3.632329, mean_q: 4.372763, mean_eps: 0.100000\n",
      " 1690417/2000000: episode: 2262, duration: 21.180s, episode steps: 958, steps per second:  45, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.191 [0.000, 5.000],  loss: 0.019769, mae: 3.654177, mean_q: 4.402355, mean_eps: 0.100000\n",
      " 1691440/2000000: episode: 2263, duration: 22.442s, episode steps: 1023, steps per second:  46, episode reward: 21.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.239 [0.000, 5.000],  loss: 0.022060, mae: 3.665814, mean_q: 4.416088, mean_eps: 0.100000\n",
      " 1692223/2000000: episode: 2264, duration: 17.383s, episode steps: 783, steps per second:  45, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.020885, mae: 3.631607, mean_q: 4.377655, mean_eps: 0.100000\n",
      " 1693215/2000000: episode: 2265, duration: 21.587s, episode steps: 992, steps per second:  46, episode reward: 28.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.538 [0.000, 5.000],  loss: 0.018601, mae: 3.652447, mean_q: 4.398547, mean_eps: 0.100000\n",
      " 1694145/2000000: episode: 2266, duration: 20.157s, episode steps: 930, steps per second:  46, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.020044, mae: 3.664834, mean_q: 4.413936, mean_eps: 0.100000\n",
      " 1694664/2000000: episode: 2267, duration: 11.789s, episode steps: 519, steps per second:  44, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.289 [0.000, 5.000],  loss: 0.022183, mae: 3.661610, mean_q: 4.410579, mean_eps: 0.100000\n",
      " 1695709/2000000: episode: 2268, duration: 23.131s, episode steps: 1045, steps per second:  45, episode reward: 18.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.826 [0.000, 5.000],  loss: 0.022753, mae: 3.657033, mean_q: 4.406195, mean_eps: 0.100000\n",
      " 1696460/2000000: episode: 2269, duration: 16.503s, episode steps: 751, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.391 [0.000, 5.000],  loss: 0.021452, mae: 3.683580, mean_q: 4.437371, mean_eps: 0.100000\n",
      " 1697589/2000000: episode: 2270, duration: 24.866s, episode steps: 1129, steps per second:  45, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.020686, mae: 3.659860, mean_q: 4.409417, mean_eps: 0.100000\n",
      " 1698399/2000000: episode: 2271, duration: 17.899s, episode steps: 810, steps per second:  45, episode reward: 16.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.285 [0.000, 5.000],  loss: 0.022573, mae: 3.643083, mean_q: 4.388609, mean_eps: 0.100000\n",
      " 1699315/2000000: episode: 2272, duration: 20.125s, episode steps: 916, steps per second:  46, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.915 [0.000, 5.000],  loss: 0.022930, mae: 3.663671, mean_q: 4.413819, mean_eps: 0.100000\n",
      " 1700282/2000000: episode: 2273, duration: 21.709s, episode steps: 967, steps per second:  45, episode reward: 30.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.722 [0.000, 5.000],  loss: 0.022688, mae: 3.661162, mean_q: 4.411909, mean_eps: 0.100000\n",
      " 1701273/2000000: episode: 2274, duration: 21.847s, episode steps: 991, steps per second:  45, episode reward: 27.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.522 [0.000, 5.000],  loss: 0.021866, mae: 3.680706, mean_q: 4.432908, mean_eps: 0.100000\n",
      " 1702150/2000000: episode: 2275, duration: 19.445s, episode steps: 877, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.783 [0.000, 5.000],  loss: 0.021270, mae: 3.658488, mean_q: 4.407130, mean_eps: 0.100000\n",
      " 1702875/2000000: episode: 2276, duration: 16.032s, episode steps: 725, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.989 [0.000, 5.000],  loss: 0.019730, mae: 3.672953, mean_q: 4.426097, mean_eps: 0.100000\n",
      " 1703590/2000000: episode: 2277, duration: 15.757s, episode steps: 715, steps per second:  45, episode reward: 13.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.024060, mae: 3.681823, mean_q: 4.437536, mean_eps: 0.100000\n",
      " 1704430/2000000: episode: 2278, duration: 18.464s, episode steps: 840, steps per second:  45, episode reward: 21.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.712 [0.000, 5.000],  loss: 0.022085, mae: 3.668826, mean_q: 4.420453, mean_eps: 0.100000\n",
      " 1704852/2000000: episode: 2279, duration: 9.300s, episode steps: 422, steps per second:  45, episode reward:  6.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.974 [0.000, 5.000],  loss: 0.022662, mae: 3.679601, mean_q: 4.433715, mean_eps: 0.100000\n",
      " 1705411/2000000: episode: 2280, duration: 12.461s, episode steps: 559, steps per second:  45, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.982 [0.000, 5.000],  loss: 0.022613, mae: 3.669016, mean_q: 4.420285, mean_eps: 0.100000\n",
      " 1706018/2000000: episode: 2281, duration: 13.429s, episode steps: 607, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.023774, mae: 3.680388, mean_q: 4.438154, mean_eps: 0.100000\n",
      " 1706484/2000000: episode: 2282, duration: 10.314s, episode steps: 466, steps per second:  45, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.079 [0.000, 5.000],  loss: 0.026132, mae: 3.664534, mean_q: 4.414423, mean_eps: 0.100000\n",
      " 1707710/2000000: episode: 2283, duration: 27.225s, episode steps: 1226, steps per second:  45, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.489 [0.000, 5.000],  loss: 0.020150, mae: 3.675992, mean_q: 4.427724, mean_eps: 0.100000\n",
      " 1708695/2000000: episode: 2284, duration: 21.670s, episode steps: 985, steps per second:  45, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.221 [0.000, 5.000],  loss: 0.026367, mae: 3.670111, mean_q: 4.424316, mean_eps: 0.100000\n",
      " 1709316/2000000: episode: 2285, duration: 13.761s, episode steps: 621, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.224 [0.000, 5.000],  loss: 0.024605, mae: 3.661674, mean_q: 4.408619, mean_eps: 0.100000\n",
      " 1710552/2000000: episode: 2286, duration: 27.442s, episode steps: 1236, steps per second:  45, episode reward: 28.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.020188, mae: 3.680226, mean_q: 4.434583, mean_eps: 0.100000\n",
      " 1710946/2000000: episode: 2287, duration: 8.673s, episode steps: 394, steps per second:  45, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 3.160 [0.000, 5.000],  loss: 0.019524, mae: 3.666760, mean_q: 4.419460, mean_eps: 0.100000\n",
      " 1711372/2000000: episode: 2288, duration: 9.399s, episode steps: 426, steps per second:  45, episode reward: 10.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.683 [0.000, 5.000],  loss: 0.023816, mae: 3.669645, mean_q: 4.421300, mean_eps: 0.100000\n",
      " 1711988/2000000: episode: 2289, duration: 13.575s, episode steps: 616, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.573 [0.000, 5.000],  loss: 0.026282, mae: 3.675928, mean_q: 4.427432, mean_eps: 0.100000\n",
      " 1712525/2000000: episode: 2290, duration: 11.945s, episode steps: 537, steps per second:  45, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 1.786 [0.000, 5.000],  loss: 0.020458, mae: 3.690864, mean_q: 4.444844, mean_eps: 0.100000\n",
      " 1712899/2000000: episode: 2291, duration: 8.175s, episode steps: 374, steps per second:  46, episode reward:  3.000, mean reward:  0.008 [ 0.000,  1.000], mean action: 2.457 [0.000, 5.000],  loss: 0.021220, mae: 3.702437, mean_q: 4.461684, mean_eps: 0.100000\n",
      " 1713418/2000000: episode: 2292, duration: 11.279s, episode steps: 519, steps per second:  46, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.022772, mae: 3.666311, mean_q: 4.416066, mean_eps: 0.100000\n",
      " 1714522/2000000: episode: 2293, duration: 24.093s, episode steps: 1104, steps per second:  46, episode reward: 27.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.021499, mae: 3.682249, mean_q: 4.436643, mean_eps: 0.100000\n",
      " 1715168/2000000: episode: 2294, duration: 14.229s, episode steps: 646, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.094 [0.000, 5.000],  loss: 0.021917, mae: 3.684470, mean_q: 4.435866, mean_eps: 0.100000\n",
      " 1715860/2000000: episode: 2295, duration: 15.385s, episode steps: 692, steps per second:  45, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.350 [0.000, 5.000],  loss: 0.020578, mae: 3.682035, mean_q: 4.439419, mean_eps: 0.100000\n",
      " 1716500/2000000: episode: 2296, duration: 14.566s, episode steps: 640, steps per second:  44, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.492 [0.000, 5.000],  loss: 0.021164, mae: 3.671053, mean_q: 4.422627, mean_eps: 0.100000\n",
      " 1717040/2000000: episode: 2297, duration: 11.964s, episode steps: 540, steps per second:  45, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.526 [0.000, 5.000],  loss: 0.024733, mae: 3.696212, mean_q: 4.452429, mean_eps: 0.100000\n",
      " 1717754/2000000: episode: 2298, duration: 15.734s, episode steps: 714, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.875 [0.000, 5.000],  loss: 0.022014, mae: 3.681022, mean_q: 4.433111, mean_eps: 0.100000\n",
      " 1718775/2000000: episode: 2299, duration: 22.435s, episode steps: 1021, steps per second:  46, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.741 [0.000, 5.000],  loss: 0.025044, mae: 3.674982, mean_q: 4.426122, mean_eps: 0.100000\n",
      " 1719617/2000000: episode: 2300, duration: 18.452s, episode steps: 842, steps per second:  46, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.622 [0.000, 5.000],  loss: 0.021773, mae: 3.681824, mean_q: 4.437004, mean_eps: 0.100000\n",
      " 1720425/2000000: episode: 2301, duration: 17.702s, episode steps: 808, steps per second:  46, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.251 [0.000, 5.000],  loss: 0.021553, mae: 3.677077, mean_q: 4.429164, mean_eps: 0.100000\n",
      " 1720786/2000000: episode: 2302, duration: 8.073s, episode steps: 361, steps per second:  45, episode reward:  8.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.812 [0.000, 5.000],  loss: 0.021502, mae: 3.687458, mean_q: 4.439473, mean_eps: 0.100000\n",
      " 1721403/2000000: episode: 2303, duration: 13.673s, episode steps: 617, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.517 [0.000, 5.000],  loss: 0.024247, mae: 3.703782, mean_q: 4.462040, mean_eps: 0.100000\n",
      " 1722688/2000000: episode: 2304, duration: 29.158s, episode steps: 1285, steps per second:  44, episode reward: 27.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: 0.022971, mae: 3.668671, mean_q: 4.418031, mean_eps: 0.100000\n",
      " 1723627/2000000: episode: 2305, duration: 20.813s, episode steps: 939, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.571 [0.000, 5.000],  loss: 0.025113, mae: 3.691342, mean_q: 4.445230, mean_eps: 0.100000\n",
      " 1725230/2000000: episode: 2306, duration: 35.447s, episode steps: 1603, steps per second:  45, episode reward: 29.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.284 [0.000, 5.000],  loss: 0.022705, mae: 3.676597, mean_q: 4.428752, mean_eps: 0.100000\n",
      " 1726050/2000000: episode: 2307, duration: 18.052s, episode steps: 820, steps per second:  45, episode reward: 20.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.656 [0.000, 5.000],  loss: 0.022765, mae: 3.695400, mean_q: 4.451391, mean_eps: 0.100000\n",
      " 1726746/2000000: episode: 2308, duration: 15.188s, episode steps: 696, steps per second:  46, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.864 [0.000, 5.000],  loss: 0.021907, mae: 3.696760, mean_q: 4.454168, mean_eps: 0.100000\n",
      " 1727650/2000000: episode: 2309, duration: 20.454s, episode steps: 904, steps per second:  44, episode reward: 14.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.058 [0.000, 5.000],  loss: 0.021763, mae: 3.712149, mean_q: 4.471534, mean_eps: 0.100000\n",
      " 1728255/2000000: episode: 2310, duration: 13.297s, episode steps: 605, steps per second:  45, episode reward:  9.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.942 [0.000, 5.000],  loss: 0.025839, mae: 3.687293, mean_q: 4.438647, mean_eps: 0.100000\n",
      " 1729148/2000000: episode: 2311, duration: 20.078s, episode steps: 893, steps per second:  44, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.302 [0.000, 5.000],  loss: 0.023597, mae: 3.707589, mean_q: 4.464126, mean_eps: 0.100000\n",
      " 1730003/2000000: episode: 2312, duration: 18.849s, episode steps: 855, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.078 [0.000, 5.000],  loss: 0.020391, mae: 3.709877, mean_q: 4.470129, mean_eps: 0.100000\n",
      " 1730614/2000000: episode: 2313, duration: 13.448s, episode steps: 611, steps per second:  45, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 3.185 [0.000, 5.000],  loss: 0.021539, mae: 3.693556, mean_q: 4.452457, mean_eps: 0.100000\n",
      " 1731021/2000000: episode: 2314, duration: 8.935s, episode steps: 407, steps per second:  46, episode reward:  5.000, mean reward:  0.012 [ 0.000,  1.000], mean action: 2.150 [0.000, 5.000],  loss: 0.024569, mae: 3.690613, mean_q: 4.444992, mean_eps: 0.100000\n",
      " 1731699/2000000: episode: 2315, duration: 14.969s, episode steps: 678, steps per second:  45, episode reward: 21.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.021532, mae: 3.687075, mean_q: 4.440500, mean_eps: 0.100000\n",
      " 1732593/2000000: episode: 2316, duration: 19.615s, episode steps: 894, steps per second:  46, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.021351, mae: 3.700237, mean_q: 4.456597, mean_eps: 0.100000\n",
      " 1733276/2000000: episode: 2317, duration: 15.289s, episode steps: 683, steps per second:  45, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.022291, mae: 3.718627, mean_q: 4.481475, mean_eps: 0.100000\n",
      " 1733918/2000000: episode: 2318, duration: 14.422s, episode steps: 642, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.818 [0.000, 5.000],  loss: 0.021930, mae: 3.710264, mean_q: 4.469428, mean_eps: 0.100000\n",
      " 1734714/2000000: episode: 2319, duration: 17.351s, episode steps: 796, steps per second:  46, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.442 [0.000, 5.000],  loss: 0.021125, mae: 3.719284, mean_q: 4.481558, mean_eps: 0.100000\n",
      " 1735555/2000000: episode: 2320, duration: 18.194s, episode steps: 841, steps per second:  46, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.260 [0.000, 5.000],  loss: 0.025460, mae: 3.705446, mean_q: 4.461087, mean_eps: 0.100000\n",
      " 1737023/2000000: episode: 2321, duration: 31.852s, episode steps: 1468, steps per second:  46, episode reward: 33.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.870 [0.000, 5.000],  loss: 0.020504, mae: 3.721613, mean_q: 4.483687, mean_eps: 0.100000\n",
      " 1737993/2000000: episode: 2322, duration: 20.947s, episode steps: 970, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.429 [0.000, 5.000],  loss: 0.024585, mae: 3.695132, mean_q: 4.449448, mean_eps: 0.100000\n",
      " 1739192/2000000: episode: 2323, duration: 26.375s, episode steps: 1199, steps per second:  45, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.023110, mae: 3.711713, mean_q: 4.471964, mean_eps: 0.100000\n",
      " 1739885/2000000: episode: 2324, duration: 15.395s, episode steps: 693, steps per second:  45, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.657 [0.000, 5.000],  loss: 0.022869, mae: 3.696032, mean_q: 4.449651, mean_eps: 0.100000\n",
      " 1740683/2000000: episode: 2325, duration: 17.507s, episode steps: 798, steps per second:  46, episode reward: 21.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.744 [0.000, 5.000],  loss: 0.021990, mae: 3.677801, mean_q: 4.425602, mean_eps: 0.100000\n",
      " 1741287/2000000: episode: 2326, duration: 13.420s, episode steps: 604, steps per second:  45, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.396 [0.000, 5.000],  loss: 0.020614, mae: 3.702712, mean_q: 4.461958, mean_eps: 0.100000\n",
      " 1742472/2000000: episode: 2327, duration: 26.064s, episode steps: 1185, steps per second:  45, episode reward: 22.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.446 [0.000, 5.000],  loss: 0.024617, mae: 3.709080, mean_q: 4.464229, mean_eps: 0.100000\n",
      " 1743323/2000000: episode: 2328, duration: 18.589s, episode steps: 851, steps per second:  46, episode reward: 17.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.360 [0.000, 5.000],  loss: 0.022118, mae: 3.696003, mean_q: 4.451128, mean_eps: 0.100000\n",
      " 1744232/2000000: episode: 2329, duration: 20.258s, episode steps: 909, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.176 [0.000, 5.000],  loss: 0.024215, mae: 3.712432, mean_q: 4.471263, mean_eps: 0.100000\n",
      " 1745450/2000000: episode: 2330, duration: 27.067s, episode steps: 1218, steps per second:  45, episode reward: 30.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.337 [0.000, 5.000],  loss: 0.025791, mae: 3.703567, mean_q: 4.459648, mean_eps: 0.100000\n",
      " 1746531/2000000: episode: 2331, duration: 23.730s, episode steps: 1081, steps per second:  46, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.388 [0.000, 5.000],  loss: 0.021497, mae: 3.686198, mean_q: 4.438734, mean_eps: 0.100000\n",
      " 1747362/2000000: episode: 2332, duration: 18.128s, episode steps: 831, steps per second:  46, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.357 [0.000, 5.000],  loss: 0.021008, mae: 3.694450, mean_q: 4.450709, mean_eps: 0.100000\n",
      " 1747850/2000000: episode: 2333, duration: 10.694s, episode steps: 488, steps per second:  46, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.836 [0.000, 5.000],  loss: 0.018885, mae: 3.667536, mean_q: 4.420876, mean_eps: 0.100000\n",
      " 1749005/2000000: episode: 2334, duration: 25.591s, episode steps: 1155, steps per second:  45, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.753 [0.000, 5.000],  loss: 0.023104, mae: 3.709835, mean_q: 4.468627, mean_eps: 0.100000\n",
      " 1749734/2000000: episode: 2335, duration: 16.284s, episode steps: 729, steps per second:  45, episode reward: 11.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.856 [0.000, 5.000],  loss: 0.023111, mae: 3.708175, mean_q: 4.465453, mean_eps: 0.100000\n",
      " 1750696/2000000: episode: 2336, duration: 21.489s, episode steps: 962, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.493 [0.000, 5.000],  loss: 0.020896, mae: 3.694953, mean_q: 4.450627, mean_eps: 0.100000\n",
      " 1751065/2000000: episode: 2337, duration: 8.294s, episode steps: 369, steps per second:  44, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.019523, mae: 3.717029, mean_q: 4.479534, mean_eps: 0.100000\n",
      " 1751554/2000000: episode: 2338, duration: 10.744s, episode steps: 489, steps per second:  46, episode reward:  8.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.018496, mae: 3.718078, mean_q: 4.478088, mean_eps: 0.100000\n",
      " 1752368/2000000: episode: 2339, duration: 18.212s, episode steps: 814, steps per second:  45, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.244 [0.000, 5.000],  loss: 0.020416, mae: 3.698174, mean_q: 4.456088, mean_eps: 0.100000\n",
      " 1753470/2000000: episode: 2340, duration: 24.000s, episode steps: 1102, steps per second:  46, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.140 [0.000, 5.000],  loss: 0.020511, mae: 3.688045, mean_q: 4.443128, mean_eps: 0.100000\n",
      " 1754326/2000000: episode: 2341, duration: 18.534s, episode steps: 856, steps per second:  46, episode reward: 24.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.936 [0.000, 5.000],  loss: 0.021018, mae: 3.706526, mean_q: 4.464237, mean_eps: 0.100000\n",
      " 1755425/2000000: episode: 2342, duration: 24.828s, episode steps: 1099, steps per second:  44, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: 0.022834, mae: 3.724033, mean_q: 4.484679, mean_eps: 0.100000\n",
      " 1756189/2000000: episode: 2343, duration: 17.111s, episode steps: 764, steps per second:  45, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.018956, mae: 3.696976, mean_q: 4.451652, mean_eps: 0.100000\n",
      " 1757319/2000000: episode: 2344, duration: 24.855s, episode steps: 1130, steps per second:  45, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.889 [0.000, 5.000],  loss: 0.022696, mae: 3.694781, mean_q: 4.450312, mean_eps: 0.100000\n",
      " 1758315/2000000: episode: 2345, duration: 22.091s, episode steps: 996, steps per second:  45, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.482 [0.000, 5.000],  loss: 0.021888, mae: 3.716161, mean_q: 4.474385, mean_eps: 0.100000\n",
      " 1759316/2000000: episode: 2346, duration: 21.995s, episode steps: 1001, steps per second:  46, episode reward: 18.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.903 [0.000, 5.000],  loss: 0.025067, mae: 3.715674, mean_q: 4.474862, mean_eps: 0.100000\n",
      " 1759726/2000000: episode: 2347, duration: 8.992s, episode steps: 410, steps per second:  46, episode reward:  4.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 1.712 [0.000, 5.000],  loss: 0.021349, mae: 3.714547, mean_q: 4.476390, mean_eps: 0.100000\n",
      " 1760415/2000000: episode: 2348, duration: 15.538s, episode steps: 689, steps per second:  44, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.433 [0.000, 5.000],  loss: 0.026452, mae: 3.692130, mean_q: 4.444975, mean_eps: 0.100000\n",
      " 1760967/2000000: episode: 2349, duration: 12.396s, episode steps: 552, steps per second:  45, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.261 [0.000, 5.000],  loss: 0.022703, mae: 3.691811, mean_q: 4.445603, mean_eps: 0.100000\n",
      " 1761766/2000000: episode: 2350, duration: 17.792s, episode steps: 799, steps per second:  45, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.309 [0.000, 5.000],  loss: 0.021637, mae: 3.698287, mean_q: 4.456018, mean_eps: 0.100000\n",
      " 1762395/2000000: episode: 2351, duration: 13.816s, episode steps: 629, steps per second:  46, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.022123, mae: 3.699429, mean_q: 4.453985, mean_eps: 0.100000\n",
      " 1763198/2000000: episode: 2352, duration: 17.802s, episode steps: 803, steps per second:  45, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.022596, mae: 3.698864, mean_q: 4.453768, mean_eps: 0.100000\n",
      " 1764104/2000000: episode: 2353, duration: 20.092s, episode steps: 906, steps per second:  45, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.562 [0.000, 5.000],  loss: 0.023102, mae: 3.663509, mean_q: 4.414303, mean_eps: 0.100000\n",
      " 1765330/2000000: episode: 2354, duration: 26.981s, episode steps: 1226, steps per second:  45, episode reward: 25.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.377 [0.000, 5.000],  loss: 0.021413, mae: 3.721346, mean_q: 4.481607, mean_eps: 0.100000\n",
      " 1766212/2000000: episode: 2355, duration: 19.743s, episode steps: 882, steps per second:  45, episode reward: 25.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.643 [0.000, 5.000],  loss: 0.023896, mae: 3.700862, mean_q: 4.456393, mean_eps: 0.100000\n",
      " 1767042/2000000: episode: 2356, duration: 18.515s, episode steps: 830, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.252 [0.000, 5.000],  loss: 0.024182, mae: 3.705131, mean_q: 4.461081, mean_eps: 0.100000\n",
      " 1767978/2000000: episode: 2357, duration: 20.528s, episode steps: 936, steps per second:  46, episode reward: 19.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.262 [0.000, 5.000],  loss: 0.021942, mae: 3.699223, mean_q: 4.452453, mean_eps: 0.100000\n",
      " 1769056/2000000: episode: 2358, duration: 23.506s, episode steps: 1078, steps per second:  46, episode reward: 12.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.212 [0.000, 5.000],  loss: 0.023279, mae: 3.698014, mean_q: 4.453771, mean_eps: 0.100000\n",
      " 1769771/2000000: episode: 2359, duration: 15.991s, episode steps: 715, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.201 [0.000, 5.000],  loss: 0.021772, mae: 3.715001, mean_q: 4.473184, mean_eps: 0.100000\n",
      " 1770544/2000000: episode: 2360, duration: 16.961s, episode steps: 773, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.278 [0.000, 5.000],  loss: 0.024409, mae: 3.701168, mean_q: 4.457307, mean_eps: 0.100000\n",
      " 1771296/2000000: episode: 2361, duration: 16.855s, episode steps: 752, steps per second:  45, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.359 [0.000, 5.000],  loss: 0.024148, mae: 3.720670, mean_q: 4.479539, mean_eps: 0.100000\n",
      " 1772182/2000000: episode: 2362, duration: 19.768s, episode steps: 886, steps per second:  45, episode reward: 22.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.608 [0.000, 5.000],  loss: 0.022941, mae: 3.694850, mean_q: 4.448786, mean_eps: 0.100000\n",
      " 1772728/2000000: episode: 2363, duration: 12.395s, episode steps: 546, steps per second:  44, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.564 [0.000, 5.000],  loss: 0.022569, mae: 3.707592, mean_q: 4.464523, mean_eps: 0.100000\n",
      " 1773405/2000000: episode: 2364, duration: 14.995s, episode steps: 677, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.022420, mae: 3.703381, mean_q: 4.459442, mean_eps: 0.100000\n",
      " 1774521/2000000: episode: 2365, duration: 24.183s, episode steps: 1116, steps per second:  46, episode reward: 24.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.866 [0.000, 5.000],  loss: 0.022977, mae: 3.715657, mean_q: 4.476323, mean_eps: 0.100000\n",
      " 1775888/2000000: episode: 2366, duration: 29.922s, episode steps: 1367, steps per second:  46, episode reward: 30.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.884 [0.000, 5.000],  loss: 0.021999, mae: 3.712425, mean_q: 4.472346, mean_eps: 0.100000\n",
      " 1777018/2000000: episode: 2367, duration: 25.299s, episode steps: 1130, steps per second:  45, episode reward: 11.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.203 [0.000, 5.000],  loss: 0.021152, mae: 3.700674, mean_q: 4.458546, mean_eps: 0.100000\n",
      " 1778117/2000000: episode: 2368, duration: 24.130s, episode steps: 1099, steps per second:  46, episode reward: 29.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.019177, mae: 3.709675, mean_q: 4.469871, mean_eps: 0.100000\n",
      " 1779369/2000000: episode: 2369, duration: 27.978s, episode steps: 1252, steps per second:  45, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.089 [0.000, 5.000],  loss: 0.021556, mae: 3.710384, mean_q: 4.468966, mean_eps: 0.100000\n",
      " 1780273/2000000: episode: 2370, duration: 19.965s, episode steps: 904, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.198 [0.000, 5.000],  loss: 0.019787, mae: 3.711150, mean_q: 4.470777, mean_eps: 0.100000\n",
      " 1781214/2000000: episode: 2371, duration: 20.818s, episode steps: 941, steps per second:  45, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.962 [0.000, 5.000],  loss: 0.022141, mae: 3.720546, mean_q: 4.484928, mean_eps: 0.100000\n",
      " 1782145/2000000: episode: 2372, duration: 20.886s, episode steps: 931, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.634 [0.000, 5.000],  loss: 0.022055, mae: 3.714407, mean_q: 4.474311, mean_eps: 0.100000\n",
      " 1782940/2000000: episode: 2373, duration: 17.612s, episode steps: 795, steps per second:  45, episode reward: 10.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.025729, mae: 3.711198, mean_q: 4.467676, mean_eps: 0.100000\n",
      " 1783926/2000000: episode: 2374, duration: 21.870s, episode steps: 986, steps per second:  45, episode reward: 22.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.411 [0.000, 5.000],  loss: 0.020598, mae: 3.729753, mean_q: 4.493856, mean_eps: 0.100000\n",
      " 1784445/2000000: episode: 2375, duration: 11.667s, episode steps: 519, steps per second:  44, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 1.909 [0.000, 5.000],  loss: 0.025457, mae: 3.704585, mean_q: 4.462601, mean_eps: 0.100000\n",
      " 1785292/2000000: episode: 2376, duration: 18.495s, episode steps: 847, steps per second:  46, episode reward: 18.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.192 [0.000, 5.000],  loss: 0.023966, mae: 3.709949, mean_q: 4.467247, mean_eps: 0.100000\n",
      " 1786110/2000000: episode: 2377, duration: 17.881s, episode steps: 818, steps per second:  46, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 3.199 [0.000, 5.000],  loss: 0.020265, mae: 3.700108, mean_q: 4.458608, mean_eps: 0.100000\n",
      " 1786500/2000000: episode: 2378, duration: 8.733s, episode steps: 390, steps per second:  45, episode reward:  2.000, mean reward:  0.005 [ 0.000,  1.000], mean action: 1.987 [0.000, 5.000],  loss: 0.020388, mae: 3.701658, mean_q: 4.458992, mean_eps: 0.100000\n",
      " 1787640/2000000: episode: 2379, duration: 25.869s, episode steps: 1140, steps per second:  44, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.844 [0.000, 5.000],  loss: 0.022790, mae: 3.712173, mean_q: 4.471426, mean_eps: 0.100000\n",
      " 1788364/2000000: episode: 2380, duration: 16.217s, episode steps: 724, steps per second:  45, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.660 [0.000, 5.000],  loss: 0.022430, mae: 3.711897, mean_q: 4.470859, mean_eps: 0.100000\n",
      " 1789099/2000000: episode: 2381, duration: 16.279s, episode steps: 735, steps per second:  45, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 3.046 [0.000, 5.000],  loss: 0.023645, mae: 3.716919, mean_q: 4.478083, mean_eps: 0.100000\n",
      " 1789974/2000000: episode: 2382, duration: 19.532s, episode steps: 875, steps per second:  45, episode reward: 19.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.904 [0.000, 5.000],  loss: 0.020601, mae: 3.721716, mean_q: 4.483073, mean_eps: 0.100000\n",
      " 1790586/2000000: episode: 2383, duration: 13.486s, episode steps: 612, steps per second:  45, episode reward: 10.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.296 [0.000, 5.000],  loss: 0.022091, mae: 3.733280, mean_q: 4.496179, mean_eps: 0.100000\n",
      " 1791366/2000000: episode: 2384, duration: 17.115s, episode steps: 780, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.697 [0.000, 5.000],  loss: 0.022240, mae: 3.737660, mean_q: 4.501303, mean_eps: 0.100000\n",
      " 1792185/2000000: episode: 2385, duration: 17.762s, episode steps: 819, steps per second:  46, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 1.746 [0.000, 5.000],  loss: 0.021906, mae: 3.727986, mean_q: 4.491634, mean_eps: 0.100000\n",
      " 1792811/2000000: episode: 2386, duration: 13.717s, episode steps: 626, steps per second:  46, episode reward: 11.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.912 [0.000, 5.000],  loss: 0.022569, mae: 3.739116, mean_q: 4.505093, mean_eps: 0.100000\n",
      " 1793517/2000000: episode: 2387, duration: 15.898s, episode steps: 706, steps per second:  44, episode reward: 16.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.654 [0.000, 5.000],  loss: 0.020437, mae: 3.744819, mean_q: 4.510228, mean_eps: 0.100000\n",
      " 1794551/2000000: episode: 2388, duration: 22.537s, episode steps: 1034, steps per second:  46, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.896 [0.000, 5.000],  loss: 0.023460, mae: 3.734148, mean_q: 4.497732, mean_eps: 0.100000\n",
      " 1795974/2000000: episode: 2389, duration: 31.098s, episode steps: 1423, steps per second:  46, episode reward: 36.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.043 [0.000, 5.000],  loss: 0.022219, mae: 3.716486, mean_q: 4.475520, mean_eps: 0.100000\n",
      " 1796734/2000000: episode: 2390, duration: 16.839s, episode steps: 760, steps per second:  45, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.583 [0.000, 5.000],  loss: 0.025143, mae: 3.748438, mean_q: 4.515004, mean_eps: 0.100000\n",
      " 1797594/2000000: episode: 2391, duration: 18.980s, episode steps: 860, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.708 [0.000, 5.000],  loss: 0.021482, mae: 3.734759, mean_q: 4.496156, mean_eps: 0.100000\n",
      " 1798220/2000000: episode: 2392, duration: 13.895s, episode steps: 626, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.796 [0.000, 5.000],  loss: 0.019555, mae: 3.734801, mean_q: 4.496944, mean_eps: 0.100000\n",
      " 1799118/2000000: episode: 2393, duration: 20.199s, episode steps: 898, steps per second:  44, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 3.057 [0.000, 5.000],  loss: 0.021968, mae: 3.728981, mean_q: 4.490590, mean_eps: 0.100000\n",
      " 1799517/2000000: episode: 2394, duration: 8.827s, episode steps: 399, steps per second:  45, episode reward:  5.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.794 [0.000, 5.000],  loss: 0.021162, mae: 3.725973, mean_q: 4.487551, mean_eps: 0.100000\n",
      " 1800404/2000000: episode: 2395, duration: 19.594s, episode steps: 887, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.579 [0.000, 5.000],  loss: 0.021151, mae: 3.733239, mean_q: 4.499745, mean_eps: 0.100000\n",
      " 1801559/2000000: episode: 2396, duration: 25.521s, episode steps: 1155, steps per second:  45, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.274 [0.000, 5.000],  loss: 0.021517, mae: 3.745511, mean_q: 4.513376, mean_eps: 0.100000\n",
      " 1802479/2000000: episode: 2397, duration: 20.170s, episode steps: 920, steps per second:  46, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.524 [0.000, 5.000],  loss: 0.021842, mae: 3.733355, mean_q: 4.495863, mean_eps: 0.100000\n",
      " 1803399/2000000: episode: 2398, duration: 20.087s, episode steps: 920, steps per second:  46, episode reward: 17.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.238 [0.000, 5.000],  loss: 0.021987, mae: 3.759329, mean_q: 4.527074, mean_eps: 0.100000\n",
      " 1804018/2000000: episode: 2399, duration: 13.946s, episode steps: 619, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.020921, mae: 3.750094, mean_q: 4.515720, mean_eps: 0.100000\n",
      " 1805012/2000000: episode: 2400, duration: 21.981s, episode steps: 994, steps per second:  45, episode reward: 23.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.022044, mae: 3.730718, mean_q: 4.491532, mean_eps: 0.100000\n",
      " 1805633/2000000: episode: 2401, duration: 13.968s, episode steps: 621, steps per second:  44, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.227 [0.000, 5.000],  loss: 0.019820, mae: 3.766127, mean_q: 4.537694, mean_eps: 0.100000\n",
      " 1806257/2000000: episode: 2402, duration: 14.529s, episode steps: 624, steps per second:  43, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.929 [0.000, 5.000],  loss: 0.020326, mae: 3.727620, mean_q: 4.489266, mean_eps: 0.100000\n",
      " 1806701/2000000: episode: 2403, duration: 10.656s, episode steps: 444, steps per second:  42, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.025565, mae: 3.719212, mean_q: 4.476050, mean_eps: 0.100000\n",
      " 1807329/2000000: episode: 2404, duration: 13.937s, episode steps: 628, steps per second:  45, episode reward:  8.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.020855, mae: 3.743751, mean_q: 4.506991, mean_eps: 0.100000\n",
      " 1808034/2000000: episode: 2405, duration: 15.566s, episode steps: 705, steps per second:  45, episode reward:  8.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 1.813 [0.000, 5.000],  loss: 0.023427, mae: 3.761533, mean_q: 4.526606, mean_eps: 0.100000\n",
      " 1808463/2000000: episode: 2406, duration: 9.506s, episode steps: 429, steps per second:  45, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.800 [0.000, 5.000],  loss: 0.019393, mae: 3.732514, mean_q: 4.493340, mean_eps: 0.100000\n",
      " 1809577/2000000: episode: 2407, duration: 24.865s, episode steps: 1114, steps per second:  45, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.807 [0.000, 5.000],  loss: 0.022946, mae: 3.764341, mean_q: 4.533229, mean_eps: 0.100000\n",
      " 1810731/2000000: episode: 2408, duration: 25.359s, episode steps: 1154, steps per second:  46, episode reward: 32.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.535 [0.000, 5.000],  loss: 0.020566, mae: 3.721581, mean_q: 4.482582, mean_eps: 0.100000\n",
      " 1811881/2000000: episode: 2409, duration: 25.540s, episode steps: 1150, steps per second:  45, episode reward: 34.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.340 [0.000, 5.000],  loss: 0.023531, mae: 3.758077, mean_q: 4.525136, mean_eps: 0.100000\n",
      " 1812769/2000000: episode: 2410, duration: 19.432s, episode steps: 888, steps per second:  46, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.539 [0.000, 5.000],  loss: 0.021265, mae: 3.767316, mean_q: 4.536068, mean_eps: 0.100000\n",
      " 1813533/2000000: episode: 2411, duration: 16.676s, episode steps: 764, steps per second:  46, episode reward: 19.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.304 [0.000, 5.000],  loss: 0.018692, mae: 3.754284, mean_q: 4.521622, mean_eps: 0.100000\n",
      " 1814073/2000000: episode: 2412, duration: 12.198s, episode steps: 540, steps per second:  44, episode reward:  8.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.213 [0.000, 5.000],  loss: 0.020399, mae: 3.747537, mean_q: 4.515454, mean_eps: 0.100000\n",
      " 1814428/2000000: episode: 2413, duration: 7.966s, episode steps: 355, steps per second:  45, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.017298, mae: 3.762507, mean_q: 4.531392, mean_eps: 0.100000\n",
      " 1815138/2000000: episode: 2414, duration: 16.128s, episode steps: 710, steps per second:  44, episode reward: 20.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.256 [0.000, 5.000],  loss: 0.022143, mae: 3.757038, mean_q: 4.528436, mean_eps: 0.100000\n",
      " 1815816/2000000: episode: 2415, duration: 15.155s, episode steps: 678, steps per second:  45, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.218 [0.000, 5.000],  loss: 0.020842, mae: 3.745225, mean_q: 4.510134, mean_eps: 0.100000\n",
      " 1817045/2000000: episode: 2416, duration: 27.405s, episode steps: 1229, steps per second:  45, episode reward: 29.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.022248, mae: 3.751195, mean_q: 4.518619, mean_eps: 0.100000\n",
      " 1817759/2000000: episode: 2417, duration: 15.626s, episode steps: 714, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.748 [0.000, 5.000],  loss: 0.020645, mae: 3.745208, mean_q: 4.511171, mean_eps: 0.100000\n",
      " 1818619/2000000: episode: 2418, duration: 19.000s, episode steps: 860, steps per second:  45, episode reward: 16.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.080 [0.000, 5.000],  loss: 0.020082, mae: 3.757181, mean_q: 4.522562, mean_eps: 0.100000\n",
      " 1819536/2000000: episode: 2419, duration: 20.460s, episode steps: 917, steps per second:  45, episode reward: 13.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.750 [0.000, 5.000],  loss: 0.023266, mae: 3.776678, mean_q: 4.548342, mean_eps: 0.100000\n",
      " 1820114/2000000: episode: 2420, duration: 13.287s, episode steps: 578, steps per second:  44, episode reward:  8.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 2.972 [0.000, 5.000],  loss: 0.018806, mae: 3.786467, mean_q: 4.561998, mean_eps: 0.100000\n",
      " 1821116/2000000: episode: 2421, duration: 22.733s, episode steps: 1002, steps per second:  44, episode reward: 24.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.622 [0.000, 5.000],  loss: 0.020780, mae: 3.763691, mean_q: 4.533349, mean_eps: 0.100000\n",
      " 1822120/2000000: episode: 2422, duration: 22.449s, episode steps: 1004, steps per second:  45, episode reward: 25.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.102 [0.000, 5.000],  loss: 0.022517, mae: 3.773221, mean_q: 4.543063, mean_eps: 0.100000\n",
      " 1822605/2000000: episode: 2423, duration: 10.888s, episode steps: 485, steps per second:  45, episode reward: 10.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.027448, mae: 3.777660, mean_q: 4.547891, mean_eps: 0.100000\n",
      " 1822998/2000000: episode: 2424, duration: 9.166s, episode steps: 393, steps per second:  43, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.967 [0.000, 5.000],  loss: 0.022364, mae: 3.778211, mean_q: 4.550008, mean_eps: 0.100000\n",
      " 1824099/2000000: episode: 2425, duration: 24.679s, episode steps: 1101, steps per second:  45, episode reward: 23.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.523 [0.000, 5.000],  loss: 0.022279, mae: 3.780215, mean_q: 4.554424, mean_eps: 0.100000\n",
      " 1824951/2000000: episode: 2426, duration: 19.199s, episode steps: 852, steps per second:  44, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.060 [0.000, 5.000],  loss: 0.020678, mae: 3.771576, mean_q: 4.542167, mean_eps: 0.100000\n",
      " 1825693/2000000: episode: 2427, duration: 16.981s, episode steps: 742, steps per second:  44, episode reward: 18.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.319 [0.000, 5.000],  loss: 0.020757, mae: 3.765304, mean_q: 4.534335, mean_eps: 0.100000\n",
      " 1826622/2000000: episode: 2428, duration: 20.883s, episode steps: 929, steps per second:  44, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.947 [0.000, 5.000],  loss: 0.020627, mae: 3.765753, mean_q: 4.536405, mean_eps: 0.100000\n",
      " 1827287/2000000: episode: 2429, duration: 14.840s, episode steps: 665, steps per second:  45, episode reward: 15.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.471 [0.000, 5.000],  loss: 0.020417, mae: 3.779844, mean_q: 4.555637, mean_eps: 0.100000\n",
      " 1828082/2000000: episode: 2430, duration: 17.645s, episode steps: 795, steps per second:  45, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.022827, mae: 3.791595, mean_q: 4.565459, mean_eps: 0.100000\n",
      " 1828561/2000000: episode: 2431, duration: 10.420s, episode steps: 479, steps per second:  46, episode reward: 13.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.020779, mae: 3.776946, mean_q: 4.548266, mean_eps: 0.100000\n",
      " 1829693/2000000: episode: 2432, duration: 25.356s, episode steps: 1132, steps per second:  45, episode reward: 24.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.973 [0.000, 5.000],  loss: 0.022267, mae: 3.785543, mean_q: 4.559842, mean_eps: 0.100000\n",
      " 1830363/2000000: episode: 2433, duration: 14.781s, episode steps: 670, steps per second:  45, episode reward: 10.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.237 [0.000, 5.000],  loss: 0.021512, mae: 3.800133, mean_q: 4.578861, mean_eps: 0.100000\n",
      " 1831221/2000000: episode: 2434, duration: 19.146s, episode steps: 858, steps per second:  45, episode reward: 14.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.073 [0.000, 5.000],  loss: 0.023193, mae: 3.800497, mean_q: 4.576206, mean_eps: 0.100000\n",
      " 1831609/2000000: episode: 2435, duration: 8.688s, episode steps: 388, steps per second:  45, episode reward: 10.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.621 [0.000, 5.000],  loss: 0.029247, mae: 3.833732, mean_q: 4.616390, mean_eps: 0.100000\n",
      " 1832091/2000000: episode: 2436, duration: 10.561s, episode steps: 482, steps per second:  46, episode reward:  5.000, mean reward:  0.010 [ 0.000,  1.000], mean action: 2.880 [0.000, 5.000],  loss: 0.018199, mae: 3.817122, mean_q: 4.600215, mean_eps: 0.100000\n",
      " 1833170/2000000: episode: 2437, duration: 23.908s, episode steps: 1079, steps per second:  45, episode reward: 30.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 3.146 [0.000, 5.000],  loss: 0.020910, mae: 3.806341, mean_q: 4.583830, mean_eps: 0.100000\n",
      " 1833930/2000000: episode: 2438, duration: 16.414s, episode steps: 760, steps per second:  46, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.325 [0.000, 5.000],  loss: 0.023204, mae: 3.822169, mean_q: 4.602013, mean_eps: 0.100000\n",
      " 1834459/2000000: episode: 2439, duration: 11.505s, episode steps: 529, steps per second:  46, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.456 [0.000, 5.000],  loss: 0.023636, mae: 3.818548, mean_q: 4.599741, mean_eps: 0.100000\n",
      " 1835410/2000000: episode: 2440, duration: 20.864s, episode steps: 951, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.666 [0.000, 5.000],  loss: 0.021918, mae: 3.796991, mean_q: 4.572108, mean_eps: 0.100000\n",
      " 1836674/2000000: episode: 2441, duration: 28.092s, episode steps: 1264, steps per second:  45, episode reward: 29.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.248 [0.000, 5.000],  loss: 0.023408, mae: 3.799885, mean_q: 4.576066, mean_eps: 0.100000\n",
      " 1837644/2000000: episode: 2442, duration: 21.680s, episode steps: 970, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.709 [0.000, 5.000],  loss: 0.020560, mae: 3.819461, mean_q: 4.599977, mean_eps: 0.100000\n",
      " 1838097/2000000: episode: 2443, duration: 10.053s, episode steps: 453, steps per second:  45, episode reward:  7.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 3.234 [0.000, 5.000],  loss: 0.021875, mae: 3.800052, mean_q: 4.576651, mean_eps: 0.100000\n",
      " 1838768/2000000: episode: 2444, duration: 14.718s, episode steps: 671, steps per second:  46, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.534 [0.000, 5.000],  loss: 0.022771, mae: 3.788778, mean_q: 4.562989, mean_eps: 0.100000\n",
      " 1839984/2000000: episode: 2445, duration: 26.613s, episode steps: 1216, steps per second:  46, episode reward: 21.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.146 [0.000, 5.000],  loss: 0.022796, mae: 3.812588, mean_q: 4.589654, mean_eps: 0.100000\n",
      " 1840924/2000000: episode: 2446, duration: 20.892s, episode steps: 940, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.190 [0.000, 5.000],  loss: 0.020976, mae: 3.819888, mean_q: 4.602229, mean_eps: 0.100000\n",
      " 1841787/2000000: episode: 2447, duration: 19.253s, episode steps: 863, steps per second:  45, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.707 [0.000, 5.000],  loss: 0.020299, mae: 3.809997, mean_q: 4.589353, mean_eps: 0.100000\n",
      " 1842969/2000000: episode: 2448, duration: 26.116s, episode steps: 1182, steps per second:  45, episode reward: 35.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.558 [0.000, 5.000],  loss: 0.021264, mae: 3.806910, mean_q: 4.584427, mean_eps: 0.100000\n",
      " 1843783/2000000: episode: 2449, duration: 18.005s, episode steps: 814, steps per second:  45, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.891 [0.000, 5.000],  loss: 0.019934, mae: 3.813225, mean_q: 4.592654, mean_eps: 0.100000\n",
      " 1844198/2000000: episode: 2450, duration: 9.121s, episode steps: 415, steps per second:  45, episode reward: 11.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 3.434 [0.000, 5.000],  loss: 0.022009, mae: 3.806314, mean_q: 4.585442, mean_eps: 0.100000\n",
      " 1845382/2000000: episode: 2451, duration: 26.169s, episode steps: 1184, steps per second:  45, episode reward: 31.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.366 [0.000, 5.000],  loss: 0.020504, mae: 3.810708, mean_q: 4.591245, mean_eps: 0.100000\n",
      " 1846194/2000000: episode: 2452, duration: 17.823s, episode steps: 812, steps per second:  46, episode reward: 20.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.821 [0.000, 5.000],  loss: 0.018855, mae: 3.823361, mean_q: 4.606265, mean_eps: 0.100000\n",
      " 1847152/2000000: episode: 2453, duration: 21.163s, episode steps: 958, steps per second:  45, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.982 [0.000, 5.000],  loss: 0.020328, mae: 3.814956, mean_q: 4.593246, mean_eps: 0.100000\n",
      " 1847696/2000000: episode: 2454, duration: 12.260s, episode steps: 544, steps per second:  44, episode reward: 13.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.300 [0.000, 5.000],  loss: 0.021822, mae: 3.820081, mean_q: 4.600160, mean_eps: 0.100000\n",
      " 1848894/2000000: episode: 2455, duration: 26.237s, episode steps: 1198, steps per second:  46, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.139 [0.000, 5.000],  loss: 0.024448, mae: 3.811765, mean_q: 4.592451, mean_eps: 0.100000\n",
      " 1849743/2000000: episode: 2456, duration: 18.876s, episode steps: 849, steps per second:  45, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.823 [0.000, 5.000],  loss: 0.021556, mae: 3.821387, mean_q: 4.602361, mean_eps: 0.100000\n",
      " 1850277/2000000: episode: 2457, duration: 11.460s, episode steps: 534, steps per second:  47, episode reward:  9.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.094 [0.000, 5.000],  loss: 0.022048, mae: 3.828742, mean_q: 4.612526, mean_eps: 0.100000\n",
      " 1850829/2000000: episode: 2458, duration: 12.332s, episode steps: 552, steps per second:  45, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: 0.021376, mae: 3.855602, mean_q: 4.645116, mean_eps: 0.100000\n",
      " 1851388/2000000: episode: 2459, duration: 12.277s, episode steps: 559, steps per second:  46, episode reward: 12.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.914 [0.000, 5.000],  loss: 0.022402, mae: 3.792294, mean_q: 4.568279, mean_eps: 0.100000\n",
      " 1852026/2000000: episode: 2460, duration: 14.021s, episode steps: 638, steps per second:  46, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.906 [0.000, 5.000],  loss: 0.021970, mae: 3.808752, mean_q: 4.584583, mean_eps: 0.100000\n",
      " 1852760/2000000: episode: 2461, duration: 16.055s, episode steps: 734, steps per second:  46, episode reward: 17.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.714 [0.000, 5.000],  loss: 0.024844, mae: 3.839198, mean_q: 4.628231, mean_eps: 0.100000\n",
      " 1853705/2000000: episode: 2462, duration: 20.999s, episode steps: 945, steps per second:  45, episode reward: 18.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.951 [0.000, 5.000],  loss: 0.021791, mae: 3.831062, mean_q: 4.614326, mean_eps: 0.100000\n",
      " 1854825/2000000: episode: 2463, duration: 24.708s, episode steps: 1120, steps per second:  45, episode reward: 28.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.367 [0.000, 5.000],  loss: 0.022974, mae: 3.819487, mean_q: 4.601987, mean_eps: 0.100000\n",
      " 1855725/2000000: episode: 2464, duration: 19.813s, episode steps: 900, steps per second:  45, episode reward: 23.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.599 [0.000, 5.000],  loss: 0.022678, mae: 3.824470, mean_q: 4.604502, mean_eps: 0.100000\n",
      " 1856224/2000000: episode: 2465, duration: 11.048s, episode steps: 499, steps per second:  45, episode reward: 10.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.551 [0.000, 5.000],  loss: 0.027624, mae: 3.859994, mean_q: 4.648217, mean_eps: 0.100000\n",
      " 1856935/2000000: episode: 2466, duration: 15.351s, episode steps: 711, steps per second:  46, episode reward: 19.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.342 [0.000, 5.000],  loss: 0.020255, mae: 3.833154, mean_q: 4.615387, mean_eps: 0.100000\n",
      " 1857980/2000000: episode: 2467, duration: 22.815s, episode steps: 1045, steps per second:  46, episode reward: 21.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.259 [0.000, 5.000],  loss: 0.022147, mae: 3.820301, mean_q: 4.600929, mean_eps: 0.100000\n",
      " 1858448/2000000: episode: 2468, duration: 10.571s, episode steps: 468, steps per second:  44, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.838 [0.000, 5.000],  loss: 0.021389, mae: 3.794168, mean_q: 4.569126, mean_eps: 0.100000\n",
      " 1859074/2000000: episode: 2469, duration: 13.791s, episode steps: 626, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 3.217 [0.000, 5.000],  loss: 0.019281, mae: 3.773169, mean_q: 4.545548, mean_eps: 0.100000\n",
      " 1859616/2000000: episode: 2470, duration: 11.927s, episode steps: 542, steps per second:  45, episode reward:  5.000, mean reward:  0.009 [ 0.000,  1.000], mean action: 1.930 [0.000, 5.000],  loss: 0.024202, mae: 3.823957, mean_q: 4.604546, mean_eps: 0.100000\n",
      " 1861070/2000000: episode: 2471, duration: 31.985s, episode steps: 1454, steps per second:  45, episode reward: 32.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.387 [0.000, 5.000],  loss: 0.022689, mae: 3.828540, mean_q: 4.614792, mean_eps: 0.100000\n",
      " 1862100/2000000: episode: 2472, duration: 22.862s, episode steps: 1030, steps per second:  45, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.607 [0.000, 5.000],  loss: 0.023494, mae: 3.847286, mean_q: 4.635272, mean_eps: 0.100000\n",
      " 1862894/2000000: episode: 2473, duration: 17.568s, episode steps: 794, steps per second:  45, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.606 [0.000, 5.000],  loss: 0.021361, mae: 3.823173, mean_q: 4.605229, mean_eps: 0.100000\n",
      " 1864082/2000000: episode: 2474, duration: 26.219s, episode steps: 1188, steps per second:  45, episode reward: 24.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.023452, mae: 3.836089, mean_q: 4.620575, mean_eps: 0.100000\n",
      " 1864776/2000000: episode: 2475, duration: 15.499s, episode steps: 694, steps per second:  45, episode reward: 18.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.352 [0.000, 5.000],  loss: 0.022286, mae: 3.823671, mean_q: 4.604322, mean_eps: 0.100000\n",
      " 1865816/2000000: episode: 2476, duration: 23.295s, episode steps: 1040, steps per second:  45, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.867 [0.000, 5.000],  loss: 0.027023, mae: 3.831474, mean_q: 4.614333, mean_eps: 0.100000\n",
      " 1866844/2000000: episode: 2477, duration: 22.710s, episode steps: 1028, steps per second:  45, episode reward: 23.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.820 [0.000, 5.000],  loss: 0.020222, mae: 3.826523, mean_q: 4.608763, mean_eps: 0.100000\n",
      " 1867819/2000000: episode: 2478, duration: 21.331s, episode steps: 975, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.410 [0.000, 5.000],  loss: 0.022548, mae: 3.836139, mean_q: 4.623807, mean_eps: 0.100000\n",
      " 1868220/2000000: episode: 2479, duration: 8.891s, episode steps: 401, steps per second:  45, episode reward:  6.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.880 [0.000, 5.000],  loss: 0.021683, mae: 3.841370, mean_q: 4.628377, mean_eps: 0.100000\n",
      " 1869206/2000000: episode: 2480, duration: 21.528s, episode steps: 986, steps per second:  46, episode reward: 17.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.171 [0.000, 5.000],  loss: 0.022138, mae: 3.836203, mean_q: 4.621943, mean_eps: 0.100000\n",
      " 1869856/2000000: episode: 2481, duration: 14.591s, episode steps: 650, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.775 [0.000, 5.000],  loss: 0.024138, mae: 3.828526, mean_q: 4.612575, mean_eps: 0.100000\n",
      " 1870494/2000000: episode: 2482, duration: 14.181s, episode steps: 638, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.019392, mae: 3.807554, mean_q: 4.585269, mean_eps: 0.100000\n",
      " 1870850/2000000: episode: 2483, duration: 8.178s, episode steps: 356, steps per second:  44, episode reward:  4.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.006 [0.000, 5.000],  loss: 0.022141, mae: 3.801338, mean_q: 4.577564, mean_eps: 0.100000\n",
      " 1871516/2000000: episode: 2484, duration: 14.693s, episode steps: 666, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.000 [0.000, 5.000],  loss: 0.020385, mae: 3.799918, mean_q: 4.574834, mean_eps: 0.100000\n",
      " 1871938/2000000: episode: 2485, duration: 9.310s, episode steps: 422, steps per second:  45, episode reward:  8.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.720 [0.000, 5.000],  loss: 0.019064, mae: 3.792963, mean_q: 4.567458, mean_eps: 0.100000\n",
      " 1872826/2000000: episode: 2486, duration: 19.593s, episode steps: 888, steps per second:  45, episode reward: 18.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.019535, mae: 3.802271, mean_q: 4.581984, mean_eps: 0.100000\n",
      " 1873454/2000000: episode: 2487, duration: 13.852s, episode steps: 628, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.384 [0.000, 5.000],  loss: 0.020844, mae: 3.803152, mean_q: 4.580582, mean_eps: 0.100000\n",
      " 1874376/2000000: episode: 2488, duration: 20.572s, episode steps: 922, steps per second:  45, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.023197, mae: 3.813664, mean_q: 4.592103, mean_eps: 0.100000\n",
      " 1874980/2000000: episode: 2489, duration: 13.575s, episode steps: 604, steps per second:  44, episode reward: 12.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.020000, mae: 3.819812, mean_q: 4.599472, mean_eps: 0.100000\n",
      " 1875905/2000000: episode: 2490, duration: 20.250s, episode steps: 925, steps per second:  46, episode reward: 25.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.797 [0.000, 5.000],  loss: 0.024519, mae: 3.789648, mean_q: 4.566023, mean_eps: 0.100000\n",
      " 1876625/2000000: episode: 2491, duration: 16.018s, episode steps: 720, steps per second:  45, episode reward: 18.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.844 [0.000, 5.000],  loss: 0.021908, mae: 3.807855, mean_q: 4.587077, mean_eps: 0.100000\n",
      " 1877015/2000000: episode: 2492, duration: 8.453s, episode steps: 390, steps per second:  46, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.715 [0.000, 5.000],  loss: 0.022478, mae: 3.805638, mean_q: 4.581013, mean_eps: 0.100000\n",
      " 1878191/2000000: episode: 2493, duration: 26.111s, episode steps: 1176, steps per second:  45, episode reward: 26.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.402 [0.000, 5.000],  loss: 0.020894, mae: 3.800174, mean_q: 4.576520, mean_eps: 0.100000\n",
      " 1878642/2000000: episode: 2494, duration: 9.924s, episode steps: 451, steps per second:  45, episode reward:  9.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.215 [0.000, 5.000],  loss: 0.022282, mae: 3.804782, mean_q: 4.583373, mean_eps: 0.100000\n",
      " 1879348/2000000: episode: 2495, duration: 15.491s, episode steps: 706, steps per second:  46, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.354 [0.000, 5.000],  loss: 0.022768, mae: 3.802960, mean_q: 4.580473, mean_eps: 0.100000\n",
      " 1880148/2000000: episode: 2496, duration: 18.155s, episode steps: 800, steps per second:  44, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.490 [0.000, 5.000],  loss: 0.024598, mae: 3.816859, mean_q: 4.599401, mean_eps: 0.100000\n",
      " 1880811/2000000: episode: 2497, duration: 14.689s, episode steps: 663, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.020541, mae: 3.836056, mean_q: 4.623098, mean_eps: 0.100000\n",
      " 1881667/2000000: episode: 2498, duration: 18.951s, episode steps: 856, steps per second:  45, episode reward: 23.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.362 [0.000, 5.000],  loss: 0.022134, mae: 3.823437, mean_q: 4.606364, mean_eps: 0.100000\n",
      " 1882193/2000000: episode: 2499, duration: 11.457s, episode steps: 526, steps per second:  46, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.414 [0.000, 5.000],  loss: 0.025554, mae: 3.817524, mean_q: 4.595054, mean_eps: 0.100000\n",
      " 1882943/2000000: episode: 2500, duration: 16.434s, episode steps: 750, steps per second:  46, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.857 [0.000, 5.000],  loss: 0.019985, mae: 3.834208, mean_q: 4.617432, mean_eps: 0.100000\n",
      " 1883557/2000000: episode: 2501, duration: 13.540s, episode steps: 614, steps per second:  45, episode reward: 16.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 3.160 [0.000, 5.000],  loss: 0.024678, mae: 3.859138, mean_q: 4.648650, mean_eps: 0.100000\n",
      " 1884558/2000000: episode: 2502, duration: 22.381s, episode steps: 1001, steps per second:  45, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.062 [0.000, 5.000],  loss: 0.022780, mae: 3.841861, mean_q: 4.628576, mean_eps: 0.100000\n",
      " 1885356/2000000: episode: 2503, duration: 17.592s, episode steps: 798, steps per second:  45, episode reward: 18.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.877 [0.000, 5.000],  loss: 0.023846, mae: 3.841982, mean_q: 4.629748, mean_eps: 0.100000\n",
      " 1885836/2000000: episode: 2504, duration: 11.079s, episode steps: 480, steps per second:  43, episode reward: 12.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 3.223 [0.000, 5.000],  loss: 0.018853, mae: 3.840116, mean_q: 4.629990, mean_eps: 0.100000\n",
      " 1886792/2000000: episode: 2505, duration: 21.472s, episode steps: 956, steps per second:  45, episode reward: 27.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.552 [0.000, 5.000],  loss: 0.022955, mae: 3.825854, mean_q: 4.606202, mean_eps: 0.100000\n",
      " 1887765/2000000: episode: 2506, duration: 21.656s, episode steps: 973, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.929 [0.000, 5.000],  loss: 0.020814, mae: 3.833442, mean_q: 4.620369, mean_eps: 0.100000\n",
      " 1888626/2000000: episode: 2507, duration: 19.204s, episode steps: 861, steps per second:  45, episode reward: 22.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.553 [0.000, 5.000],  loss: 0.019119, mae: 3.837626, mean_q: 4.625552, mean_eps: 0.100000\n",
      " 1889236/2000000: episode: 2508, duration: 13.543s, episode steps: 610, steps per second:  45, episode reward: 15.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: 0.019971, mae: 3.828266, mean_q: 4.610919, mean_eps: 0.100000\n",
      " 1890194/2000000: episode: 2509, duration: 21.175s, episode steps: 958, steps per second:  45, episode reward: 23.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.718 [0.000, 5.000],  loss: 0.022853, mae: 3.826978, mean_q: 4.610932, mean_eps: 0.100000\n",
      " 1890925/2000000: episode: 2510, duration: 15.761s, episode steps: 731, steps per second:  46, episode reward: 19.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.479 [0.000, 5.000],  loss: 0.023819, mae: 3.841415, mean_q: 4.624901, mean_eps: 0.100000\n",
      " 1891423/2000000: episode: 2511, duration: 11.533s, episode steps: 498, steps per second:  43, episode reward: 13.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.912 [0.000, 5.000],  loss: 0.025220, mae: 3.851990, mean_q: 4.640535, mean_eps: 0.100000\n",
      " 1891800/2000000: episode: 2512, duration: 8.569s, episode steps: 377, steps per second:  44, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.886 [0.000, 5.000],  loss: 0.023970, mae: 3.837769, mean_q: 4.623137, mean_eps: 0.100000\n",
      " 1892457/2000000: episode: 2513, duration: 14.399s, episode steps: 657, steps per second:  46, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.040 [0.000, 5.000],  loss: 0.023229, mae: 3.855426, mean_q: 4.645365, mean_eps: 0.100000\n",
      " 1893362/2000000: episode: 2514, duration: 19.842s, episode steps: 905, steps per second:  46, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.749 [0.000, 5.000],  loss: 0.025123, mae: 3.801332, mean_q: 4.579167, mean_eps: 0.100000\n",
      " 1894327/2000000: episode: 2515, duration: 21.241s, episode steps: 965, steps per second:  45, episode reward: 30.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.650 [0.000, 5.000],  loss: 0.026275, mae: 3.846718, mean_q: 4.632688, mean_eps: 0.100000\n",
      " 1894943/2000000: episode: 2516, duration: 13.743s, episode steps: 616, steps per second:  45, episode reward: 14.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.706 [0.000, 5.000],  loss: 0.023826, mae: 3.827967, mean_q: 4.611240, mean_eps: 0.100000\n",
      " 1895579/2000000: episode: 2517, duration: 13.980s, episode steps: 636, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.498 [0.000, 5.000],  loss: 0.022456, mae: 3.821844, mean_q: 4.603698, mean_eps: 0.100000\n",
      " 1896104/2000000: episode: 2518, duration: 11.717s, episode steps: 525, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.023 [0.000, 5.000],  loss: 0.021148, mae: 3.822919, mean_q: 4.606282, mean_eps: 0.100000\n",
      " 1897031/2000000: episode: 2519, duration: 20.501s, episode steps: 927, steps per second:  45, episode reward: 23.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.528 [0.000, 5.000],  loss: 0.023359, mae: 3.835178, mean_q: 4.619776, mean_eps: 0.100000\n",
      " 1897688/2000000: episode: 2520, duration: 14.652s, episode steps: 657, steps per second:  45, episode reward: 12.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.081 [0.000, 5.000],  loss: 0.023114, mae: 3.825677, mean_q: 4.610416, mean_eps: 0.100000\n",
      " 1898711/2000000: episode: 2521, duration: 22.401s, episode steps: 1023, steps per second:  46, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.290 [0.000, 5.000],  loss: 0.021768, mae: 3.844325, mean_q: 4.630591, mean_eps: 0.100000\n",
      " 1899416/2000000: episode: 2522, duration: 15.616s, episode steps: 705, steps per second:  45, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.913 [0.000, 5.000],  loss: 0.020992, mae: 3.845017, mean_q: 4.630248, mean_eps: 0.100000\n",
      " 1900298/2000000: episode: 2523, duration: 19.324s, episode steps: 882, steps per second:  46, episode reward: 16.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.399 [0.000, 5.000],  loss: 0.022660, mae: 3.850818, mean_q: 4.641038, mean_eps: 0.100000\n",
      " 1900975/2000000: episode: 2524, duration: 14.793s, episode steps: 677, steps per second:  46, episode reward: 18.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.223 [0.000, 5.000],  loss: 0.022811, mae: 3.821987, mean_q: 4.605979, mean_eps: 0.100000\n",
      " 1901916/2000000: episode: 2525, duration: 20.357s, episode steps: 941, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.665 [0.000, 5.000],  loss: 0.022501, mae: 3.812647, mean_q: 4.593058, mean_eps: 0.100000\n",
      " 1903240/2000000: episode: 2526, duration: 29.549s, episode steps: 1324, steps per second:  45, episode reward: 33.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.363 [0.000, 5.000],  loss: 0.025087, mae: 3.830577, mean_q: 4.613568, mean_eps: 0.100000\n",
      " 1904215/2000000: episode: 2527, duration: 21.587s, episode steps: 975, steps per second:  45, episode reward: 25.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.132 [0.000, 5.000],  loss: 0.024821, mae: 3.809462, mean_q: 4.587659, mean_eps: 0.100000\n",
      " 1905307/2000000: episode: 2528, duration: 24.048s, episode steps: 1092, steps per second:  45, episode reward: 28.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.419 [0.000, 5.000],  loss: 0.021812, mae: 3.830606, mean_q: 4.615692, mean_eps: 0.100000\n",
      " 1905947/2000000: episode: 2529, duration: 13.867s, episode steps: 640, steps per second:  46, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.023458, mae: 3.808210, mean_q: 4.587344, mean_eps: 0.100000\n",
      " 1906950/2000000: episode: 2530, duration: 22.074s, episode steps: 1003, steps per second:  45, episode reward: 26.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.126 [0.000, 5.000],  loss: 0.024441, mae: 3.833699, mean_q: 4.616593, mean_eps: 0.100000\n",
      " 1908231/2000000: episode: 2531, duration: 28.152s, episode steps: 1281, steps per second:  46, episode reward: 32.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.187 [0.000, 5.000],  loss: 0.020885, mae: 3.829625, mean_q: 4.613792, mean_eps: 0.100000\n",
      " 1909307/2000000: episode: 2532, duration: 24.072s, episode steps: 1076, steps per second:  45, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.311 [0.000, 5.000],  loss: 0.023255, mae: 3.827403, mean_q: 4.609838, mean_eps: 0.100000\n",
      " 1910235/2000000: episode: 2533, duration: 20.469s, episode steps: 928, steps per second:  45, episode reward: 24.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.768 [0.000, 5.000],  loss: 0.022428, mae: 3.835389, mean_q: 4.622840, mean_eps: 0.100000\n",
      " 1910876/2000000: episode: 2534, duration: 14.179s, episode steps: 641, steps per second:  45, episode reward: 16.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.189 [0.000, 5.000],  loss: 0.019603, mae: 3.819576, mean_q: 4.602023, mean_eps: 0.100000\n",
      " 1911896/2000000: episode: 2535, duration: 22.447s, episode steps: 1020, steps per second:  45, episode reward: 20.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.514 [0.000, 5.000],  loss: 0.022056, mae: 3.830083, mean_q: 4.612193, mean_eps: 0.100000\n",
      " 1912937/2000000: episode: 2536, duration: 23.294s, episode steps: 1041, steps per second:  45, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.834 [0.000, 5.000],  loss: 0.021987, mae: 3.817184, mean_q: 4.597624, mean_eps: 0.100000\n",
      " 1913475/2000000: episode: 2537, duration: 11.999s, episode steps: 538, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.400 [0.000, 5.000],  loss: 0.019736, mae: 3.817494, mean_q: 4.602566, mean_eps: 0.100000\n",
      " 1914935/2000000: episode: 2538, duration: 31.976s, episode steps: 1460, steps per second:  46, episode reward: 35.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.022064, mae: 3.839707, mean_q: 4.625315, mean_eps: 0.100000\n",
      " 1915695/2000000: episode: 2539, duration: 16.767s, episode steps: 760, steps per second:  45, episode reward: 15.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 1.936 [0.000, 5.000],  loss: 0.021399, mae: 3.837927, mean_q: 4.622969, mean_eps: 0.100000\n",
      " 1916720/2000000: episode: 2540, duration: 22.345s, episode steps: 1025, steps per second:  46, episode reward: 31.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.023162, mae: 3.823381, mean_q: 4.604269, mean_eps: 0.100000\n",
      " 1918049/2000000: episode: 2541, duration: 29.197s, episode steps: 1329, steps per second:  46, episode reward: 32.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.104 [0.000, 5.000],  loss: 0.022984, mae: 3.830430, mean_q: 4.615595, mean_eps: 0.100000\n",
      " 1918532/2000000: episode: 2542, duration: 11.137s, episode steps: 483, steps per second:  43, episode reward:  9.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 1.952 [0.000, 5.000],  loss: 0.020310, mae: 3.830658, mean_q: 4.613089, mean_eps: 0.100000\n",
      " 1919246/2000000: episode: 2543, duration: 15.882s, episode steps: 714, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.406 [0.000, 5.000],  loss: 0.021955, mae: 3.845513, mean_q: 4.632471, mean_eps: 0.100000\n",
      " 1920396/2000000: episode: 2544, duration: 25.351s, episode steps: 1150, steps per second:  45, episode reward: 25.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.172 [0.000, 5.000],  loss: 0.021016, mae: 3.826752, mean_q: 4.606957, mean_eps: 0.100000\n",
      " 1921124/2000000: episode: 2545, duration: 16.059s, episode steps: 728, steps per second:  45, episode reward: 15.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.250 [0.000, 5.000],  loss: 0.020862, mae: 3.836059, mean_q: 4.621629, mean_eps: 0.100000\n",
      " 1921727/2000000: episode: 2546, duration: 13.510s, episode steps: 603, steps per second:  45, episode reward: 13.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.308 [0.000, 5.000],  loss: 0.020973, mae: 3.839537, mean_q: 4.622046, mean_eps: 0.100000\n",
      " 1922421/2000000: episode: 2547, duration: 15.350s, episode steps: 694, steps per second:  45, episode reward: 17.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 1.990 [0.000, 5.000],  loss: 0.022717, mae: 3.844972, mean_q: 4.628833, mean_eps: 0.100000\n",
      " 1923323/2000000: episode: 2548, duration: 19.655s, episode steps: 902, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.440 [0.000, 5.000],  loss: 0.022531, mae: 3.848099, mean_q: 4.633601, mean_eps: 0.100000\n",
      " 1924388/2000000: episode: 2549, duration: 24.061s, episode steps: 1065, steps per second:  44, episode reward: 29.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.077 [0.000, 5.000],  loss: 0.023880, mae: 3.841528, mean_q: 4.627486, mean_eps: 0.100000\n",
      " 1925613/2000000: episode: 2550, duration: 27.030s, episode steps: 1225, steps per second:  45, episode reward: 34.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.049 [0.000, 5.000],  loss: 0.023555, mae: 3.846732, mean_q: 4.635592, mean_eps: 0.100000\n",
      " 1926567/2000000: episode: 2551, duration: 21.124s, episode steps: 954, steps per second:  45, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.364 [0.000, 5.000],  loss: 0.023601, mae: 3.818437, mean_q: 4.600279, mean_eps: 0.100000\n",
      " 1927485/2000000: episode: 2552, duration: 20.424s, episode steps: 918, steps per second:  45, episode reward: 17.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.597 [0.000, 5.000],  loss: 0.024314, mae: 3.836104, mean_q: 4.623867, mean_eps: 0.100000\n",
      " 1928567/2000000: episode: 2553, duration: 23.923s, episode steps: 1082, steps per second:  45, episode reward: 25.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 1.882 [0.000, 5.000],  loss: 0.024751, mae: 3.851705, mean_q: 4.640311, mean_eps: 0.100000\n",
      " 1929527/2000000: episode: 2554, duration: 21.268s, episode steps: 960, steps per second:  45, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.022679, mae: 3.844163, mean_q: 4.631247, mean_eps: 0.100000\n",
      " 1929928/2000000: episode: 2555, duration: 8.859s, episode steps: 401, steps per second:  45, episode reward:  7.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 3.364 [0.000, 5.000],  loss: 0.028143, mae: 3.849419, mean_q: 4.636802, mean_eps: 0.100000\n",
      " 1930485/2000000: episode: 2556, duration: 12.519s, episode steps: 557, steps per second:  44, episode reward: 12.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.811 [0.000, 5.000],  loss: 0.023551, mae: 3.844475, mean_q: 4.635123, mean_eps: 0.100000\n",
      " 1931032/2000000: episode: 2557, duration: 12.131s, episode steps: 547, steps per second:  45, episode reward: 11.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.463 [0.000, 5.000],  loss: 0.022614, mae: 3.849672, mean_q: 4.637343, mean_eps: 0.100000\n",
      " 1931721/2000000: episode: 2558, duration: 15.064s, episode steps: 689, steps per second:  46, episode reward: 14.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.451 [0.000, 5.000],  loss: 0.019080, mae: 3.835663, mean_q: 4.623700, mean_eps: 0.100000\n",
      " 1932692/2000000: episode: 2559, duration: 21.133s, episode steps: 971, steps per second:  46, episode reward: 24.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: 0.022531, mae: 3.852682, mean_q: 4.640132, mean_eps: 0.100000\n",
      " 1933505/2000000: episode: 2560, duration: 18.011s, episode steps: 813, steps per second:  45, episode reward: 25.000, mean reward:  0.031 [ 0.000,  1.000], mean action: 2.355 [0.000, 5.000],  loss: 0.022897, mae: 3.838776, mean_q: 4.623585, mean_eps: 0.100000\n",
      " 1933885/2000000: episode: 2561, duration: 8.321s, episode steps: 380, steps per second:  46, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.147 [0.000, 5.000],  loss: 0.024142, mae: 3.832206, mean_q: 4.618594, mean_eps: 0.100000\n",
      " 1934795/2000000: episode: 2562, duration: 19.775s, episode steps: 910, steps per second:  46, episode reward: 26.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.612 [0.000, 5.000],  loss: 0.020758, mae: 3.857271, mean_q: 4.649586, mean_eps: 0.100000\n",
      " 1935472/2000000: episode: 2563, duration: 15.347s, episode steps: 677, steps per second:  44, episode reward: 13.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.020914, mae: 3.823925, mean_q: 4.607180, mean_eps: 0.100000\n",
      " 1936253/2000000: episode: 2564, duration: 17.497s, episode steps: 781, steps per second:  45, episode reward: 22.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.725 [0.000, 5.000],  loss: 0.020386, mae: 3.839309, mean_q: 4.625845, mean_eps: 0.100000\n",
      " 1937298/2000000: episode: 2565, duration: 23.169s, episode steps: 1045, steps per second:  45, episode reward: 17.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.165 [0.000, 5.000],  loss: 0.020427, mae: 3.831044, mean_q: 4.613859, mean_eps: 0.100000\n",
      " 1938032/2000000: episode: 2566, duration: 16.137s, episode steps: 734, steps per second:  45, episode reward: 12.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.057 [0.000, 5.000],  loss: 0.023515, mae: 3.851870, mean_q: 4.639109, mean_eps: 0.100000\n",
      " 1939065/2000000: episode: 2567, duration: 22.741s, episode steps: 1033, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.258 [0.000, 5.000],  loss: 0.024903, mae: 3.825829, mean_q: 4.608155, mean_eps: 0.100000\n",
      " 1939431/2000000: episode: 2568, duration: 8.054s, episode steps: 366, steps per second:  45, episode reward:  6.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.530 [0.000, 5.000],  loss: 0.027416, mae: 3.872971, mean_q: 4.664958, mean_eps: 0.100000\n",
      " 1939775/2000000: episode: 2569, duration: 7.376s, episode steps: 344, steps per second:  47, episode reward:  6.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 1.631 [0.000, 5.000],  loss: 0.015848, mae: 3.827791, mean_q: 4.611841, mean_eps: 0.100000\n",
      " 1940435/2000000: episode: 2570, duration: 14.684s, episode steps: 660, steps per second:  45, episode reward: 16.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 2.965 [0.000, 5.000],  loss: 0.019689, mae: 3.846308, mean_q: 4.636231, mean_eps: 0.100000\n",
      " 1941250/2000000: episode: 2571, duration: 17.965s, episode steps: 815, steps per second:  45, episode reward: 18.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.545 [0.000, 5.000],  loss: 0.024153, mae: 3.859577, mean_q: 4.649342, mean_eps: 0.100000\n",
      " 1942217/2000000: episode: 2572, duration: 21.659s, episode steps: 967, steps per second:  45, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.632 [0.000, 5.000],  loss: 0.021489, mae: 3.873658, mean_q: 4.666237, mean_eps: 0.100000\n",
      " 1942687/2000000: episode: 2573, duration: 10.270s, episode steps: 470, steps per second:  46, episode reward:  6.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.734 [0.000, 5.000],  loss: 0.022850, mae: 3.849557, mean_q: 4.641235, mean_eps: 0.100000\n",
      " 1943221/2000000: episode: 2574, duration: 11.858s, episode steps: 534, steps per second:  45, episode reward: 11.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 3.021 [0.000, 5.000],  loss: 0.022724, mae: 3.835299, mean_q: 4.623429, mean_eps: 0.100000\n",
      " 1944107/2000000: episode: 2575, duration: 19.671s, episode steps: 886, steps per second:  45, episode reward: 20.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.726 [0.000, 5.000],  loss: 0.023992, mae: 3.857812, mean_q: 4.649012, mean_eps: 0.100000\n",
      " 1944847/2000000: episode: 2576, duration: 16.377s, episode steps: 740, steps per second:  45, episode reward: 16.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.904 [0.000, 5.000],  loss: 0.024309, mae: 3.859951, mean_q: 4.647280, mean_eps: 0.100000\n",
      " 1946132/2000000: episode: 2577, duration: 28.750s, episode steps: 1285, steps per second:  45, episode reward: 30.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.728 [0.000, 5.000],  loss: 0.023828, mae: 3.868554, mean_q: 4.660278, mean_eps: 0.100000\n",
      " 1946666/2000000: episode: 2578, duration: 11.924s, episode steps: 534, steps per second:  45, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.193 [0.000, 5.000],  loss: 0.023788, mae: 3.859568, mean_q: 4.650598, mean_eps: 0.100000\n",
      " 1947476/2000000: episode: 2579, duration: 17.975s, episode steps: 810, steps per second:  45, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.051 [0.000, 5.000],  loss: 0.020470, mae: 3.854418, mean_q: 4.643788, mean_eps: 0.100000\n",
      " 1948238/2000000: episode: 2580, duration: 17.072s, episode steps: 762, steps per second:  45, episode reward: 16.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.168 [0.000, 5.000],  loss: 0.022805, mae: 3.845663, mean_q: 4.634659, mean_eps: 0.100000\n",
      " 1948893/2000000: episode: 2581, duration: 14.486s, episode steps: 655, steps per second:  45, episode reward: 13.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 3.011 [0.000, 5.000],  loss: 0.021980, mae: 3.869109, mean_q: 4.660197, mean_eps: 0.100000\n",
      " 1949341/2000000: episode: 2582, duration: 9.765s, episode steps: 448, steps per second:  46, episode reward:  7.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.663 [0.000, 5.000],  loss: 0.019266, mae: 3.854076, mean_q: 4.642149, mean_eps: 0.100000\n",
      " 1949755/2000000: episode: 2583, duration: 9.179s, episode steps: 414, steps per second:  45, episode reward: 10.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.087 [0.000, 5.000],  loss: 0.023753, mae: 3.865343, mean_q: 4.656145, mean_eps: 0.100000\n",
      " 1950448/2000000: episode: 2584, duration: 15.090s, episode steps: 693, steps per second:  46, episode reward: 11.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 1.980 [0.000, 5.000],  loss: 0.018146, mae: 3.842444, mean_q: 4.627430, mean_eps: 0.100000\n",
      " 1951001/2000000: episode: 2585, duration: 12.181s, episode steps: 553, steps per second:  45, episode reward:  7.000, mean reward:  0.013 [ 0.000,  1.000], mean action: 2.633 [0.000, 5.000],  loss: 0.023083, mae: 3.857087, mean_q: 4.643937, mean_eps: 0.100000\n",
      " 1952164/2000000: episode: 2586, duration: 26.165s, episode steps: 1163, steps per second:  44, episode reward: 25.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.611 [0.000, 5.000],  loss: 0.020295, mae: 3.848842, mean_q: 4.636377, mean_eps: 0.100000\n",
      " 1953048/2000000: episode: 2587, duration: 19.417s, episode steps: 884, steps per second:  46, episode reward: 24.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.330 [0.000, 5.000],  loss: 0.020314, mae: 3.839798, mean_q: 4.625014, mean_eps: 0.100000\n",
      " 1954158/2000000: episode: 2588, duration: 24.779s, episode steps: 1110, steps per second:  45, episode reward: 22.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.019943, mae: 3.869533, mean_q: 4.660630, mean_eps: 0.100000\n",
      " 1954786/2000000: episode: 2589, duration: 13.852s, episode steps: 628, steps per second:  45, episode reward: 13.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.763 [0.000, 5.000],  loss: 0.022461, mae: 3.858041, mean_q: 4.647884, mean_eps: 0.100000\n",
      " 1955573/2000000: episode: 2590, duration: 17.127s, episode steps: 787, steps per second:  46, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.835 [0.000, 5.000],  loss: 0.025729, mae: 3.864717, mean_q: 4.652199, mean_eps: 0.100000\n",
      " 1956329/2000000: episode: 2591, duration: 16.397s, episode steps: 756, steps per second:  46, episode reward: 26.000, mean reward:  0.034 [ 0.000,  1.000], mean action: 2.700 [0.000, 5.000],  loss: 0.022195, mae: 3.856526, mean_q: 4.645003, mean_eps: 0.100000\n",
      " 1957185/2000000: episode: 2592, duration: 19.177s, episode steps: 856, steps per second:  45, episode reward: 25.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.542 [0.000, 5.000],  loss: 0.020754, mae: 3.841278, mean_q: 4.626800, mean_eps: 0.100000\n",
      " 1958097/2000000: episode: 2593, duration: 20.189s, episode steps: 912, steps per second:  45, episode reward: 21.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.298 [0.000, 5.000],  loss: 0.019580, mae: 3.854875, mean_q: 4.641549, mean_eps: 0.100000\n",
      " 1958596/2000000: episode: 2594, duration: 11.153s, episode steps: 499, steps per second:  45, episode reward:  7.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 3.615 [0.000, 5.000],  loss: 0.018942, mae: 3.859919, mean_q: 4.650099, mean_eps: 0.100000\n",
      " 1959695/2000000: episode: 2595, duration: 24.341s, episode steps: 1099, steps per second:  45, episode reward: 27.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 2.069 [0.000, 5.000],  loss: 0.022679, mae: 3.849076, mean_q: 4.634522, mean_eps: 0.100000\n",
      " 1960341/2000000: episode: 2596, duration: 14.724s, episode steps: 646, steps per second:  44, episode reward: 14.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.023339, mae: 3.855680, mean_q: 4.645133, mean_eps: 0.100000\n",
      " 1961501/2000000: episode: 2597, duration: 25.517s, episode steps: 1160, steps per second:  45, episode reward: 33.000, mean reward:  0.028 [ 0.000,  1.000], mean action: 2.512 [0.000, 5.000],  loss: 0.021975, mae: 3.868295, mean_q: 4.660752, mean_eps: 0.100000\n",
      " 1962310/2000000: episode: 2598, duration: 18.045s, episode steps: 809, steps per second:  45, episode reward: 13.000, mean reward:  0.016 [ 0.000,  1.000], mean action: 2.565 [0.000, 5.000],  loss: 0.025753, mae: 3.879474, mean_q: 4.673304, mean_eps: 0.100000\n",
      " 1962937/2000000: episode: 2599, duration: 14.092s, episode steps: 627, steps per second:  44, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.255 [0.000, 5.000],  loss: 0.024668, mae: 3.878194, mean_q: 4.673836, mean_eps: 0.100000\n",
      " 1963618/2000000: episode: 2600, duration: 15.055s, episode steps: 681, steps per second:  45, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 3.090 [0.000, 5.000],  loss: 0.023647, mae: 3.879383, mean_q: 4.672882, mean_eps: 0.100000\n",
      " 1964311/2000000: episode: 2601, duration: 15.637s, episode steps: 693, steps per second:  44, episode reward: 20.000, mean reward:  0.029 [ 0.000,  1.000], mean action: 2.369 [0.000, 5.000],  loss: 0.022284, mae: 3.871653, mean_q: 4.664172, mean_eps: 0.100000\n",
      " 1965102/2000000: episode: 2602, duration: 17.854s, episode steps: 791, steps per second:  44, episode reward: 19.000, mean reward:  0.024 [ 0.000,  1.000], mean action: 3.102 [0.000, 5.000],  loss: 0.021973, mae: 3.865413, mean_q: 4.656646, mean_eps: 0.100000\n",
      " 1966065/2000000: episode: 2603, duration: 21.774s, episode steps: 963, steps per second:  44, episode reward: 16.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.313 [0.000, 5.000],  loss: 0.021279, mae: 3.857376, mean_q: 4.649323, mean_eps: 0.100000\n",
      " 1967109/2000000: episode: 2604, duration: 23.185s, episode steps: 1044, steps per second:  45, episode reward: 27.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.163 [0.000, 5.000],  loss: 0.024201, mae: 3.868396, mean_q: 4.661840, mean_eps: 0.100000\n",
      " 1967489/2000000: episode: 2605, duration: 8.544s, episode steps: 380, steps per second:  44, episode reward:  7.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.613 [0.000, 5.000],  loss: 0.019534, mae: 3.884943, mean_q: 4.684139, mean_eps: 0.100000\n",
      " 1968294/2000000: episode: 2606, duration: 18.118s, episode steps: 805, steps per second:  44, episode reward: 22.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.595 [0.000, 5.000],  loss: 0.023471, mae: 3.861183, mean_q: 4.651701, mean_eps: 0.100000\n",
      " 1969061/2000000: episode: 2607, duration: 17.244s, episode steps: 767, steps per second:  44, episode reward: 23.000, mean reward:  0.030 [ 0.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.022744, mae: 3.874642, mean_q: 4.666259, mean_eps: 0.100000\n",
      " 1969830/2000000: episode: 2608, duration: 17.050s, episode steps: 769, steps per second:  45, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.231 [0.000, 5.000],  loss: 0.027841, mae: 3.895051, mean_q: 4.689590, mean_eps: 0.100000\n",
      " 1970489/2000000: episode: 2609, duration: 14.661s, episode steps: 659, steps per second:  45, episode reward: 14.000, mean reward:  0.021 [ 0.000,  1.000], mean action: 2.684 [0.000, 5.000],  loss: 0.024340, mae: 3.873901, mean_q: 4.667085, mean_eps: 0.100000\n",
      " 1971869/2000000: episode: 2610, duration: 30.222s, episode steps: 1380, steps per second:  46, episode reward: 31.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 1.828 [0.000, 5.000],  loss: 0.022202, mae: 3.862088, mean_q: 4.652075, mean_eps: 0.100000\n",
      " 1973005/2000000: episode: 2611, duration: 25.152s, episode steps: 1136, steps per second:  45, episode reward: 17.000, mean reward:  0.015 [ 0.000,  1.000], mean action: 2.680 [0.000, 5.000],  loss: 0.023522, mae: 3.868775, mean_q: 4.662842, mean_eps: 0.100000\n",
      " 1973537/2000000: episode: 2612, duration: 11.899s, episode steps: 532, steps per second:  45, episode reward: 10.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.910 [0.000, 5.000],  loss: 0.020788, mae: 3.900800, mean_q: 4.698735, mean_eps: 0.100000\n",
      " 1974198/2000000: episode: 2613, duration: 14.590s, episode steps: 661, steps per second:  45, episode reward: 11.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.023235, mae: 3.875090, mean_q: 4.668303, mean_eps: 0.100000\n",
      " 1975286/2000000: episode: 2614, duration: 24.108s, episode steps: 1088, steps per second:  45, episode reward: 21.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.752 [0.000, 5.000],  loss: 0.021914, mae: 3.857404, mean_q: 4.649658, mean_eps: 0.100000\n",
      " 1976623/2000000: episode: 2615, duration: 29.115s, episode steps: 1337, steps per second:  46, episode reward: 29.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.585 [0.000, 5.000],  loss: 0.022527, mae: 3.892033, mean_q: 4.690653, mean_eps: 0.100000\n",
      " 1977307/2000000: episode: 2616, duration: 15.162s, episode steps: 684, steps per second:  45, episode reward: 17.000, mean reward:  0.025 [ 0.000,  1.000], mean action: 1.852 [0.000, 5.000],  loss: 0.020541, mae: 3.878763, mean_q: 4.674750, mean_eps: 0.100000\n",
      " 1977746/2000000: episode: 2617, duration: 9.853s, episode steps: 439, steps per second:  45, episode reward:  8.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.269 [0.000, 5.000],  loss: 0.021820, mae: 3.898125, mean_q: 4.694531, mean_eps: 0.100000\n",
      " 1978393/2000000: episode: 2618, duration: 14.280s, episode steps: 647, steps per second:  45, episode reward: 12.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.063 [0.000, 5.000],  loss: 0.021642, mae: 3.880641, mean_q: 4.676402, mean_eps: 0.100000\n",
      " 1979384/2000000: episode: 2619, duration: 22.128s, episode steps: 991, steps per second:  45, episode reward: 19.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: 0.025818, mae: 3.897002, mean_q: 4.695079, mean_eps: 0.100000\n",
      " 1980340/2000000: episode: 2620, duration: 21.178s, episode steps: 956, steps per second:  45, episode reward: 22.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.725 [0.000, 5.000],  loss: 0.020128, mae: 3.880070, mean_q: 4.675274, mean_eps: 0.100000\n",
      " 1980802/2000000: episode: 2621, duration: 10.579s, episode steps: 462, steps per second:  44, episode reward: 10.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.110 [0.000, 5.000],  loss: 0.026360, mae: 3.879458, mean_q: 4.673115, mean_eps: 0.100000\n",
      " 1981933/2000000: episode: 2622, duration: 25.039s, episode steps: 1131, steps per second:  45, episode reward: 26.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.316 [0.000, 5.000],  loss: 0.023090, mae: 3.879236, mean_q: 4.674381, mean_eps: 0.100000\n",
      " 1982907/2000000: episode: 2623, duration: 21.244s, episode steps: 974, steps per second:  46, episode reward: 21.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.321 [0.000, 5.000],  loss: 0.022784, mae: 3.882389, mean_q: 4.675094, mean_eps: 0.100000\n",
      " 1983690/2000000: episode: 2624, duration: 17.422s, episode steps: 783, steps per second:  45, episode reward: 15.000, mean reward:  0.019 [ 0.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: 0.022714, mae: 3.883306, mean_q: 4.676274, mean_eps: 0.100000\n",
      " 1984852/2000000: episode: 2625, duration: 26.174s, episode steps: 1162, steps per second:  44, episode reward: 27.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.121 [0.000, 5.000],  loss: 0.022946, mae: 3.892895, mean_q: 4.689375, mean_eps: 0.100000\n",
      " 1986317/2000000: episode: 2626, duration: 32.259s, episode steps: 1465, steps per second:  45, episode reward: 25.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.423 [0.000, 5.000],  loss: 0.021325, mae: 3.874988, mean_q: 4.665544, mean_eps: 0.100000\n",
      " 1987105/2000000: episode: 2627, duration: 17.696s, episode steps: 788, steps per second:  45, episode reward: 21.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.787 [0.000, 5.000],  loss: 0.023745, mae: 3.885941, mean_q: 4.681333, mean_eps: 0.100000\n",
      " 1987866/2000000: episode: 2628, duration: 16.861s, episode steps: 761, steps per second:  45, episode reward: 20.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.644 [0.000, 5.000],  loss: 0.024243, mae: 3.881152, mean_q: 4.673133, mean_eps: 0.100000\n",
      " 1988688/2000000: episode: 2629, duration: 17.996s, episode steps: 822, steps per second:  46, episode reward: 15.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 2.678 [0.000, 5.000],  loss: 0.022788, mae: 3.886144, mean_q: 4.681307, mean_eps: 0.100000\n",
      " 1989320/2000000: episode: 2630, duration: 13.971s, episode steps: 632, steps per second:  45, episode reward:  9.000, mean reward:  0.014 [ 0.000,  1.000], mean action: 1.962 [0.000, 5.000],  loss: 0.023294, mae: 3.864556, mean_q: 4.654946, mean_eps: 0.100000\n",
      " 1989893/2000000: episode: 2631, duration: 13.172s, episode steps: 573, steps per second:  44, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.541 [0.000, 5.000],  loss: 0.024506, mae: 3.887412, mean_q: 4.680482, mean_eps: 0.100000\n",
      " 1990716/2000000: episode: 2632, duration: 18.289s, episode steps: 823, steps per second:  45, episode reward: 19.000, mean reward:  0.023 [ 0.000,  1.000], mean action: 2.328 [0.000, 5.000],  loss: 0.023865, mae: 3.893365, mean_q: 4.690390, mean_eps: 0.100000\n",
      " 1991300/2000000: episode: 2633, duration: 13.021s, episode steps: 584, steps per second:  45, episode reward: 15.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 2.397 [0.000, 5.000],  loss: 0.026429, mae: 3.889086, mean_q: 4.683349, mean_eps: 0.100000\n",
      " 1991983/2000000: episode: 2634, duration: 14.958s, episode steps: 683, steps per second:  46, episode reward: 15.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.742 [0.000, 5.000],  loss: 0.020429, mae: 3.900538, mean_q: 4.700647, mean_eps: 0.100000\n",
      " 1992772/2000000: episode: 2635, duration: 17.629s, episode steps: 789, steps per second:  45, episode reward: 14.000, mean reward:  0.018 [ 0.000,  1.000], mean action: 3.027 [0.000, 5.000],  loss: 0.020806, mae: 3.903493, mean_q: 4.700472, mean_eps: 0.100000\n",
      " 1993391/2000000: episode: 2636, duration: 13.639s, episode steps: 619, steps per second:  45, episode reward:  7.000, mean reward:  0.011 [ 0.000,  1.000], mean action: 2.814 [0.000, 5.000],  loss: 0.025437, mae: 3.884653, mean_q: 4.677950, mean_eps: 0.100000\n",
      " 1994317/2000000: episode: 2637, duration: 20.577s, episode steps: 926, steps per second:  45, episode reward: 20.000, mean reward:  0.022 [ 0.000,  1.000], mean action: 2.374 [0.000, 5.000],  loss: 0.025780, mae: 3.891329, mean_q: 4.685339, mean_eps: 0.100000\n",
      " 1995373/2000000: episode: 2638, duration: 23.458s, episode steps: 1056, steps per second:  45, episode reward: 28.000, mean reward:  0.027 [ 0.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.023923, mae: 3.891418, mean_q: 4.685207, mean_eps: 0.100000\n",
      " 1995966/2000000: episode: 2639, duration: 13.133s, episode steps: 593, steps per second:  45, episode reward: 10.000, mean reward:  0.017 [ 0.000,  1.000], mean action: 2.619 [0.000, 5.000],  loss: 0.019889, mae: 3.893017, mean_q: 4.689693, mean_eps: 0.100000\n",
      " 1997591/2000000: episode: 2640, duration: 35.600s, episode steps: 1625, steps per second:  46, episode reward: 33.000, mean reward:  0.020 [ 0.000,  1.000], mean action: 2.145 [0.000, 5.000],  loss: 0.020821, mae: 3.883031, mean_q: 4.674921, mean_eps: 0.100000\n",
      " 1998831/2000000: episode: 2641, duration: 28.942s, episode steps: 1240, steps per second:  43, episode reward: 32.000, mean reward:  0.026 [ 0.000,  1.000], mean action: 1.735 [0.000, 5.000],  loss: 0.023519, mae: 3.873481, mean_q: 4.662571, mean_eps: 0.100000\n",
      "done, took 36879.748 seconds\n",
      "Entrenamiento completado. Pesos guardados.\n",
      "Evaluando el modelo entrenado...\n",
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 29.000, steps: 1118\n",
      "Episode 2: reward: 12.000, steps: 568\n",
      "Episode 3: reward: 14.000, steps: 594\n",
      "Episode 4: reward: 32.000, steps: 1205\n",
      "Episode 5: reward: 9.000, steps: 515\n",
      "Episode 6: reward: 34.000, steps: 1748\n",
      "Episode 7: reward: 9.000, steps: 469\n",
      "Episode 8: reward: 19.000, steps: 771\n",
      "Episode 9: reward: 20.000, steps: 1009\n",
      "Episode 10: reward: 18.000, steps: 554\n",
      "Episode 11: reward: 29.000, steps: 1137\n",
      "Episode 12: reward: 33.000, steps: 1396\n",
      "Episode 13: reward: 30.000, steps: 1037\n",
      "Episode 14: reward: 12.000, steps: 540\n",
      "Episode 15: reward: 20.000, steps: 767\n",
      "Episode 16: reward: 18.000, steps: 775\n",
      "Episode 17: reward: 21.000, steps: 749\n",
      "Episode 18: reward: 29.000, steps: 1283\n",
      "Episode 19: reward: 28.000, steps: 1063\n",
      "Episode 20: reward: 30.000, steps: 1129\n",
      "Episode 21: reward: 23.000, steps: 828\n",
      "Episode 22: reward: 24.000, steps: 1009\n",
      "Episode 23: reward: 15.000, steps: 733\n",
      "Episode 24: reward: 26.000, steps: 1227\n",
      "Episode 25: reward: 15.000, steps: 722\n",
      "Episode 26: reward: 33.000, steps: 1346\n",
      "Episode 27: reward: 7.000, steps: 377\n",
      "Episode 28: reward: 13.000, steps: 663\n",
      "Episode 29: reward: 26.000, steps: 1039\n",
      "Episode 30: reward: 13.000, steps: 585\n",
      "Episode 31: reward: 22.000, steps: 792\n",
      "Episode 32: reward: 17.000, steps: 789\n",
      "Episode 33: reward: 10.000, steps: 482\n",
      "Episode 34: reward: 13.000, steps: 609\n",
      "Episode 35: reward: 18.000, steps: 826\n",
      "Episode 36: reward: 29.000, steps: 1249\n",
      "Episode 37: reward: 20.000, steps: 976\n",
      "Episode 38: reward: 30.000, steps: 1164\n",
      "Episode 39: reward: 24.000, steps: 958\n",
      "Episode 40: reward: 29.000, steps: 1205\n",
      "Episode 41: reward: 7.000, steps: 358\n",
      "Episode 42: reward: 18.000, steps: 701\n",
      "Episode 43: reward: 15.000, steps: 587\n",
      "Episode 44: reward: 10.000, steps: 459\n",
      "Episode 45: reward: 20.000, steps: 851\n",
      "Episode 46: reward: 34.000, steps: 1193\n",
      "Episode 47: reward: 15.000, steps: 561\n",
      "Episode 48: reward: 13.000, steps: 637\n",
      "Episode 49: reward: 23.000, steps: 828\n",
      "Episode 50: reward: 19.000, steps: 745\n",
      "Episode 51: reward: 24.000, steps: 922\n",
      "Episode 52: reward: 11.000, steps: 503\n",
      "Episode 53: reward: 19.000, steps: 764\n",
      "Episode 54: reward: 15.000, steps: 717\n",
      "Episode 55: reward: 20.000, steps: 656\n",
      "Episode 56: reward: 6.000, steps: 358\n",
      "Episode 57: reward: 18.000, steps: 903\n",
      "Episode 58: reward: 22.000, steps: 804\n",
      "Episode 59: reward: 22.000, steps: 908\n",
      "Episode 60: reward: 8.000, steps: 383\n",
      "Episode 61: reward: 16.000, steps: 563\n",
      "Episode 62: reward: 12.000, steps: 586\n",
      "Episode 63: reward: 10.000, steps: 463\n",
      "Episode 64: reward: 22.000, steps: 854\n",
      "Episode 65: reward: 15.000, steps: 639\n",
      "Episode 66: reward: 18.000, steps: 788\n",
      "Episode 67: reward: 22.000, steps: 677\n",
      "Episode 68: reward: 9.000, steps: 455\n",
      "Episode 69: reward: 24.000, steps: 802\n",
      "Episode 70: reward: 19.000, steps: 783\n",
      "Episode 71: reward: 11.000, steps: 545\n",
      "Episode 72: reward: 22.000, steps: 790\n",
      "Episode 73: reward: 25.000, steps: 822\n",
      "Episode 74: reward: 17.000, steps: 614\n",
      "Episode 75: reward: 13.000, steps: 650\n",
      "Episode 76: reward: 18.000, steps: 715\n",
      "Episode 77: reward: 15.000, steps: 646\n",
      "Episode 78: reward: 18.000, steps: 613\n",
      "Episode 79: reward: 20.000, steps: 817\n",
      "Episode 80: reward: 14.000, steps: 624\n",
      "Episode 81: reward: 17.000, steps: 706\n",
      "Episode 82: reward: 24.000, steps: 835\n",
      "Episode 83: reward: 28.000, steps: 1187\n",
      "Episode 84: reward: 35.000, steps: 1235\n",
      "Episode 85: reward: 12.000, steps: 552\n",
      "Episode 86: reward: 25.000, steps: 898\n",
      "Episode 87: reward: 32.000, steps: 1166\n",
      "Episode 88: reward: 32.000, steps: 1230\n",
      "Episode 89: reward: 15.000, steps: 644\n",
      "Episode 90: reward: 26.000, steps: 1006\n",
      "Episode 91: reward: 15.000, steps: 967\n",
      "Episode 92: reward: 16.000, steps: 626\n",
      "Episode 93: reward: 11.000, steps: 448\n",
      "Episode 94: reward: 10.000, steps: 466\n",
      "Episode 95: reward: 32.000, steps: 1309\n",
      "Episode 96: reward: 14.000, steps: 659\n",
      "Episode 97: reward: 15.000, steps: 792\n",
      "Episode 98: reward: 30.000, steps: 1070\n",
      "Episode 99: reward: 12.000, steps: 528\n",
      "Episode 100: reward: 16.000, steps: 780\n",
      "\n",
      "===== RESULTADOS DEL TESTING =====\n",
      "Media de recompensa: 19.49\n",
      "Desviación estándar: 7.37\n",
      "Puntuación máxima: 35.00\n",
      "Puntuación mínima: 6.00\n",
      "Objetivo no alcanzado. Se necesita una media >= 20, obtenida: 19.49\n"
     ]
    }
   ],
   "source": [
    "# ENTRENAMIENTO\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "\n",
    "history = dqn.fit(\n",
    "    env,\n",
    "    nb_steps=2000000,        # 2 millones de pasos de entrenamiento\n",
    "    visualize=False,         # No mostrar el juego durante entrenamiento\n",
    "    verbose=2,               # Nivel de verbosidad\n",
    "    callbacks=callbacks,     # Callbacks para guardado\n",
    "    log_interval=10000       # Intervalo de logging\n",
    ")\n",
    "\n",
    "# Guardar los pesos finales del modelo\n",
    "dqn.save_weights('dqn_space_invaders_weights_final.h5f', overwrite=True)\n",
    "print(\"Entrenamiento completado. Pesos guardados.\")\n",
    "\n",
    "print(\"Evaluando el modelo entrenado...\")\n",
    "\n",
    "scores = dqn.test(\n",
    "    env,\n",
    "    nb_episodes=100,    # Número de episodios para testing\n",
    "    visualize=False,    # No mostrar durante testing\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "mean_score = np.mean(scores.history['episode_reward'])\n",
    "std_score = np.std(scores.history['episode_reward'])\n",
    "max_score = np.max(scores.history['episode_reward'])\n",
    "min_score = np.min(scores.history['episode_reward'])\n",
    "\n",
    "print(f\"\\n===== RESULTADOS DEL TESTING =====\")\n",
    "print(f\"Media de recompensa: {mean_score:.2f}\")\n",
    "print(f\"Desviación estándar: {std_score:.2f}\")\n",
    "print(f\"Puntuación máxima: {max_score:.2f}\")\n",
    "print(f\"Puntuación mínima: {min_score:.2f}\")\n",
    "\n",
    "if mean_score >= 20:\n",
    "    print(\"¡OBJETIVO ALCANZADO! Media de recompensa >= 20 puntos\")\n",
    "else:\n",
    "    print(f\"Objetivo no alcanzado. Se necesita una media >= 20, obtenida: {mean_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NAlu8b1Gb2b"
   },
   "source": [
    "3. Justificación de los parámetros seleccionados y de los resultados obtenidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ANFQiicXK3sO"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gym_atari",
   "language": "python",
   "name": "gym_atari"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
